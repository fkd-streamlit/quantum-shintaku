# -*- coding: utf-8 -*-
# ============================================================
# app.py é‡å­ç¥è¨—ï¼ˆè©¦ä½œï¼‰â€” ç¸ã®çƒä½“ï¼ˆQUBO Ã— ã‚¢ãƒ¼ãƒˆï¼‰
# ä»•æ§˜ï¼ˆã”è¦æœ›åæ˜ ï¼‰:
# - QUOTESãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆå…¥ã‚ŠExcelã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦èª­ã¿è¾¼ã‚€ï¼ˆst.file_uploaderï¼‰
# - æ—¢å®šExcelï¼ˆãƒ­ãƒ¼ã‚«ãƒ«/åŒæ¢±ï¼‰ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚‚èª­ã‚ã‚‹
# - BGMã¯ assets/bgm.mp4ï¼ˆst.audio + audio/mp4ï¼‰
# - ç‚¹æ»…ï¼ˆãƒ•ãƒ©ãƒƒã‚·ãƒ¥ï¼‰ã‚’æ’é™¤ï¼šè‡ªå‹•æ›´æ–°ãªã—ã€æ˜Ÿå±‘å›ºå®šã€åŒæ¡ä»¶ãªã‚‰é…ç½®å›ºå®šï¼ˆseedå›ºå®šï¼‰
# - Plotlyã®ç·šãƒˆãƒ¬ãƒ¼ã‚¹ã‚’é›†ç´„ã—ã¦è»½é‡åŒ–ï¼ˆNoneåŒºåˆ‡ã‚Šï¼‰
# - ã€ŒQUBOã‚’ä½¿ã£ã¦ã„ã‚‹æ„Ÿã€ã‚’å³å´ã§å¯è¦–åŒ–ï¼ˆQè¡Œåˆ—ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—/å¼·ã„çµã³ã¤ãï¼‰
# - ã€Œæ°—ã«ãªã‚‹å˜èªâ†’æ ¼è¨€å€™è£œã€å°ç·š
# ============================================================

import os
import re
import io
import zlib
import time
import hashlib
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import numpy as np
import plotly.graph_objects as go
import streamlit as st

# pandasï¼ˆExcelèª­ã¿è¾¼ã¿ç”¨ï¼‰
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except Exception:
    PANDAS_AVAILABLE = False
    pd = None


# ============================================================
# 0) ãƒšãƒ¼ã‚¸è¨­å®š + CSS
# ============================================================
st.set_page_config(page_title="é‡å­ç¥è¨— - ç¸ã®çƒä½“", layout="wide")

SPACE_CSS = """
<style>
.stApp{
  background:
    radial-gradient(circle at 18% 24%, rgba(110,150,255,0.12), transparent 38%),
    radial-gradient(circle at 78% 68%, rgba(255,160,220,0.08), transparent 44%),
    radial-gradient(circle at 50% 50%, rgba(255,255,255,0.03), transparent 55%),
    linear-gradient(180deg, rgba(6,8,18,1), rgba(10,12,26,1));
}
.block-container{ padding-top: 1.2rem; }
div[data-testid="stMarkdownContainer"] p,
div[data-testid="stMarkdownContainer"] li{
  font-family: "Hiragino Mincho ProN", "Yu Mincho", "Noto Serif JP", serif;
  letter-spacing: 0.02em;
  color: rgba(245,245,255,0.92);
}
h1,h2,h3{
  font-family: "Hiragino Mincho ProN", "Yu Mincho", "Noto Serif JP", serif !important;
  font-weight: 600 !important;
  color: rgba(245,245,255,0.95);
}
section[data-testid="stSidebar"]{
  background: rgba(255,255,255,0.08);
  border-right: 1px solid rgba(255,255,255,0.10);
  backdrop-filter: blur(10px);
}
div[data-testid="stPlotlyChart"] > div{
  position: relative;
  border-radius: 18px;
  overflow: hidden;
  box-shadow: 0 18px 60px rgba(0,0,0,0.30);
}
div[data-testid="stPlotlyChart"] > div::after{
  content:"";
  position:absolute;
  inset:0;
  background:
    radial-gradient(circle at 30% 25%, rgba(120,160,255,0.10), transparent 45%),
    radial-gradient(circle at 70% 65%, rgba(255,180,220,0.06), transparent 52%),
    radial-gradient(circle at 50% 50%, rgba(0,0,0,0.00), rgba(0,0,0,0.38));
  pointer-events:none;
}
.smallnote{opacity:0.78; font-size:0.92rem;}
.card{
  background: rgba(255,255,255,0.06);
  border: 1px solid rgba(255,255,255,0.10);
  border-radius: 18px;
  padding: 16px 16px 10px 16px;
  box-shadow: 0 18px 60px rgba(0,0,0,0.18);
}
</style>
"""
st.markdown(SPACE_CSS, unsafe_allow_html=True)


# ============================================================
# 0.5) ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ åˆæœŸåŒ–
# ============================================================
def init_session_state():
    defaults = {
        "bgm_on": False,
        "last_params_hash": "",
        "network": None,
        "pos": None,
        "keywords": [],
        "center_set": set(),
        "famous_quotes": None,          # Excelçµ±åˆå¾Œã®æ ¼è¨€DB
        "quotes_meta": {"loaded": 0},   # è¡¨ç¤ºç”¨
    }
    for k, v in defaults.items():
        if k not in st.session_state:
            st.session_state[k] = v

init_session_state()


# ============================================================
# 1) ã‚°ãƒ­ãƒ¼ãƒãƒ«å˜èªDBï¼ˆä»–ã®äººã®è¨€è‘‰ï¼‰
# ============================================================
GLOBAL_WORDS_DATABASE = [
    "ä¸–ç•Œå¹³å’Œ","è²¢çŒ®","æˆé•·","å­¦ã³","æŒ‘æˆ¦","å¤¢","å¸Œæœ›","æœªæ¥",
    "æ„Ÿè¬","æ„›","å¹¸ã›","å–œã³","å®‰å¿ƒ","å……å®Ÿ","æº€è¶³","å¹³å’Œ",
    "åŠªåŠ›","ç¶™ç¶š","å¿è€","èª å®Ÿ","æ­£ç›´","å„ªã—ã•","æ€ã„ã‚„ã‚Š","å…±æ„Ÿ",
    "èª¿å’Œ","ãƒãƒ©ãƒ³ã‚¹","è‡ªç„¶","ç¾","çœŸå®Ÿ","è‡ªç”±","æ­£ç¾©","é“",
    "çµ†","ã¤ãªãŒã‚Š","å®¶æ—","å‹äºº","ä»²é–“","ä¿¡é ¼","å°Šæ•¬","å”åŠ›",
    "ä»Š","ç¬é–“","éç¨‹","å¤‰åŒ–","é€²åŒ–","ç™ºå±•","å¾ªç’°","æµã‚Œ",
    "é™ã‘ã•","é›†ä¸­","è¦šæ‚Ÿ","æ±ºæ„","å‹‡æ°—","å¼·ã•","æŸ”è»Ÿæ€§","å¯›å®¹",
]

CATEGORIES = {
    "é¡˜ã„": ["ä¸–ç•Œå¹³å’Œ","è²¢çŒ®","æˆé•·","å¤¢","å¸Œæœ›","æœªæ¥"],
    "æ„Ÿæƒ…": ["æ„Ÿè¬","æ„›","å¹¸ã›","å–œã³","å®‰å¿ƒ","æº€è¶³","å¹³å’Œ"],
    "è¡Œå‹•": ["åŠªåŠ›","ç¶™ç¶š","å¿è€","èª å®Ÿ","æ­£ç›´"],
    "å“²å­¦": ["èª¿å’Œ","ãƒãƒ©ãƒ³ã‚¹","è‡ªç„¶","ç¾","é“","çœŸå®Ÿ","è‡ªç”±","æ­£ç¾©"],
    "é–¢ä¿‚": ["çµ†","ã¤ãªãŒã‚Š","å®¶æ—","å‹äºº","ä»²é–“","ä¿¡é ¼","å°Šæ•¬","å”åŠ›"],
    "å†…çš„": ["é™ã‘ã•","é›†ä¸­","è¦šæ‚Ÿ","æ±ºæ„","å‹‡æ°—","å¼·ã•","æŸ”è»Ÿæ€§","å¯›å®¹"],
    "æ™‚é–“": ["ä»Š","ç¬é–“","éç¨‹","å¤‰åŒ–","é€²åŒ–","ç™ºå±•","å¾ªç’°","æµã‚Œ"],
}


# ============================================================
# 2) æ ¼è¨€DBï¼ˆãƒ™ãƒ¼ã‚¹ + Excel QUOTESï¼‰
# ============================================================
BASE_FAMOUS_QUOTES = [
    {
        "keywords": ["å¹³å’Œ","ä¸–ç•Œ","è²¢çŒ®","å¸Œæœ›"],
        "quote": "é›ªã®ä¸‹ã§ç¨®ã¯æ˜¥ã‚’å¾…ã£ã¦ã„ã‚‹ã€‚ç„¦ã‚‹ã¹ã‹ã‚‰ãšã€æ™‚æº€ã¡ã‚‹ã‚’å¾…ã¦ã€‚",
        "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ/å¯“è©±èª¿",
        "note": "å…¬é–‹ç”¨ã«å…¸æ‹ ä»˜ãæ ¼è¨€ã¸å·®ã—æ›¿ãˆå¯"
    },
    {
        "keywords": ["æˆé•·","åŠªåŠ›","ç¶™ç¶š","æŒ‘æˆ¦"],
        "quote": "åƒé‡Œã®é“ã‚‚ä¸€æ­©ã‹ã‚‰ã€‚æ­©ã¿ã‚’æ­¢ã‚ãšã€ç¶šã‘ã‚‹ã“ã¨ã«æ„å‘³ãŒã‚ã‚‹ã€‚",
        "source": "æ•…äº‹æˆèªï¼ˆè¦å…¸æ‹ ç¢ºèªï¼‰â€”æš«å®š",
        "note": "çŸ­æ–‡åŒ–ã—ãŸå®šå‹å¥ã€‚å…¬é–‹å‰ã«å…¸æ‹ ç²¾æŸ»æ¨å¥¨"
    },
    {
        "keywords": ["æ„Ÿè¬","æ„›","çµ†","ã¤ãªãŒã‚Š"],
        "quote": "ä¸€æœŸä¸€ä¼šã€‚ä»Šã“ã®ç¬é–“ã‚’å¤§åˆ‡ã«ã€‚ã™ã¹ã¦ã¯ç¸ã§ç¹‹ãŒã£ã¦ã„ã‚‹ã€‚",
        "source": "ä¸€æœŸä¸€ä¼šï¼ˆèŒ¶é“æ€æƒ³ï¼‰ï¼‹é‡å­ç¥è¨— è©¦ä½œï¼ˆç·¨é›†/æ„è¨³ï¼‰",
        "note": ""
    },
    {
        "keywords": ["è‡ªç„¶","èª¿å’Œ","ãƒãƒ©ãƒ³ã‚¹","æµã‚Œ"],
        "quote": "æ°´ã¯ã€äº‰ã‚ãªã„ã€‚å½¢ã«ã“ã ã‚ã‚‰ãšã€æµã‚Œã‚‹ãŒã¾ã¾ã«ã€‚",
        "source": "è€å­ã€é“å¾³çµŒã€ï¼ˆä¸Šå–„è‹¥æ°´ï¼‰â€”æ„è¨³/ç·¨é›†",
        "note": "å³å¯†ãªåŸæ–‡å¼•ç”¨ã§ã¯ãªãæ„è¨³"
    },
    {
        "keywords": ["é™ã‘ã•","é›†ä¸­","ä»Š","ç¬é–“"],
        "quote": "æ­¢ã¾ã‚‹ã“ã¨ã§ã€æµã‚ŒãŒè¦‹ãˆã‚‹ã€‚å‹•ã®ä¸­ã«é™ãŒã‚ã‚‹ã€‚",
        "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ",
        "note": ""
    },
    {
        "keywords": ["å‹‡æ°—","æ±ºæ„","æŒ‘æˆ¦","é“"],
        "quote": "é“ãŒåˆ†ã‹ã‚Œã¦ã„ãŸã‚‰ã€å¿µã®ãªã„æ–¹ã¸è¡Œã‘ã€‚",
        "source": "å‡ºå…¸è¦ç¢ºèªï¼ˆæµé€šå¥ï¼‰â€”æš«å®š",
        "note": "å…¬é–‹å‰ã«å…¸æ‹ ã‚’ç¢ºå®šæ¨å¥¨"
    },
    {
        "keywords": ["æ€ã„ã‚„ã‚Š","å„ªã—ã•","å…±æ„Ÿ","ä¿¡é ¼"],
        "quote": "äººã®å¿ƒã«å¯„ã‚Šæ·»ã†ã€‚ãã‚ŒãŒçœŸã®å¼·ã•ã§ã‚ã‚‹ã€‚",
        "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ",
        "note": ""
    },
    {
        "keywords": ["å¤‰åŒ–","é€²åŒ–","ç™ºå±•","æœªæ¥"],
        "quote": "ç„¡ç‚ºã«ã—ã¦ç‚ºã™ã€‚å‹•ãã“ã¨ãŒé™ã§ã‚ã‚‹ã€‚",
        "source": "æ±æ´‹æ€æƒ³ï¼ˆç„¡ç‚ºè‡ªç„¶ï¼‰â€”æ„è¨³/ç·¨é›†",
        "note": ""
    },
    {
        "keywords": ["ç¾","çœŸå®Ÿ","è‡ªç„¶","èª¿å’Œ"],
        "quote": "é–“ã“ããŒç­”ãˆã§ã‚ã‚‹ã€‚ä½™ç™½ã«ã“ãæœ¬è³ªãŒã‚ã‚‹ã€‚",
        "source": "ç¾å­¦ï¼ˆé–“/ä½™ç™½ï¼‰ï¼‹é‡å­ç¥è¨— è©¦ä½œï¼ˆç·¨é›†ï¼‰",
        "note": ""
    },
    {
        "keywords": ["è‡ªç”±","æ­£ç¾©","é“","èª å®Ÿ"],
        "quote": "å·±ã«èª å®Ÿã§ã‚ã‚‹ã“ã¨ã€‚ãã‚ŒãŒè‡ªç”±ã¸ã®é“ã§ã‚ã‚‹ã€‚",
        "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ",
        "note": ""
    },
]

# æ—¢å®šExcelï¼ˆãƒ­ãƒ¼ã‚«ãƒ«/åŒæ¢±å‘ã‘ï¼‰
# â€»Streamlit Cloudã§ã¯å­˜åœ¨ã—ãªã„ã®ã§ã€åŸºæœ¬ã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¨å¥¨
EXCEL_DEFAULT_PATH = "quantum_shintaku_pack_v3_with_sense_20260213_oposite_modify (2).xlsx"

def _hash_bytes(b: bytes) -> str:
    return hashlib.sha256(b).hexdigest()

@st.cache_data(show_spinner=False)
def load_quotes_from_excel_bytes(excel_bytes: bytes, file_hash: str) -> List[Dict]:
    """Excelãƒã‚¤ãƒŠãƒªã‹ã‚‰ QUOTES ã‚·ãƒ¼ãƒˆã‚’èª­ã‚€ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å®‰å®šã®ãŸã‚ file_hash ã‚’å¼•æ•°ã«å«ã‚ã‚‹ï¼‰"""
    if not PANDAS_AVAILABLE or not excel_bytes:
        return []
    try:
        bio = io.BytesIO(excel_bytes)
        df = pd.read_excel(bio, sheet_name="QUOTES", engine="openpyxl")
    except Exception:
        return []

    quotes: List[Dict] = []

    def pick_text(row, candidates):
        for col in candidates:
            if col in df.columns:
                v = str(row.get(col, "")).strip()
                if v and v.lower() not in ("nan", "none"):
                    return v
        return ""

    for _, row in df.iterrows():
        quote_text = pick_text(row, ["æ ¼è¨€", "QUOTE", "Quote", "quote", "ãƒ†ã‚­ã‚¹ãƒˆ", "æ–‡", "è¨€è‘‰"])
        if not quote_text:
            continue

        kw_str = pick_text(row, ["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "KEYWORDS", "Keywords", "keywords", "ã‚¿ã‚°", "TAG", "Tag"])
        keywords = [k.strip() for k in kw_str.replace("ã€", ",").split(",") if k.strip()] if kw_str else []

        source = pick_text(row, ["å‡ºå…¸", "SOURCE", "Source", "source", "å‡ºæ‰€", "å…¸æ‹ ", "ä½œè€…"]) or "ä¼çµ±çš„ãªæ•™ãˆ"
        note   = pick_text(row, ["å‚™è€ƒ", "NOTE", "Note", "note", "æ³¨", "ãƒ¡ãƒ¢"])

        quotes.append({"quote": quote_text, "keywords": keywords, "source": source, "note": note})

    return quotes

def build_famous_quotes(excel_bytes: Optional[bytes] = None) -> Tuple[List[Dict], int]:
    """BASE +ï¼ˆä»»æ„ã§ï¼‰Excelã®QUOTESã‚’çµ±åˆ"""
    fam = list(BASE_FAMOUS_QUOTES)
    excel_quotes: List[Dict] = []

    if excel_bytes:
        h = _hash_bytes(excel_bytes)
        excel_quotes = load_quotes_from_excel_bytes(excel_bytes, file_hash=h)

    if excel_quotes:
        existing = {q.get("quote", "") for q in fam}
        for q in excel_quotes:
            qt = q.get("quote", "")
            if qt and qt not in existing:
                fam.append(q)
                existing.add(qt)

    return fam, len(excel_quotes)


# ============================================================
# 3) ãƒ†ã‚­ã‚¹ãƒˆâ†’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆç°¡æ˜“ï¼‰
# ============================================================
STOP_TOKENS = set([
    "ã—ãŸ","ãŸã„","ã„ã‚‹","ã„","ã“ã¨","ãã‚Œ","ã“ã‚Œ","ãŸã‚","ã‚ˆã†","ã®ã§","ã‹ã‚‰",
    "ã§ã™","ã¾ã™","ã‚ã‚‹","ãªã„","ãã—ã¦","ã§ã‚‚","ã—ã‹ã—","ã¾ãŸ",
    "è‡ªåˆ†","ç§","ã‚ãªãŸ","ã‚‚ã®","æ„Ÿã˜","æ°—æŒã¡","ä»Š","ä»Šæ—¥"
])

def extract_keywords(text: str, top_n: int = 5) -> List[str]:
    text = (text or "").strip()
    if not text:
        return ["é™ã‘ã•", "è¿·ã„"]

    # ã¾ãšã¯DBèªã®ç›´æ¥ãƒ’ãƒƒãƒˆï¼ˆå„ªå…ˆï¼‰
    found = [w for w in GLOBAL_WORDS_DATABASE if w in text]
    if found:
        return found[:top_n]

    # ã–ã£ãã‚Šåˆ†å‰²ï¼ˆä¾å­˜å¢—ã‚„ã•ãªã„æ–¹é‡ï¼‰
    text_clean = re.sub(r"[0-9ï¼-ï¼™ã€ã€‚ï¼,.!ï¼?ï¼Ÿ\(\)\[\]{}ã€Œã€ã€ã€\"'ï¼š:;ï¼/\\\n\r\t]+", " ", text)
    tokens = [t.strip() for t in re.split(r"\s+", text_clean) if t.strip()]
    tokens = [t for t in tokens if (len(t) >= 2 and t not in STOP_TOKENS)]

    if not tokens:
        return ["é™ã‘ã•", "è¿·ã„"]

    tokens = sorted(tokens, key=lambda s: (-len(s), s))
    return tokens[:top_n]


# ============================================================
# 4) â€œã‚¨ãƒãƒ«ã‚®ãƒ¼â€è¨ˆç®—ï¼ˆQUBOçš„ç›¸äº’ä½œç”¨ï¼‰
# ============================================================
def calculate_semantic_similarity(word1: str, word2: str) -> float:
    if word1 == word2:
        return 1.0

    common_chars = set(word1) & set(word2)
    char_sim = len(common_chars) / max(len(set(word1)), len(set(word2)), 1)

    category_sim = 0.0
    for _, ws in CATEGORIES.items():
        w1_in = word1 in ws
        w2_in = word2 in ws
        if w1_in and w2_in:
            category_sim = 1.0
            break
        elif w1_in or w2_in:
            category_sim = max(category_sim, 0.3)

    len_sim = 1.0 - abs(len(word1) - len(word2)) / max(len(word1), len(word2), 1)
    similarity = 0.4 * char_sim + 0.4 * category_sim + 0.2 * len_sim
    return float(np.clip(similarity, 0.0, 1.0))

def calculate_energy_between_words(word1: str, word2: str, rng: np.random.Generator, jitter: float) -> float:
    similarity = calculate_semantic_similarity(word1, word2)
    # ä¼¼ã¦ã‚‹ã»ã©ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒä½ã„ï¼ˆçµã³ã¤ãï¼‰
    energy = -2.0 * similarity + 0.5

    common = set(word1) & set(word2)
    if common:
        energy -= 0.20 * len(common) / max(len(word1), len(word2), 1)

    for _, ws in CATEGORIES.items():
        if (word1 in ws) and (word2 in ws):
            energy -= 0.60
            break

    if jitter > 0:
        energy += rng.normal(0, jitter)
    return float(energy)

def build_qubo_matrix_for_words(words: List[str], rng: np.random.Generator, jitter: float) -> np.ndarray:
    n = len(words)
    Q = np.zeros((n, n), dtype=float)
    np.fill_diagonal(Q, -0.5)
    for i in range(n):
        for j in range(i + 1, n):
            e = calculate_energy_between_words(words[i], words[j], rng, jitter)
            Q[i, j] = e
            Q[j, i] = e
    return Q

def solve_qubo_placement(
    Q: np.ndarray,
    words: List[str],
    center_indices: List[int],
    energies: Dict[str, float],
    rng: np.random.Generator,
    n_iterations: int = 100,
    progress_callback=None,
) -> np.ndarray:
    n = len(words)
    pos = np.zeros((n, 3), dtype=float)
    for idx in center_indices:
        if idx < n:
            pos[idx] = [0.0, 0.0, 0.0]

    ev = list(energies.values()) if energies else []
    if ev:
        mn, mx = min(ev), max(ev)
        er = (mx - mn) if mx != mn else 1.0
    else:
        mn, er = -3.0, 3.0

    golden_angle = np.pi * (3 - np.sqrt(5))
    k = 0
    for i in range(n):
        if i in center_indices:
            continue
        w = words[i]
        e = energies.get(w, 0.0)
        norm = (e - mn) / er
        dist = 0.3 + (1.0 - norm) * 2.2

        theta = golden_angle * k
        y = 1 - (k / float(max(1, n - len(center_indices) - 1))) * 2
        r = np.sqrt(max(0.0, 1 - y * y))
        x = np.cos(theta) * r * dist
        z = np.sin(theta) * r * dist
        pos[i] = [x, y * dist * 0.6, z]
        k += 1

    for it in range(n_iterations):
        for i in range(n):
            if i in center_indices:
                continue
            force = np.zeros(3, dtype=float)

            # ä¸­å¿ƒã¨ã®è·é›¢ã‚’ä¿ã¤
            for cidx in center_indices:
                vec = pos[cidx] - pos[i]
                d = np.linalg.norm(vec)
                if d > 0.01:
                    w = words[i]
                    e = energies.get(w, 0.0)
                    norm = (e - mn) / er if er > 0 else 0.5
                    target = 0.3 + (1.0 - norm) * 2.2
                    if d < target * 0.9:
                        force -= vec / d * 0.05
                    elif d > target * 1.1:
                        force += vec / d * 0.10

            # Qç›¸äº’ä½œç”¨
            for j in range(n):
                if i == j or j in center_indices:
                    continue
                eij = Q[i, j]
                if eij < -0.3:
                    vec = pos[j] - pos[i]
                    d = np.linalg.norm(vec)
                    if d > 0.01:
                        force += vec / d * (abs(eij) * 0.08)
                elif eij > 0.2:
                    vec = pos[i] - pos[j]
                    d = np.linalg.norm(vec)
                    if d > 0.01:
                        force += vec / d * (abs(eij) * 0.03)

            pos[i] += force * 0.15

        if progress_callback:
            progress_callback(it + 1, n_iterations)

    return pos

def build_word_network(center_words: List[str], database: List[str], n_total: int,
                       rng: np.random.Generator, jitter: float) -> Dict:
    all_words = list(dict.fromkeys(center_words + database))  # é †åºç¶­æŒ
    energies: Dict[str, float] = {}

    for w in all_words:
        if w in center_words:
            energies[w] = -3.0
        else:
            e_list = [calculate_energy_between_words(c, w, rng, jitter) for c in center_words]
            energies[w] = float(np.mean(e_list))

    sorted_words = sorted(energies.items(), key=lambda x: x[1])  # ä½ã„ã»ã©ä¸­å¿ƒã«è¿‘ã„
    selected: List[str] = []
    for w, _ in sorted_words:
        if w in center_words and w not in selected:
            selected.append(w)
    for w, _ in sorted_words:
        if w not in selected:
            selected.append(w)
        if len(selected) >= n_total:
            break

    Q = build_qubo_matrix_for_words(selected, rng, jitter)
    center_indices = [i for i, w in enumerate(selected) if w in center_words]

    edges: List[Tuple[int, int, float]] = []
    n = len(selected)
    for i in range(n):
        for j in range(i + 1, n):
            e = Q[i, j]
            if e < -0.25:
                edges.append((i, j, float(e)))

    return {
        "words": selected,
        "energies": {w: energies[w] for w in selected},
        "edges": edges,
        "Q": Q,
        "center_indices": center_indices,
    }


# ============================================================
# 5) æ ¼è¨€é¸æŠï¼ˆå‡ºæ‰€ã¤ãï¼‰
# ============================================================
def select_relevant_quote(keywords: List[str], famous_quotes: List[Dict]) -> Dict[str, str]:
    if not keywords:
        keywords = ["ä»Š"]

    ks = set()
    for kw in keywords:
        k = kw.strip().lower()
        ks.add(k)
        if len(k) > 2:
            for i in range(len(k) - 1):
                ks.add(k[i:i+2])

    best = None
    best_score = -1.0

    for q in famous_quotes:
        qk = q.get("keywords", [])
        if not qk:
            continue

        qks = set()
        for k in qk:
            kk = k.strip().lower()
            qks.add(kk)
            if len(kk) > 2:
                for i in range(len(kk) - 1):
                    qks.add(kk[i:i+2])

        exact = len(ks & qks)
        partial = 0.0
        for a in ks:
            for b in qks:
                if a in b or b in a:
                    partial += 0.5

        text = q.get("quote", "").lower()
        text_match = 0.0
        for a in ks:
            if len(a) >= 2 and a in text:
                text_match += 0.3

        score = exact * 2.0 + partial + text_match
        if score > best_score:
            best_score = score
            best = q

    if best is None or best_score < 0.1:
        return {"quote": "ã‚ãªãŸã®è¦³æ¸¬ãŒã€ã“ã®ä¸–ç•Œç·šã‚’ç¢ºå®šã•ã›ã¾ã—ãŸã€‚", "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ", "note": ""}

    return {"quote": best.get("quote", ""), "source": best.get("source", "ä¼çµ±çš„ãªæ•™ãˆ"), "note": best.get("note", "")}

def quote_candidates_for_word(word: str, famous_quotes: List[Dict], max_n: int = 6) -> List[Dict]:
    if not word:
        return []
    w = word.strip().lower()
    scored = []
    for q in famous_quotes:
        ks = [k.strip().lower() for k in q.get("keywords", [])]
        score = 0.0
        if w in ks:
            score += 3.0
        else:
            for k in ks:
                if w in k or k in w:
                    score += 1.0
        if w in (q.get("quote","").lower()):
            score += 0.5
        if score > 0:
            scored.append((score, q))
    scored.sort(key=lambda x: (-x[0], x[1].get("quote","")))
    return [q for _, q in scored[:max_n]]


# ============================================================
# 6) seedå›ºå®šï¼ˆåŒæ¡ä»¶ãªã‚‰åŒé…ç½®ï¼‰
# ============================================================
def make_seed(s: str) -> int:
    return int(zlib.adler32(s.encode("utf-8")) & 0xFFFFFFFF)


# ============================================================
# 7) UIï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼‰
# ============================================================
st.title("é‡å­ç¥è¨—ï¼ˆè©¦ä½œï¼‰â€” ç¸ã®çƒä½“ï¼ˆQUBO Ã— ã‚¢ãƒ¼ãƒˆï¼‰")

BGM_PATH = Path("assets/bgm.mp4")
BGM_FORMAT = "audio/mp4"

with st.sidebar:
    st.markdown("### ğŸ“˜ æ ¼è¨€ãƒ‡ãƒ¼ã‚¿ï¼ˆExcel/QUOTESï¼‰")

    if not PANDAS_AVAILABLE:
        st.error("pandas ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€Excelèª­ã¿è¾¼ã¿ãŒç„¡åŠ¹ã§ã™ã€‚requirements.txt ã« pandas ã¨ openpyxl ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚")

    uploaded_excel = st.file_uploader(
        "QUOTESãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆå…¥ã‚ŠExcelã‚’é¸æŠ",
        type=["xlsx"],
        accept_multiple_files=False
    )

    excel_bytes: Optional[bytes] = None
    source_label = "BASEã®ã¿"

    if uploaded_excel is not None:
        excel_bytes = uploaded_excel.getvalue()
        source_label = "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"
        st.success("Excelã‚’èª­ã¿è¾¼ã¿å¯¾è±¡ã«è¨­å®šã—ã¾ã—ãŸï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼‰")
    else:
        # æ—¢å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒåŒæ¢±ã•ã‚Œã¦ã„ã‚Œã°èª­ã‚€ï¼ˆä»»æ„ï¼‰
        if os.path.exists(EXCEL_DEFAULT_PATH):
            try:
                with open(EXCEL_DEFAULT_PATH, "rb") as f:
                    excel_bytes = f.read()
                source_label = f"åŒæ¢±: {EXCEL_DEFAULT_PATH}"
                st.info("Excelã‚’èª­ã¿è¾¼ã¿å¯¾è±¡ã«è¨­å®šã—ã¾ã—ãŸï¼ˆåŒæ¢±ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰")
            except Exception:
                excel_bytes = None
        else:
            st.caption("ExcelæœªæŒ‡å®šï¼šBASE_FAMOUS_QUOTESã®ã¿ã§å‹•ä½œã—ã¾ã™ã€‚")

    # Famous quotes ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿æŒï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤‰æ›´æ™‚ã®ã¿æ›´æ–°ã•ã‚Œã‚‹ï¼‰
    # â€»uploaded_excel ãŒå¤‰ã‚ã£ãŸã‚‰ st ãŒå†å®Ÿè¡Œã•ã‚Œã‚‹ã®ã§è‡ªç„¶ã«æ›´æ–°ã•ã‚Œã¾ã™
    famous_quotes, excel_loaded = build_famous_quotes(excel_bytes)
    st.session_state["famous_quotes"] = famous_quotes
    st.session_state["quotes_meta"] = {"loaded": excel_loaded, "source": source_label}

import mimetypes

st.markdown("---")
st.markdown("### ğŸµ éŸ³æ¥½")

st.session_state["bgm_on"] = st.toggle(
    "BGMã‚’å†ç”Ÿï¼ˆâ–¶ã‚’æŠ¼ã™ã¨é³´ã‚Šã¾ã™ï¼‰",
    value=st.session_state.get("bgm_on", False)
)

if st.session_state["bgm_on"]:
    if BGM_PATH.exists():
        bgm_bytes = BGM_PATH.read_bytes()

        # MIMEã‚’è‡ªå‹•åˆ¤å®š
        mime, _ = mimetypes.guess_type(str(BGM_PATH))
        mime = mime or "audio/mp4"

        st.caption(f"å½¢å¼: {mime} / ã‚µã‚¤ã‚º: {len(bgm_bytes)/1024/1024:.2f} MB")

        try:
            # ã¾ãš audio ã¨ã—ã¦å†ç”Ÿ
            st.audio(bgm_bytes, format=mime)

            # mp4ãŒé³´ã‚‰ãªã„ç’°å¢ƒå‘ã‘ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if str(BGM_PATH).lower().endswith(".mp4"):
                with st.expander("ğŸ” å†ç”Ÿã§ããªã„å ´åˆã¯ã“ã¡ã‚‰ï¼ˆvideoãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"):
                    st.video(bgm_bytes, format="video/mp4")

        except Exception as e:
            st.error(f"å†ç”Ÿã‚¨ãƒ©ãƒ¼: {e}")
            st.caption("mp4ã§å†ç”Ÿã§ããªã„å ´åˆã¯ mp3 å½¢å¼ã¸ã®å¤‰æ›ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
    else:
        st.error(f"âš  BGMãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {BGM_PATH}ï¼ˆassets/bgm.mp4 ã‚’GitHubã«è¿½åŠ ã—ã¦ãã ã•ã„ï¼‰")
    st.markdown("---")
    st.markdown("### ä»Šã®æ°—æŒã¡ï¼ˆå…¥åŠ›ï¼‰")
    user_input = st.text_area(
        "çŸ­ã„ä¸€æ–‡ã§OKï¼ˆä¾‹ï¼šäººã¨ã®ä¼šè©±ã«ç–²ã‚ŒãŸã€‚å°‘ã—è¿·ã£ã¦ã„ã‚‹ã€‚ï¼‰",
        value="äººã¨ã®ä¼šè©±ã«ç–²ã‚ŒãŸã€‚å°‘ã—è¿·ã£ã¦ã„ã‚‹ã€‚",
        height=90,
        key="user_input_text",
    )

    st.markdown("---")
    st.markdown("### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    top_n = st.slider("æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°", 2, 10, 5, 1)
    n_total = st.slider("ç©ºé–“ã«å‡ºã™å˜èªæ•°ï¼ˆä¸­å¿ƒï¼‹å‘¨è¾ºï¼‰", 15, 60, 30, 1)

    st.caption("â€»ç‚¹æ»…é˜²æ­¢ã®ãŸã‚ã€è‡ªå‹•æ›´æ–°ï¼ˆã‚†ã‚‰ãï¼‰ã¯ç„¡åŠ¹åŒ–ã—ã¦ã„ã¾ã™ã€‚")

    noise = st.slider("ä½ç½®ã®ã‚†ã‚‰ãï¼ˆå†è¨ˆç®—æ™‚ã®ã¿ï¼‰", 0.00, 0.20, 0.06, 0.01)
    jitter = st.slider("ã‚¨ãƒãƒ«ã‚®ãƒ¼æºã‚‰ã", 0.00, 0.25, 0.10, 0.01)
    qubo_iterations = st.slider("QUBOæœ€é©åŒ–ã®åå¾©å›æ•°", 50, 200, 80, 10)

    st.markdown("---")
    st.markdown("### å®‡å®™ã®å¯†åº¦")
    star_count = st.slider("æ˜Ÿå±‘ã®æ•°", 200, 2200, 900, 50)
    st.caption("â€»æ˜Ÿã®ã¾ãŸãŸãï¼ˆç‚¹æ»…ï¼‰ã¯ç„¡åŠ¹åŒ–ã—ã¦ã„ã¾ã™ã€‚")

    st.markdown("---")
    enable_zoom = st.toggle("ãƒã‚¦ã‚¹ãƒ›ã‚¤ãƒ¼ãƒ«ã§ã‚ºãƒ¼ãƒ ", value=True)

    if st.button("ğŸ”„ å†è¨ˆç®—", use_container_width=True):
        st.session_state["last_params_hash"] = ""  # å¼·åˆ¶å†è¨ˆç®—
        st.rerun()


# ============================================================
# 8) å†è¨ˆç®—åˆ¤å®šï¼ˆé™æ­¢è¡¨ç¤ºï¼‰
# ============================================================
# ExcelãŒå¤‰ã‚ã£ãŸå ´åˆã‚‚é…ç½®ã‚’å¤‰ãˆã‚‹ï¼ˆæ ¼è¨€DBç”±æ¥ã®ä½“é¨“ãŒå¤‰åŒ–ã™ã‚‹ãŸã‚ï¼‰
quotes_meta = st.session_state.get("quotes_meta", {})
quotes_sig = f"{quotes_meta.get('source','')}_{quotes_meta.get('loaded',0)}"

params_hash = f"{user_input}|{top_n}|{n_total}|{noise}|{jitter}|{qubo_iterations}|{star_count}|{quotes_sig}"
needs_recalc = params_hash != st.session_state["last_params_hash"]


# ============================================================
# 9) è¨ˆç®—ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ / é…ç½®ï¼‰
# ============================================================
def compute_all():
    progress_placeholder = st.empty()

    seed = make_seed(params_hash)
    rng = np.random.default_rng(seed)

    with progress_placeholder.container():
        st.info("ğŸ”„ è¨ˆç®—ã‚’é–‹å§‹ã—ã¾ã™...")
        progress_bar = st.progress(0)
        status_text = st.empty()

    status_text.text("ğŸ“ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºä¸­...")
    progress_bar.progress(10)
    keywords = extract_keywords(user_input, top_n=top_n)
    center_set = set(keywords)

    status_text.text("ğŸ”— å˜èªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ä¸­...")
    progress_bar.progress(30)
    network = build_word_network(keywords, GLOBAL_WORDS_DATABASE, n_total=n_total, rng=rng, jitter=jitter)

    status_text.text("ğŸŒ QUBOæœ€é©åŒ–ã§3Dé…ç½®ã‚’è¨ˆç®—ä¸­...")
    progress_bar.progress(50)

    def update_progress(current, total):
        p = 50 + int((current / total) * 40)
        progress_bar.progress(p)
        status_text.text(f"ğŸŒ QUBOæœ€é©åŒ–ä¸­... ({current}/{total} åå¾©)")

    pos = solve_qubo_placement(
        network["Q"],
        network["words"],
        network["center_indices"],
        network["energies"],
        rng=rng,
        n_iterations=qubo_iterations,
        progress_callback=update_progress,
    )

    # ä½ç½®ã‚†ã‚‰ãã¯å†è¨ˆç®—æ™‚ã ã‘
    if noise > 0:
        pos = pos + rng.normal(0, noise, size=pos.shape)

    progress_bar.progress(100)
    status_text.text("âœ… è¨ˆç®—å®Œäº†ï¼")
    time.sleep(0.08)
    progress_placeholder.empty()

    st.session_state["network"] = network
    st.session_state["pos"] = pos
    st.session_state["keywords"] = keywords
    st.session_state["center_set"] = center_set
    st.session_state["last_params_hash"] = params_hash

# åˆå› or å¤‰æ›´æ™‚ã®ã¿è¨ˆç®—ï¼ˆé€šå¸¸ã¯é™æ­¢ï¼‰
if (st.session_state["network"] is None) or needs_recalc:
    compute_all()

network = st.session_state["network"]
pos = st.session_state["pos"]
keywords = st.session_state["keywords"]
center_set = st.session_state["center_set"]
FAMOUS_QUOTES = st.session_state.get("famous_quotes") or list(BASE_FAMOUS_QUOTES)

if network is None or pos is None or len(network.get("words", [])) == 0:
    st.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒä¸å®Œå…¨ã§ã™ã€‚ã€ŒğŸ”„ å†è¨ˆç®—ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
    st.stop()


# ============================================================
# 10) Plotlyæç”»ï¼ˆæ˜Ÿå±‘ï¼‹ç¸ï¼‹çƒä½“ï¼‹ãƒ©ãƒ™ãƒ«ï¼‰
#     - ç·šã‚’é›†ç´„ã—ã¦è»½é‡åŒ–ï¼ˆNoneåŒºåˆ‡ã‚Šï¼‰
# ============================================================
fig = go.Figure()

# --- æ˜Ÿå±‘ï¼ˆç‚¹æ»…æ’é™¤ï¼šå®Œå…¨å›ºå®šï¼‰ ---
star_rng = np.random.default_rng(12345)
sx = star_rng.uniform(-3.2, 3.2, star_count)
sy = star_rng.uniform(-2.4, 2.4, star_count)
sz = star_rng.uniform(-2.0, 2.0, star_count)
alpha = np.full(star_count, 0.22, dtype=float)
star_size = star_rng.uniform(1.0, 2.4, star_count)
star_colors = [f"rgba(255,255,255,{a})" for a in alpha]

fig.add_trace(go.Scatter3d(
    x=sx, y=sy, z=sz,
    mode="markers",
    marker=dict(size=star_size, color=star_colors),
    hoverinfo="skip",
    showlegend=False
))

words = network["words"]
energies = network.get("energies", {})
center_indices = network.get("center_indices", [])
edges = network.get("edges", [])

# --- ç·šï¼šä¸­å¿ƒâ†’å‘¨è¾ºï¼ˆé›†ç´„ï¼‰ ---
xL, yL, zL, hoverL = [], [], [], []
for cidx in center_indices:
    if cidx >= len(words):
        continue
    cx, cy, cz = pos[cidx]
    cword = words[cidx]
    for i, w in enumerate(words):
        if i == cidx or i in center_indices:
            continue
        x, y, z = pos[i]
        e = energies.get(w, 0.0)
        d = float(np.linalg.norm(pos[i] - pos[cidx]))
        xL += [cx, x, None]
        yL += [cy, y, None]
        zL += [cz, z, None]
        hoverL += [f"{cword} â†’ {w}<br>è·é›¢:{d:.2f}<br>ã‚¨ãƒãƒ«ã‚®ãƒ¼:{e:.2f}", "", ""]

fig.add_trace(go.Scatter3d(
    x=xL, y=yL, z=zL,
    mode="lines",
    line=dict(width=2, color="rgba(150,200,255,0.35)"),
    hoverinfo="text",
    text=hoverL,
    showlegend=False
))

# --- ç·šï¼šå˜èªé–“ã‚¨ãƒƒã‚¸ï¼ˆé›†ç´„ï¼‰ ---
xE, yE, zE, hoverE = [], [], [], []
for i, j, e in edges:
    if i in center_indices or j in center_indices:
        continue
    x0, y0, z0 = pos[i]
    x1, y1, z1 = pos[j]
    d = float(np.linalg.norm(pos[j] - pos[i]))
    xE += [x0, x1, None]
    yE += [y0, y1, None]
    zE += [z0, z1, None]
    hoverE += [f"{words[i]} â†” {words[j]}<br>è·é›¢:{d:.2f}<br>ã‚¨ãƒãƒ«ã‚®ãƒ¼:{e:.2f}", "", ""]

fig.add_trace(go.Scatter3d(
    x=xE, y=yE, z=zE,
    mode="lines",
    line=dict(width=1, color="rgba(200,220,255,0.22)"),
    hoverinfo="text",
    text=hoverE,
    showlegend=False
))

# --- çƒä½“ï¼ˆè¨€è‘‰ï¼‰ + ãƒ©ãƒ™ãƒ«è‰²åˆ†ã‘ ---
sizes, colors, labels = [], [], []
for w in words:
    e = energies.get(w, 0.0)
    if w in center_set:
        sizes.append(28)
        colors.append("rgba(255,235,100,0.98)")
        labels.append(w)
    else:
        en = min(1.0, abs(e) / 3.0)
        sizes.append(12 + int(8 * en))
        if e < -1.5:
            colors.append("rgba(180,220,255,0.85)")
        elif e < -0.5:
            colors.append("rgba(220,240,255,0.75)")
        else:
            colors.append("rgba(255,255,255,0.60)")
        labels.append(w)

center_idx = [i for i, w in enumerate(labels) if w in center_set]
other_idx  = [i for i, w in enumerate(labels) if w not in center_set]

# â‘  ãã‚Œä»¥å¤–ï¼ˆç™½æ–‡å­—ï¼‰
if other_idx:
    oi = np.array(other_idx, dtype=int)
    fig.add_trace(go.Scatter3d(
        x=pos[oi, 0], y=pos[oi, 1], z=pos[oi, 2],
        mode="markers+text",
        text=[labels[i] for i in oi],
        textposition="top center",
        textfont=dict(size=18, color="rgba(255,255,255,1.0)"),
        marker=dict(
            size=[sizes[i] for i in oi],
            color=[colors[i] for i in oi],
            line=dict(width=1, color="rgba(0,0,0,0.10)")
        ),
        hovertemplate="<b>%{text}</b><extra></extra>",
        showlegend=False
    ))

# â‘¡ ä¸­å¿ƒèªï¼ˆèµ¤æ–‡å­—ï¼‰
if center_idx:
    ci = np.array(center_idx, dtype=int)
    fig.add_trace(go.Scatter3d(
        x=pos[ci, 0], y=pos[ci, 1], z=pos[ci, 2],
        mode="markers+text",
        text=[labels[i] for i in ci],
        textposition="top center",
        textfont=dict(size=24, color="rgba(255,80,80,1.0)"),
        marker=dict(
            size=[sizes[i] for i in ci],
            color=[colors[i] for i in ci],
            line=dict(width=2, color="rgba(255,80,80,0.8)")
        ),
        hovertemplate="<b>%{text}</b><br>ä¸­å¿ƒèª<extra></extra>",
        showlegend=False
    ))

# ä¸­å¿ƒèªã®â€œå…‰ã®å±¤â€ï¼ˆå›ºå®šï¼‰
for cidx in center_indices:
    if cidx >= len(words):
        continue
    cx, cy, cz = pos[cidx]
    for layer, mult in enumerate([1.0, 1.3, 1.6], 1):
        opacity = 0.15 / layer
        fig.add_trace(go.Scatter3d(
            x=[cx], y=[cy], z=[cz],
            mode="markers",
            marker=dict(size=[35 * mult], color=f"rgba(150,200,255,{opacity})", line=dict(width=0)),
            hoverinfo="skip",
            showlegend=False
        ))

fig.update_layout(
    paper_bgcolor="rgba(6,8,18,1)",
    scene=dict(
        xaxis=dict(visible=False),
        yaxis=dict(visible=False),
        zaxis=dict(visible=False),
        bgcolor="rgba(6,8,18,1)",
        camera=dict(
            eye=dict(x=1.6, y=1.15, z=1.05),
            center=dict(x=0, y=0, z=0),
            up=dict(x=0, y=1, z=0),
        ),
        dragmode="orbit",
    ),
    margin=dict(l=0, r=0, t=0, b=0),
)

plotly_config = {
    "displayModeBar": True,
    "scrollZoom": bool(enable_zoom),
    "displaylogo": False,
    "responsive": True,
    "toImageButtonOptions": {"format": "png", "filename": "quantum_oracle", "height": 800, "width": 1200, "scale": 1},
    "doubleClick": "reset",
}


# ============================================================
# 11) ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆå·¦ï¼šå®‡å®™ / å³ï¼šæ ¼è¨€+QUBOå¯è¦–åŒ–ï¼‰
# ============================================================
left, right = st.columns([2.0, 1.0], gap="large")

with left:
    st.plotly_chart(fig, use_container_width=True, config=plotly_config)
    st.caption("å˜èªï¼ˆçƒä½“ï¼‰ã¨ç¸ï¼ˆç·šï¼‰ã€‚ãƒã‚¦ã‚¹ã§å›è»¢ãƒ»ã‚ºãƒ¼ãƒ ã§ãã¾ã™ã€‚ï¼ˆç‚¹æ»…ãªã—ãƒ»é™æ­¢è¡¨ç¤ºï¼‰")

with right:
    st.markdown("<div class='card'>", unsafe_allow_html=True)

    st.markdown("### ğŸ“Š ç¾åœ¨ã®çŠ¶æ…‹")
    st.markdown(f"**æ ¸ï¼ˆæ¨å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰**ï¼š`{', '.join(keywords)}`")
    st.markdown(f"**è¨ˆç®—æ¸ˆã¿å˜èªæ•°**: {len(words)}èª")
    st.markdown(f"**æ¥ç¶šæ•°**: {len(edges)}æœ¬")
    if energies:
        mn = min(energies.values())
        mx = max(energies.values())
        st.markdown(f"**ã‚¨ãƒãƒ«ã‚®ãƒ¼ç¯„å›²**: {mn:.2f} ï½ {mx:.2f}")

    qm = st.session_state.get("quotes_meta", {})
    st.markdown(f"**æ ¼è¨€DB**: {len(FAMOUS_QUOTES)}ä»¶ï¼ˆExcelèª­è¾¼: {qm.get('loaded',0)} / {qm.get('source','')}ï¼‰")

    st.markdown("---")
    st.markdown("### ğŸ§  QUBOï¼ˆç¬¬ä¸‰è€…å‘ã‘èª¬æ˜ï¼‰")
    st.markdown(
        "- å„å˜èªã‚’ãƒãƒ¼ãƒ‰ã€å˜èªé–“ã®ç›¸äº’ä½œç”¨ã‚’ **Qè¡Œåˆ—** ã«ç½®ãã¾ã™ã€‚  \n"
        "- **ä¼¼ã¦ã„ã‚‹ã»ã©ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒä½ã„**ï¼ˆçµã³ã¤ãï¼‰ã‚ˆã†ã«è¨­è¨ˆã—ã¦ã„ã¾ã™ã€‚  \n"
        "- é…ç½®ã¯ Q ã«åŸºã¥ãç–‘ä¼¼æœ€é©åŒ–ã§ â€œç¸â€ ãŒå¼·ã„å˜èªåŒå£«ãŒè¿‘ã¥ãã‚ˆã†ã«èª¿æ•´ã—ã¾ã™ã€‚"
    )

    with st.expander("QUBOã®å½¢ï¼ˆæ¦‚å¿µï¼‰", expanded=False):
        st.latex(r"E(\mathbf{x})=\sum_i Q_{ii}x_i + \sum_{i<j} Q_{ij}x_i x_j")
        st.markdown("<div class='smallnote'>â€» æœ¬ã‚¢ãƒ—ãƒªã¯ã€ŒQç›¸äº’ä½œç”¨ã€ã‚’å¯è¦–åŒ–ãƒ»é…ç½®ã«åˆ©ç”¨ã—ã¦ã„ã¾ã™ï¼ˆç‚¹æ»…ãªã—ãƒ»é™æ­¢ï¼‰ã€‚</div>", unsafe_allow_html=True)

    # Qãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆå°ã•ã‚ï¼‰
    Q = network["Q"]
    hm = go.Figure(data=go.Heatmap(z=Q, showscale=True))
    hm.update_layout(margin=dict(l=0, r=0, t=0, b=0), height=220)
    st.plotly_chart(hm, use_container_width=True, config={"displayModeBar": False, "responsive": True})

    # å¼·ã„çµã³ã¤ãï¼ˆQãŒä½ã„ãƒšã‚¢ï¼‰
    pairs = []
    n = Q.shape[0]
    for i in range(n):
        for j in range(i + 1, n):
            pairs.append((Q[i, j], i, j))
    pairs.sort(key=lambda x: x[0])  # ä½ã„ã»ã©å¼·ã„
    top_pairs = pairs[:8]

    with st.expander("å¼·ã„çµã³ã¤ãï¼ˆQãŒä½ã„ãƒšã‚¢ï¼‰", expanded=False):
        for val, i, j in top_pairs:
            st.write(f"- {words[i]} â†” {words[j]} : Q={val:.2f}")

    st.markdown("---")
    st.markdown("### å…ˆäººã®ã“ã¨ã°")
    q = select_relevant_quote(keywords, FAMOUS_QUOTES)
    st.markdown(f"#### ã€Œ{q['quote']}ã€")
    st.markdown(f"**å‡ºæ‰€ï¼š** {q.get('source','â€”') if q.get('source') else 'â€”'}")
    if q.get("note"):
        st.markdown(f"<div class='smallnote'>â€» {q['note']}</div>", unsafe_allow_html=True)

    st.markdown("---")
    st.markdown("### ğŸ‘‰ æ°—ã«ãªã‚‹å˜èªã‹ã‚‰æ·±æ˜ã‚Š")
    default_word = keywords[0] if keywords else (words[0] if words else "")
    try:
        default_index = words.index(default_word) if default_word in words else 0
    except Exception:
        default_index = 0

    selected_word = st.selectbox("å˜èªã‚’é¸ã¶", options=words, index=default_index)
    cands = quote_candidates_for_word(selected_word, FAMOUS_QUOTES)

    if cands:
        st.markdown(f"**ã€Œ{selected_word}ã€ã«é–¢é€£ã™ã‚‹æ ¼è¨€å€™è£œ**")
        for qq in cands:
            st.markdown(
                f"- **{qq.get('quote','')}**  \n"
                f"  <span class='smallnote'>å‡ºæ‰€ï¼š{qq.get('source','â€”')}</span>",
                unsafe_allow_html=True
            )
    else:
        st.markdown("<div class='smallnote'>ã“ã®å˜èªã«ç›´æ¥ãƒ’ãƒƒãƒˆã™ã‚‹æ ¼è¨€ã¯æœªç™»éŒ²ã§ã™ï¼ˆExcelã®QUOTESã‚’å¢—ã‚„ã™ã¨å¼·åŒ–ã•ã‚Œã¾ã™ï¼‰ã€‚</div>", unsafe_allow_html=True)

    st.markdown("</div>", unsafe_allow_html=True)
