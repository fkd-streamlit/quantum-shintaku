# -*- coding: utf-8 -*-
# ============================================================
# app.py é‡å­ç¥è¨—ï¼ˆè©¦ä½œï¼‰â€” ç¸ã®çƒä½“ï¼ˆQUBO Ã— ã‚¢ãƒ¼ãƒˆï¼‰
#
# âœ… æ”¹å–„ç‚¹ï¼ˆä»Šå›ã®ä¸å…·åˆå¯¾å¿œï¼‹å¼·åŒ–ï¼‰
# - NameErrorå¯¾ç­–ï¼šUIå€¤ã¯ã™ã¹ã¦ session_state ã‹ã‚‰å–å¾—ï¼ˆkeyä»˜ä¸ï¼‰
# - Excelï¼šQUOTESãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆå…¥ã‚Š.xlsx ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦èª­ã¿è¾¼ã¿ï¼ˆpandas + openpyxlï¼‰
# - æ ¼è¨€å€™è£œãŒå‡ºãªã„å•é¡Œï¼š2-gramä¸€è‡´ã§ã€Œå¿…ãšå€™è£œTopNã€ã‚’è¿”ã™å¼·åŒ–ç‰ˆ
# - BGMï¼šassets/bgm.mp4 ã‚’ audio ã¨ã—ã¦å†ç”Ÿã—ã€ãƒ€ãƒ¡ãªã‚‰ video ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚è¡¨ç¤º
# - ç‚¹æ»…æ’é™¤ï¼šè‡ªå‹•æ›´æ–°ãªã—ã€æ˜Ÿå±‘å›ºå®šã€åŒæ¡ä»¶ãªã‚‰åŒé…ç½®ï¼ˆseedå›ºå®šï¼‰
# - Plotlyï¼šç·šãƒˆãƒ¬ãƒ¼ã‚¹ã‚’é›†ç´„ã—ã¦è»½é‡åŒ–ï¼ˆNoneåŒºåˆ‡ã‚Šï¼‰
# - ã€ŒQUBOã‚’ä½¿ã£ã¦ã„ã‚‹æ„Ÿã€ï¼šQè¡Œåˆ—ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼‹å¼·ã„çµã³ã¤ãä¸Šä½è¡¨ç¤º
# ============================================================

import os
import re
import io
import zlib
import time
import hashlib
import mimetypes
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import numpy as np
import plotly.graph_objects as go
import streamlit as st

# pandasï¼ˆExcelèª­ã¿è¾¼ã¿ç”¨ï¼‰
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except Exception:
    PANDAS_AVAILABLE = False
    pd = None


# ============================================================
# 0) ãƒšãƒ¼ã‚¸è¨­å®š + CSS
# ============================================================
st.set_page_config(page_title="é‡å­ç¥è¨— - ç¸ã®çƒä½“", layout="wide")

SPACE_CSS = """
<style>
.stApp{
  background:
    radial-gradient(circle at 18% 24%, rgba(110,150,255,0.12), transparent 38%),
    radial-gradient(circle at 78% 68%, rgba(255,160,220,0.08), transparent 44%),
    radial-gradient(circle at 50% 50%, rgba(255,255,255,0.03), transparent 55%),
    linear-gradient(180deg, rgba(6,8,18,1), rgba(10,12,26,1));
}
.block-container{ padding-top: 1.2rem; }
div[data-testid="stMarkdownContainer"] p,
div[data-testid="stMarkdownContainer"] li{
  font-family: "Hiragino Mincho ProN", "Yu Mincho", "Noto Serif JP", serif;
  letter-spacing: 0.02em;
  color: rgba(245,245,255,0.92);
}
h1,h2,h3{
  font-family: "Hiragino Mincho ProN", "Yu Mincho", "Noto Serif JP", serif !important;
  font-weight: 600 !important;
  color: rgba(245,245,255,0.95);
}
section[data-testid="stSidebar"]{
  background: rgba(255,255,255,0.08);
  border-right: 1px solid rgba(255,255,255,0.10);
  backdrop-filter: blur(10px);
}
div[data-testid="stPlotlyChart"] > div{
  position: relative;
  border-radius: 18px;
  overflow: hidden;
  box-shadow: 0 18px 60px rgba(0,0,0,0.30);
}
div[data-testid="stPlotlyChart"] > div::after{
  content:"";
  position:absolute;
  inset:0;
  background:
    radial-gradient(circle at 30% 25%, rgba(120,160,255,0.10), transparent 45%),
    radial-gradient(circle at 70% 65%, rgba(255,180,220,0.06), transparent 52%),
    radial-gradient(circle at 50% 50%, rgba(0,0,0,0.00), rgba(0,0,0,0.38));
  pointer-events:none;
}
.smallnote{opacity:0.78; font-size:0.92rem;}
.card{
  background: rgba(255,255,255,0.06);
  border: 1px solid rgba(255,255,255,0.10);
  border-radius: 18px;
  padding: 16px 16px 10px 16px;
  box-shadow: 0 18px 60px rgba(0,0,0,0.18);
}
</style>
"""
st.markdown(SPACE_CSS, unsafe_allow_html=True)


# ============================================================
# 0.5) ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ åˆæœŸåŒ–
# ============================================================
def init_session_state():
    defaults = {
        "bgm_on": False,
        "last_params_hash": "",
        "network": None,
        "pos": None,
        "keywords": [],
        "center_set": set(),
        "famous_quotes": None,          # Excelçµ±åˆå¾Œã®æ ¼è¨€DB
        "quotes_meta": {"loaded": 0, "source": "BASEã®ã¿"},
        # UI defaultsï¼ˆkeyä»˜ãï¼‰
        "user_input_text": "äººã¨ã®ä¼šè©±ã«ç–²ã‚ŒãŸã€‚å°‘ã—è¿·ã£ã¦ã„ã‚‹ã€‚",
        "top_n": 5,
        "n_total": 30,
        "noise": 0.06,
        "jitter": 0.10,
        "qubo_iterations": 80,
        "star_count": 900,
        "enable_zoom": True,
    }
    for k, v in defaults.items():
        if k not in st.session_state:
            st.session_state[k] = v

init_session_state()


# ============================================================
# 1) ã‚°ãƒ­ãƒ¼ãƒãƒ«å˜èªDBï¼ˆä»–ã®äººã®è¨€è‘‰ï¼‰
# ============================================================
GLOBAL_WORDS_DATABASE = [
    "ä¸–ç•Œå¹³å’Œ","è²¢çŒ®","æˆé•·","å­¦ã³","æŒ‘æˆ¦","å¤¢","å¸Œæœ›","æœªæ¥",
    "æ„Ÿè¬","æ„›","å¹¸ã›","å–œã³","å®‰å¿ƒ","å……å®Ÿ","æº€è¶³","å¹³å’Œ",
    "åŠªåŠ›","ç¶™ç¶š","å¿è€","èª å®Ÿ","æ­£ç›´","å„ªã—ã•","æ€ã„ã‚„ã‚Š","å…±æ„Ÿ",
    "èª¿å’Œ","ãƒãƒ©ãƒ³ã‚¹","è‡ªç„¶","ç¾","çœŸå®Ÿ","è‡ªç”±","æ­£ç¾©","é“",
    "çµ†","ã¤ãªãŒã‚Š","å®¶æ—","å‹äºº","ä»²é–“","ä¿¡é ¼","å°Šæ•¬","å”åŠ›",
    "ä»Š","ç¬é–“","éç¨‹","å¤‰åŒ–","é€²åŒ–","ç™ºå±•","å¾ªç’°","æµã‚Œ",
    "é™ã‘ã•","é›†ä¸­","è¦šæ‚Ÿ","æ±ºæ„","å‹‡æ°—","å¼·ã•","æŸ”è»Ÿæ€§","å¯›å®¹",
]

CATEGORIES = {
    "é¡˜ã„": ["ä¸–ç•Œå¹³å’Œ","è²¢çŒ®","æˆé•·","å¤¢","å¸Œæœ›","æœªæ¥"],
    "æ„Ÿæƒ…": ["æ„Ÿè¬","æ„›","å¹¸ã›","å–œã³","å®‰å¿ƒ","æº€è¶³","å¹³å’Œ"],
    "è¡Œå‹•": ["åŠªåŠ›","ç¶™ç¶š","å¿è€","èª å®Ÿ","æ­£ç›´"],
    "å“²å­¦": ["èª¿å’Œ","ãƒãƒ©ãƒ³ã‚¹","è‡ªç„¶","ç¾","é“","çœŸå®Ÿ","è‡ªç”±","æ­£ç¾©"],
    "é–¢ä¿‚": ["çµ†","ã¤ãªãŒã‚Š","å®¶æ—","å‹äºº","ä»²é–“","ä¿¡é ¼","å°Šæ•¬","å”åŠ›"],
    "å†…çš„": ["é™ã‘ã•","é›†ä¸­","è¦šæ‚Ÿ","æ±ºæ„","å‹‡æ°—","å¼·ã•","æŸ”è»Ÿæ€§","å¯›å®¹"],
    "æ™‚é–“": ["ä»Š","ç¬é–“","éç¨‹","å¤‰åŒ–","é€²åŒ–","ç™ºå±•","å¾ªç’°","æµã‚Œ"],
}


# ============================================================
# 2) æ ¼è¨€DBï¼ˆãƒ™ãƒ¼ã‚¹ + Excel QUOTESï¼‰
# ============================================================
BASE_FAMOUS_QUOTES = [
    {
        "keywords": ["å¹³å’Œ","ä¸–ç•Œ","è²¢çŒ®","å¸Œæœ›"],
        "quote": "é›ªã®ä¸‹ã§ç¨®ã¯æ˜¥ã‚’å¾…ã£ã¦ã„ã‚‹ã€‚ç„¦ã‚‹ã¹ã‹ã‚‰ãšã€æ™‚æº€ã¡ã‚‹ã‚’å¾…ã¦ã€‚",
        "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ/å¯“è©±èª¿",
        "note": "å…¬é–‹ç”¨ã«å…¸æ‹ ä»˜ãæ ¼è¨€ã¸å·®ã—æ›¿ãˆå¯"
    },
    {
        "keywords": ["æˆé•·","åŠªåŠ›","ç¶™ç¶š","æŒ‘æˆ¦"],
        "quote": "åƒé‡Œã®é“ã‚‚ä¸€æ­©ã‹ã‚‰ã€‚æ­©ã¿ã‚’æ­¢ã‚ãšã€ç¶šã‘ã‚‹ã“ã¨ã«æ„å‘³ãŒã‚ã‚‹ã€‚",
        "source": "æ•…äº‹æˆèªï¼ˆè¦å…¸æ‹ ç¢ºèªï¼‰â€”æš«å®š",
        "note": "çŸ­æ–‡åŒ–ã—ãŸå®šå‹å¥ã€‚å…¬é–‹å‰ã«å…¸æ‹ ç²¾æŸ»æ¨å¥¨"
    },
    {
        "keywords": ["æ„Ÿè¬","æ„›","çµ†","ã¤ãªãŒã‚Š"],
        "quote": "ä¸€æœŸä¸€ä¼šã€‚ä»Šã“ã®ç¬é–“ã‚’å¤§åˆ‡ã«ã€‚ã™ã¹ã¦ã¯ç¸ã§ç¹‹ãŒã£ã¦ã„ã‚‹ã€‚",
        "source": "ä¸€æœŸä¸€ä¼šï¼ˆèŒ¶é“æ€æƒ³ï¼‰ï¼‹é‡å­ç¥è¨— è©¦ä½œï¼ˆç·¨é›†/æ„è¨³ï¼‰",
        "note": ""
    },
    {
        "keywords": ["è‡ªç„¶","èª¿å’Œ","ãƒãƒ©ãƒ³ã‚¹","æµã‚Œ"],
        "quote": "æ°´ã¯ã€äº‰ã‚ãªã„ã€‚å½¢ã«ã“ã ã‚ã‚‰ãšã€æµã‚Œã‚‹ãŒã¾ã¾ã«ã€‚",
        "source": "è€å­ã€é“å¾³çµŒã€ï¼ˆä¸Šå–„è‹¥æ°´ï¼‰â€”æ„è¨³/ç·¨é›†",
        "note": "å³å¯†ãªåŸæ–‡å¼•ç”¨ã§ã¯ãªãæ„è¨³"
    },
    {
        "keywords": ["é™ã‘ã•","é›†ä¸­","ä»Š","ç¬é–“"],
        "quote": "æ­¢ã¾ã‚‹ã“ã¨ã§ã€æµã‚ŒãŒè¦‹ãˆã‚‹ã€‚å‹•ã®ä¸­ã«é™ãŒã‚ã‚‹ã€‚",
        "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ",
        "note": ""
    },
    {
        "keywords": ["å‹‡æ°—","æ±ºæ„","æŒ‘æˆ¦","é“"],
        "quote": "é“ãŒåˆ†ã‹ã‚Œã¦ã„ãŸã‚‰ã€å¿µã®ãªã„æ–¹ã¸è¡Œã‘ã€‚",
        "source": "å‡ºå…¸è¦ç¢ºèªï¼ˆæµé€šå¥ï¼‰â€”æš«å®š",
        "note": "å…¬é–‹å‰ã«å…¸æ‹ ã‚’ç¢ºå®šæ¨å¥¨"
    },
    {
        "keywords": ["æ€ã„ã‚„ã‚Š","å„ªã—ã•","å…±æ„Ÿ","ä¿¡é ¼"],
        "quote": "äººã®å¿ƒã«å¯„ã‚Šæ·»ã†ã€‚ãã‚ŒãŒçœŸã®å¼·ã•ã§ã‚ã‚‹ã€‚",
        "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ",
        "note": ""
    },
    {
        "keywords": ["å¤‰åŒ–","é€²åŒ–","ç™ºå±•","æœªæ¥"],
        "quote": "ç„¡ç‚ºã«ã—ã¦ç‚ºã™ã€‚å‹•ãã“ã¨ãŒé™ã§ã‚ã‚‹ã€‚",
        "source": "æ±æ´‹æ€æƒ³ï¼ˆç„¡ç‚ºè‡ªç„¶ï¼‰â€”æ„è¨³/ç·¨é›†",
        "note": ""
    },
    {
        "keywords": ["ç¾","çœŸå®Ÿ","è‡ªç„¶","èª¿å’Œ"],
        "quote": "é–“ã“ããŒç­”ãˆã§ã‚ã‚‹ã€‚ä½™ç™½ã«ã“ãæœ¬è³ªãŒã‚ã‚‹ã€‚",
        "source": "ç¾å­¦ï¼ˆé–“/ä½™ç™½ï¼‰ï¼‹é‡å­ç¥è¨— è©¦ä½œï¼ˆç·¨é›†ï¼‰",
        "note": ""
    },
    {
        "keywords": ["è‡ªç”±","æ­£ç¾©","é“","èª å®Ÿ"],
        "quote": "å·±ã«èª å®Ÿã§ã‚ã‚‹ã“ã¨ã€‚ãã‚ŒãŒè‡ªç”±ã¸ã®é“ã§ã‚ã‚‹ã€‚",
        "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ",
        "note": ""
    },
]

# åŒæ¢±Excelã‚’ä½¿ã†å ´åˆã®ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆä»»æ„ï¼‰
EXCEL_DEFAULT_PATH = "quantum_shintaku_pack_v3_with_sense_20260213_oposite_modify (2).xlsx"

def _hash_bytes(b: bytes) -> str:
    return hashlib.sha256(b).hexdigest()

@st.cache_data(show_spinner=False)
def load_quotes_from_excel_bytes(excel_bytes: bytes, file_hash: str) -> List[Dict]:
    """Excelãƒã‚¤ãƒŠãƒªã‹ã‚‰ QUOTES ã‚·ãƒ¼ãƒˆã‚’èª­ã‚€ï¼ˆfile_hashã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®‰å®šç”¨ï¼‰"""
    if not PANDAS_AVAILABLE or not excel_bytes:
        return []
    try:
        bio = io.BytesIO(excel_bytes)
        df = pd.read_excel(bio, sheet_name="QUOTES", engine="openpyxl")
    except Exception:
        return []

    quotes: List[Dict] = []

    def pick_text(row, candidates):
        for col in candidates:
            if col in df.columns:
                v = str(row.get(col, "")).strip()
                if v and v.lower() not in ("nan", "none"):
                    return v
        return ""

    for _, row in df.iterrows():
        quote_text = pick_text(row, ["æ ¼è¨€", "QUOTE", "Quote", "quote", "ãƒ†ã‚­ã‚¹ãƒˆ", "æ–‡", "è¨€è‘‰"])
        if not quote_text:
            continue

        kw_str = pick_text(row, ["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "KEYWORDS", "Keywords", "keywords", "ã‚¿ã‚°", "TAG", "Tag"])
        keywords = [k.strip() for k in kw_str.replace("ã€", ",").split(",") if k.strip()] if kw_str else []

        source = pick_text(row, ["å‡ºå…¸", "SOURCE", "Source", "source", "å‡ºæ‰€", "å…¸æ‹ ", "ä½œè€…"]) or "ä¼çµ±çš„ãªæ•™ãˆ"
        note   = pick_text(row, ["å‚™è€ƒ", "NOTE", "Note", "note", "æ³¨", "ãƒ¡ãƒ¢"])

        quotes.append({"quote": quote_text, "keywords": keywords, "source": source, "note": note})

    return quotes

def build_famous_quotes(excel_bytes: Optional[bytes] = None) -> Tuple[List[Dict], int]:
    """BASE +ï¼ˆä»»æ„ã§ï¼‰Excelã®QUOTESã‚’çµ±åˆ"""
    fam = list(BASE_FAMOUS_QUOTES)
    excel_quotes: List[Dict] = []

    if excel_bytes:
        h = _hash_bytes(excel_bytes)
        excel_quotes = load_quotes_from_excel_bytes(excel_bytes, file_hash=h)

    if excel_quotes:
        existing = {q.get("quote", "") for q in fam}
        for q in excel_quotes:
            qt = q.get("quote", "")
            if qt and qt not in existing:
                fam.append(q)
                existing.add(qt)

    return fam, len(excel_quotes)


# ============================================================
# 3) ãƒ†ã‚­ã‚¹ãƒˆâ†’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆç°¡æ˜“ï¼‰
# ============================================================
STOP_TOKENS = set([
    "ã—ãŸ","ãŸã„","ã„ã‚‹","ã„","ã“ã¨","ãã‚Œ","ã“ã‚Œ","ãŸã‚","ã‚ˆã†","ã®ã§","ã‹ã‚‰",
    "ã§ã™","ã¾ã™","ã‚ã‚‹","ãªã„","ãã—ã¦","ã§ã‚‚","ã—ã‹ã—","ã¾ãŸ",
    "è‡ªåˆ†","ç§","ã‚ãªãŸ","ã‚‚ã®","æ„Ÿã˜","æ°—æŒã¡","ä»Š","ä»Šæ—¥"
])

def extract_keywords(text: str, top_n: int = 5) -> List[str]:
    text = (text or "").strip()
    if not text:
        return ["é™ã‘ã•", "è¿·ã„"]

    # ã¾ãšã¯DBèªã®ç›´æ¥ãƒ’ãƒƒãƒˆï¼ˆå„ªå…ˆï¼‰
    found = [w for w in GLOBAL_WORDS_DATABASE if w in text]
    if found:
        return found[:top_n]

    text_clean = re.sub(r"[0-9ï¼-ï¼™ã€ã€‚ï¼,.!ï¼?ï¼Ÿ\(\)\[\]{}ã€Œã€ã€ã€\"'ï¼š:;ï¼/\\\n\r\t]+", " ", text)
    tokens = [t.strip() for t in re.split(r"\s+", text_clean) if t.strip()]
    tokens = [t for t in tokens if (len(t) >= 2 and t not in STOP_TOKENS)]

    if not tokens:
        return ["é™ã‘ã•", "è¿·ã„"]

    tokens = sorted(tokens, key=lambda s: (-len(s), s))
    return tokens[:top_n]


# ============================================================
# 4) â€œã‚¨ãƒãƒ«ã‚®ãƒ¼â€è¨ˆç®—ï¼ˆQUBOçš„ç›¸äº’ä½œç”¨ï¼‰
# ============================================================
def calculate_semantic_similarity(word1: str, word2: str) -> float:
    if word1 == word2:
        return 1.0

    common_chars = set(word1) & set(word2)
    char_sim = len(common_chars) / max(len(set(word1)), len(set(word2)), 1)

    category_sim = 0.0
    for _, ws in CATEGORIES.items():
        w1_in = word1 in ws
        w2_in = word2 in ws
        if w1_in and w2_in:
            category_sim = 1.0
            break
        elif w1_in or w2_in:
            category_sim = max(category_sim, 0.3)

    len_sim = 1.0 - abs(len(word1) - len(word2)) / max(len(word1), len(word2), 1)
    similarity = 0.4 * char_sim + 0.4 * category_sim + 0.2 * len_sim
    return float(np.clip(similarity, 0.0, 1.0))

def calculate_energy_between_words(word1: str, word2: str, rng: np.random.Generator, jitter: float) -> float:
    similarity = calculate_semantic_similarity(word1, word2)
    # ä¼¼ã¦ã‚‹ã»ã©ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒä½ã„ï¼ˆçµã³ã¤ãï¼‰
    energy = -2.0 * similarity + 0.5

    common = set(word1) & set(word2)
    if common:
        energy -= 0.20 * len(common) / max(len(word1), len(word2), 1)

    for _, ws in CATEGORIES.items():
        if (word1 in ws) and (word2 in ws):
            energy -= 0.60
            break

    if jitter > 0:
        energy += rng.normal(0, jitter)
    return float(energy)

def build_qubo_matrix_for_words(words: List[str], rng: np.random.Generator, jitter: float) -> np.ndarray:
    n = len(words)
    Q = np.zeros((n, n), dtype=float)
    np.fill_diagonal(Q, -0.5)
    for i in range(n):
        for j in range(i + 1, n):
            e = calculate_energy_between_words(words[i], words[j], rng, jitter)
            Q[i, j] = e
            Q[j, i] = e
    return Q

def solve_qubo_placement(
    Q: np.ndarray,
    words: List[str],
    center_indices: List[int],
    energies: Dict[str, float],
    rng: np.random.Generator,
    n_iterations: int = 100,
    progress_callback=None,
) -> np.ndarray:
    n = len(words)
    pos = np.zeros((n, 3), dtype=float)
    for idx in center_indices:
        if idx < n:
            pos[idx] = [0.0, 0.0, 0.0]

    ev = list(energies.values()) if energies else []
    if ev:
        mn, mx = min(ev), max(ev)
        er = (mx - mn) if mx != mn else 1.0
    else:
        mn, er = -3.0, 3.0

    golden_angle = np.pi * (3 - np.sqrt(5))
    k = 0
    for i in range(n):
        if i in center_indices:
            continue
        w = words[i]
        e = energies.get(w, 0.0)
        norm = (e - mn) / er if er > 0 else 0.5
        dist = 0.3 + (1.0 - norm) * 2.2

        theta = golden_angle * k
        y = 1 - (k / float(max(1, n - len(center_indices) - 1))) * 2
        r = np.sqrt(max(0.0, 1 - y * y))
        x = np.cos(theta) * r * dist
        z = np.sin(theta) * r * dist
        pos[i] = [x, y * dist * 0.6, z]
        k += 1

    for it in range(n_iterations):
        for i in range(n):
            if i in center_indices:
                continue
            force = np.zeros(3, dtype=float)

            # ä¸­å¿ƒã¨ã®è·é›¢ã‚’ä¿ã¤
            for cidx in center_indices:
                vec = pos[cidx] - pos[i]
                d = np.linalg.norm(vec)
                if d > 0.01:
                    w = words[i]
                    e = energies.get(w, 0.0)
                    norm = (e - mn) / er if er > 0 else 0.5
                    target = 0.3 + (1.0 - norm) * 2.2
                    if d < target * 0.9:
                        force -= vec / d * 0.05
                    elif d > target * 1.1:
                        force += vec / d * 0.10

            # Qç›¸äº’ä½œç”¨
            for j in range(n):
                if i == j or j in center_indices:
                    continue
                eij = Q[i, j]
                if eij < -0.3:
                    vec = pos[j] - pos[i]
                    d = np.linalg.norm(vec)
                    if d > 0.01:
                        force += vec / d * (abs(eij) * 0.08)
                elif eij > 0.2:
                    vec = pos[i] - pos[j]
                    d = np.linalg.norm(vec)
                    if d > 0.01:
                        force += vec / d * (abs(eij) * 0.03)

            pos[i] += force * 0.15

        if progress_callback:
            progress_callback(it + 1, n_iterations)

    return pos

def build_word_network(center_words: List[str], database: List[str], n_total: int,
                       rng: np.random.Generator, jitter: float) -> Dict:
    all_words = list(dict.fromkeys(center_words + database))  # é †åºç¶­æŒ
    energies: Dict[str, float] = {}

    for w in all_words:
        if w in center_words:
            energies[w] = -3.0
        else:
            e_list = [calculate_energy_between_words(c, w, rng, jitter) for c in center_words]
            energies[w] = float(np.mean(e_list))

    sorted_words = sorted(energies.items(), key=lambda x: x[1])  # ä½ã„ã»ã©ä¸­å¿ƒã«è¿‘ã„
    selected: List[str] = []
    for w, _ in sorted_words:
        if w in center_words and w not in selected:
            selected.append(w)
    for w, _ in sorted_words:
        if w not in selected:
            selected.append(w)
        if len(selected) >= n_total:
            break

    Q = build_qubo_matrix_for_words(selected, rng, jitter)
    center_indices = [i for i, w in enumerate(selected) if w in center_words]

    edges: List[Tuple[int, int, float]] = []
    n = len(selected)
    for i in range(n):
        for j in range(i + 1, n):
            e = Q[i, j]
            if e < -0.25:
                edges.append((i, j, float(e)))

    return {
        "words": selected,
        "energies": {w: energies[w] for w in selected},
        "edges": edges,
        "Q": Q,
        "center_indices": center_indices,
    }


# ============================================================
# 5) æ ¼è¨€æ¤œç´¢ï¼ˆå¼·åŒ–ç‰ˆï¼šå€™è£œã‚¼ãƒ­ã‚’é¿ã‘ã‚‹ï¼‰
# ============================================================
def select_relevant_quote(keywords: List[str], famous_quotes: List[Dict]) -> Dict[str, str]:
    if not keywords:
        keywords = ["ä»Š"]

    ks = set()
    for kw in keywords:
        k = kw.strip().lower()
        ks.add(k)
        if len(k) > 2:
            for i in range(len(k) - 1):
                ks.add(k[i:i+2])

    best = None
    best_score = -1.0

    for q in famous_quotes:
        qk = q.get("keywords", [])
        if not qk:
            continue

        qks = set()
        for k in qk:
            kk = str(k).strip().lower()
            qks.add(kk)
            if len(kk) > 2:
                for i in range(len(kk) - 1):
                    qks.add(kk[i:i+2])

        exact = len(ks & qks)

        partial = 0.0
        for a in ks:
            for b in qks:
                if a in b or b in a:
                    partial += 0.5

        text = (q.get("quote", "") or "").lower()
        text_match = 0.0
        for a in ks:
            if len(a) >= 2 and a in text:
                text_match += 0.3

        score = exact * 2.0 + partial + text_match
        if score > best_score:
            best_score = score
            best = q

    if best is None or best_score < 0.1:
        return {"quote": "ã‚ãªãŸã®è¦³æ¸¬ãŒã€ã“ã®ä¸–ç•Œç·šã‚’ç¢ºå®šã•ã›ã¾ã—ãŸã€‚", "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ", "note": ""}

    return {"quote": best.get("quote", ""), "source": best.get("source", "ä¼çµ±çš„ãªæ•™ãˆ"), "note": best.get("note", "")}

def _bigrams(s: str) -> set:
    s = (s or "").strip().lower()
    s = re.sub(r"\s+", "", s)
    if len(s) <= 1:
        return {s} if s else set()
    return {s[i:i+2] for i in range(len(s)-1)}

def quote_candidates_for_word(word: str, famous_quotes: List[Dict], max_n: int = 6) -> List[Dict]:
    """
    æ”¹å–„ç‰ˆï¼š
    - keywordsä¸€è‡´ãŒå¼±ãã¦ã‚‚ã€æœ¬æ–‡/keywordsã®2-gramä¸€è‡´ã§è¿‘ã„é †ã«å‡ºã™
    - ã€Œå€™è£œãªã—ã€ã‚’æ¥µåŠ›ãªãã™ï¼ˆå¸¸ã«TopNã‚’è¿”ã™ï¼‰
    """
    if not famous_quotes:
        return []
    w = (word or "").strip()
    if not w:
        return famous_quotes[:max_n]

    w2 = w.lower()
    w_bg = _bigrams(w2)

    scored = []
    for q in famous_quotes:
        quote = (q.get("quote") or "")
        ks = q.get("keywords") or []
        ks_join = " ".join([str(k) for k in ks])

        score = 0.0

        # 1) keywords ç›´æ¥ä¸€è‡´/éƒ¨åˆ†ä¸€è‡´
        ks_low = [str(k).strip().lower() for k in ks]
        if w2 in ks_low:
            score += 5.0
        else:
            for k in ks_low:
                if (w2 in k) or (k in w2):
                    score += 2.0

        # 2) 2-gramï¼ˆè¡¨è¨˜ã‚†ã‚Œå¯¾ç­–ï¼‰
        q_bg = _bigrams(quote) | _bigrams(ks_join)
        inter = len(w_bg & q_bg)
        if inter > 0:
            score += 0.6 * inter

        # 3) æ–‡å­—åˆ—ãã®ã¾ã¾å«ã¾ã‚Œã‚‹
        if w2 and (w2 in quote.lower()):
            score += 1.0

        scored.append((score, q))

    scored.sort(key=lambda x: (-x[0], (x[1].get("quote") or "")))
    return [q for _, q in scored[:max_n]]


# ============================================================
# 6) seedå›ºå®šï¼ˆåŒæ¡ä»¶ãªã‚‰åŒé…ç½®ï¼‰
# ============================================================
def make_seed(s: str) -> int:
    return int(zlib.adler32(s.encode("utf-8")) & 0xFFFFFFFF)


# ============================================================
# 7) UIï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼‰
# ============================================================
st.title("é‡å­ç¥è¨—ï¼ˆè©¦ä½œï¼‰â€” ç¸ã®çƒä½“ï¼ˆQUBO Ã— ã‚¢ãƒ¼ãƒˆï¼‰")

BGM_PATH = Path("assets/bgm.mp4")

with st.sidebar:
    st.markdown("### ğŸ“˜ æ ¼è¨€ãƒ‡ãƒ¼ã‚¿ï¼ˆExcel/QUOTESï¼‰")

    if not PANDAS_AVAILABLE:
        st.error("pandas ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€Excelèª­ã¿è¾¼ã¿ãŒç„¡åŠ¹ã§ã™ã€‚requirements.txt ã« pandas ã¨ openpyxl ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚")

    uploaded_excel = st.file_uploader(
        "QUOTESãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆå…¥ã‚ŠExcelã‚’é¸æŠ",
        type=["xlsx"],
        accept_multiple_files=False
    )

    excel_bytes: Optional[bytes] = None
    source_label = "BASEã®ã¿"

    if uploaded_excel is not None:
        excel_bytes = uploaded_excel.getvalue()
        source_label = "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"
        st.success("Excelã‚’èª­ã¿è¾¼ã¿å¯¾è±¡ã«è¨­å®šã—ã¾ã—ãŸï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼‰")
    else:
        if os.path.exists(EXCEL_DEFAULT_PATH):
            try:
                with open(EXCEL_DEFAULT_PATH, "rb") as f:
                    excel_bytes = f.read()
                source_label = f"åŒæ¢±: {EXCEL_DEFAULT_PATH}"
                st.info("Excelã‚’èª­ã¿è¾¼ã¿å¯¾è±¡ã«è¨­å®šã—ã¾ã—ãŸï¼ˆåŒæ¢±ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰")
            except Exception:
                excel_bytes = None
        else:
            st.caption("ExcelæœªæŒ‡å®šï¼šBASE_FAMOUS_QUOTESã®ã¿ã§å‹•ä½œã—ã¾ã™ã€‚")

    famous_quotes, excel_loaded = build_famous_quotes(excel_bytes)
    st.session_state["famous_quotes"] = famous_quotes
    st.session_state["quotes_meta"] = {"loaded": excel_loaded, "source": source_label}

    # ------------------------------
    # BGMï¼ˆmp4å†ç”Ÿ + ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    # ------------------------------
    st.markdown("---")
    st.markdown("### ğŸµ éŸ³æ¥½")

    st.session_state["bgm_on"] = st.toggle(
        "BGMã‚’å†ç”Ÿï¼ˆâ–¶ã‚’æŠ¼ã™ã¨é³´ã‚Šã¾ã™ï¼‰",
        value=st.session_state.get("bgm_on", False),
        key="bgm_on"
    )

    if st.session_state["bgm_on"]:
        if BGM_PATH.exists():
            bgm_bytes = BGM_PATH.read_bytes()
            mime, _ = mimetypes.guess_type(str(BGM_PATH))
            mime = mime or "audio/mp4"

            st.caption(f"å½¢å¼: {mime} / ã‚µã‚¤ã‚º: {len(bgm_bytes)/1024/1024:.2f} MB")

            try:
                st.audio(bgm_bytes, format=mime)
                if str(BGM_PATH).lower().endswith(".mp4"):
                    with st.expander("ğŸ” å†ç”Ÿã§ããªã„å ´åˆã¯ã“ã¡ã‚‰ï¼ˆvideoãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"):
                        st.video(bgm_bytes, format="video/mp4")
                st.caption("â€»ãƒ–ãƒ©ã‚¦ã‚¶åˆ¶é™ã«ã‚ˆã‚Šè‡ªå‹•å†ç”Ÿã¯ã§ãã¾ã›ã‚“ã€‚â–¶ ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
            except Exception as e:
                st.error(f"å†ç”Ÿã‚¨ãƒ©ãƒ¼: {e}")
                st.caption("mp4ã§é³´ã‚‰ãªã„å ´åˆã¯ mp3 å½¢å¼ãŒæœ€ã‚‚å®‰å®šã—ã¾ã™ã€‚")
        else:
            st.error(f"âš  BGMãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {BGM_PATH}ï¼ˆassets/bgm.mp4 ã‚’GitHubã«è¿½åŠ ã—ã¦ãã ã•ã„ï¼‰")

    # ------------------------------
    # å…¥åŠ›ï¼‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã™ã¹ã¦ key ä»˜ãï¼‰
    # ------------------------------
    st.markdown("---")
    st.markdown("### ä»Šã®æ°—æŒã¡ï¼ˆå…¥åŠ›ï¼‰")

    st.text_area(
        "çŸ­ã„ä¸€æ–‡ã§OKï¼ˆä¾‹ï¼šäººã¨ã®ä¼šè©±ã«ç–²ã‚ŒãŸã€‚å°‘ã—è¿·ã£ã¦ã„ã‚‹ã€‚ï¼‰",
        height=90,
        key="user_input_text",
    )

    st.markdown("---")
    st.markdown("### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")

    st.slider("æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°", 2, 10, st.session_state["top_n"], 1, key="top_n")
    st.slider("ç©ºé–“ã«å‡ºã™å˜èªæ•°ï¼ˆä¸­å¿ƒï¼‹å‘¨è¾ºï¼‰", 15, 60, st.session_state["n_total"], 1, key="n_total")
    st.caption("â€»ç‚¹æ»…é˜²æ­¢ã®ãŸã‚ã€è‡ªå‹•æ›´æ–°ï¼ˆã‚†ã‚‰ãï¼‰ã¯ç„¡åŠ¹åŒ–ã—ã¦ã„ã¾ã™ã€‚")
    st.slider("ä½ç½®ã®ã‚†ã‚‰ãï¼ˆå†è¨ˆç®—æ™‚ã®ã¿ï¼‰", 0.00, 0.20, float(st.session_state["noise"]), 0.01, key="noise")
    st.slider("ã‚¨ãƒãƒ«ã‚®ãƒ¼æºã‚‰ã", 0.00, 0.25, float(st.session_state["jitter"]), 0.01, key="jitter")
    st.slider("QUBOæœ€é©åŒ–ã®åå¾©å›æ•°", 50, 200, st.session_state["qubo_iterations"], 10, key="qubo_iterations")

    st.markdown("---")
    st.markdown("### å®‡å®™ã®å¯†åº¦")
    st.slider("æ˜Ÿå±‘ã®æ•°", 200, 2200, st.session_state["star_count"], 50, key="star_count")
    st.caption("â€»æ˜Ÿã®ã¾ãŸãŸãï¼ˆç‚¹æ»…ï¼‰ã¯ç„¡åŠ¹åŒ–ã—ã¦ã„ã¾ã™ã€‚")

    st.markdown("---")
    st.toggle("ãƒã‚¦ã‚¹ãƒ›ã‚¤ãƒ¼ãƒ«ã§ã‚ºãƒ¼ãƒ ", value=st.session_state["enable_zoom"], key="enable_zoom")

    if st.button("ğŸ”„ å†è¨ˆç®—", use_container_width=True):
        st.session_state["last_params_hash"] = ""  # å¼·åˆ¶å†è¨ˆç®—
        st.rerun()


# ============================================================
# 8) å†è¨ˆç®—åˆ¤å®šï¼ˆé™æ­¢è¡¨ç¤º / NameErrorå›é¿ï¼šsession_stateã‹ã‚‰å–å¾—ï¼‰
# ============================================================
quotes_meta = st.session_state.get("quotes_meta", {})
quotes_sig = f"{quotes_meta.get('source','')}_{quotes_meta.get('loaded',0)}"

params_hash = (
    f"{st.session_state.get('user_input_text','')}"
    f"|{st.session_state.get('top_n',5)}"
    f"|{st.session_state.get('n_total',30)}"
    f"|{st.session_state.get('noise',0.06)}"
    f"|{st.session_state.get('jitter',0.10)}"
    f"|{st.session_state.get('qubo_iterations',80)}"
    f"|{st.session_state.get('star_count',900)}"
    f"|{quotes_sig}"
)

needs_recalc = params_hash != st.session_state.get("last_params_hash", "")


# ============================================================
# 9) è¨ˆç®—ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ / é…ç½®ï¼‰
# ============================================================
def compute_all():
    progress_placeholder = st.empty()

    seed = make_seed(params_hash)
    rng = np.random.default_rng(seed)

    with progress_placeholder.container():
        st.info("ğŸ”„ è¨ˆç®—ã‚’é–‹å§‹ã—ã¾ã™...")
        progress_bar = st.progress(0)
        status_text = st.empty()

    user_input = st.session_state.get("user_input_text", "")
    top_n = int(st.session_state.get("top_n", 5))
    n_total = int(st.session_state.get("n_total", 30))
    jitter = float(st.session_state.get("jitter", 0.10))
    noise = float(st.session_state.get("noise", 0.06))
    iters = int(st.session_state.get("qubo_iterations", 80))

    status_text.text("ğŸ“ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºä¸­...")
    progress_bar.progress(10)
    keywords = extract_keywords(user_input, top_n=top_n)
    center_set = set(keywords)

    status_text.text("ğŸ”— å˜èªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ä¸­...")
    progress_bar.progress(30)
    network = build_word_network(keywords, GLOBAL_WORDS_DATABASE, n_total=n_total, rng=rng, jitter=jitter)

    status_text.text("ğŸŒ QUBOæœ€é©åŒ–ã§3Dé…ç½®ã‚’è¨ˆç®—ä¸­...")
    progress_bar.progress(50)

    def update_progress(current, total):
        p = 50 + int((current / total) * 40)
        progress_bar.progress(p)
        status_text.text(f"ğŸŒ QUBOæœ€é©åŒ–ä¸­... ({current}/{total} åå¾©)")

    pos = solve_qubo_placement(
        network["Q"],
        network["words"],
        network["center_indices"],
        network["energies"],
        rng=rng,
        n_iterations=iters,
        progress_callback=update_progress,
    )

    if noise > 0:
        pos = pos + rng.normal(0, noise, size=pos.shape)

    progress_bar.progress(100)
    status_text.text("âœ… è¨ˆç®—å®Œäº†ï¼")
    time.sleep(0.06)
    progress_placeholder.empty()

    st.session_state["network"] = network
    st.session_state["pos"] = pos
    st.session_state["keywords"] = keywords
    st.session_state["center_set"] = center_set
    st.session_state["last_params_hash"] = params_hash

if (st.session_state.get("network") is None) or needs_recalc:
    compute_all()

network = st.session_state.get("network")
pos = st.session_state.get("pos")
keywords = st.session_state.get("keywords", [])
center_set = st.session_state.get("center_set", set())
FAMOUS_QUOTES = st.session_state.get("famous_quotes") or list(BASE_FAMOUS_QUOTES)

if network is None or pos is None or len(network.get("words", [])) == 0:
    st.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒä¸å®Œå…¨ã§ã™ã€‚ã€ŒğŸ”„ å†è¨ˆç®—ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
    st.stop()


# ============================================================
# 10) Plotlyæç”»ï¼ˆæ˜Ÿå±‘ï¼‹ç¸ï¼‹çƒä½“ï¼‹ãƒ©ãƒ™ãƒ«ï¼‰
# ============================================================
fig = go.Figure()

# æ˜Ÿå±‘å›ºå®š
star_count = int(st.session_state.get("star_count", 900))
star_rng = np.random.default_rng(12345)
sx = star_rng.uniform(-3.2, 3.2, star_count)
sy = star_rng.uniform(-2.4, 2.4, star_count)
sz = star_rng.uniform(-2.0, 2.0, star_count)
alpha = np.full(star_count, 0.22, dtype=float)
star_size = star_rng.uniform(1.0, 2.4, star_count)
star_colors = [f"rgba(255,255,255,{a})" for a in alpha]

fig.add_trace(go.Scatter3d(
    x=sx, y=sy, z=sz,
    mode="markers",
    marker=dict(size=star_size, color=star_colors),
    hoverinfo="skip",
    showlegend=False
))

words = network["words"]
energies = network.get("energies", {})
center_indices = network.get("center_indices", [])
edges = network.get("edges", [])

# ç·šï¼šä¸­å¿ƒâ†’å‘¨è¾ºï¼ˆé›†ç´„ï¼‰
xL, yL, zL, hoverL = [], [], [], []
for cidx in center_indices:
    if cidx >= len(words):
        continue
    cx, cy, cz = pos[cidx]
    cword = words[cidx]
    for i, w in enumerate(words):
        if i == cidx or i in center_indices:
            continue
        x, y, z = pos[i]
        e = energies.get(w, 0.0)
        d = float(np.linalg.norm(pos[i] - pos[cidx]))
        xL += [cx, x, None]
        yL += [cy, y, None]
        zL += [cz, z, None]
        hoverL += [f"{cword} â†’ {w}<br>è·é›¢:{d:.2f}<br>ã‚¨ãƒãƒ«ã‚®ãƒ¼:{e:.2f}", "", ""]

fig.add_trace(go.Scatter3d(
    x=xL, y=yL, z=zL,
    mode="lines",
    line=dict(width=2, color="rgba(150,200,255,0.35)"),
    hoverinfo="text",
    text=hoverL,
    showlegend=False
))

# ç·šï¼šå˜èªé–“ã‚¨ãƒƒã‚¸ï¼ˆé›†ç´„ï¼‰
xE, yE, zE, hoverE = [], [], [], []
for i, j, e in edges:
    if i in center_indices or j in center_indices:
        continue
    x0, y0, z0 = pos[i]
    x1, y1, z1 = pos[j]
    d = float(np.linalg.norm(pos[j] - pos[i]))
    xE += [x0, x1, None]
    yE += [y0, y1, None]
    zE += [z0, z1, None]
    hoverE += [f"{words[i]} â†” {words[j]}<br>è·é›¢:{d:.2f}<br>ã‚¨ãƒãƒ«ã‚®ãƒ¼:{e:.2f}", "", ""]

fig.add_trace(go.Scatter3d(
    x=xE, y=yE, z=zE,
    mode="lines",
    line=dict(width=1, color="rgba(200,220,255,0.22)"),
    hoverinfo="text",
    text=hoverE,
    showlegend=False
))

# çƒä½“ï¼ˆè¨€è‘‰ï¼‰ + ãƒ©ãƒ™ãƒ«è‰²åˆ†ã‘
sizes, colors, labels = [], [], []
for w in words:
    e = energies.get(w, 0.0)
    if w in center_set:
        sizes.append(28)
        colors.append("rgba(255,235,100,0.98)")
        labels.append(w)
    else:
        en = min(1.0, abs(e) / 3.0)
        sizes.append(12 + int(8 * en))
        if e < -1.5:
            colors.append("rgba(180,220,255,0.85)")
        elif e < -0.5:
            colors.append("rgba(220,240,255,0.75)")
        else:
            colors.append("rgba(255,255,255,0.60)")
        labels.append(w)

center_idx = [i for i, w in enumerate(labels) if w in center_set]
other_idx  = [i for i, w in enumerate(labels) if w not in center_set]

if other_idx:
    oi = np.array(other_idx, dtype=int)
    fig.add_trace(go.Scatter3d(
        x=pos[oi, 0], y=pos[oi, 1], z=pos[oi, 2],
        mode="markers+text",
        text=[labels[i] for i in oi],
        textposition="top center",
        textfont=dict(size=18, color="rgba(255,255,255,1.0)"),
        marker=dict(
            size=[sizes[i] for i in oi],
            color=[colors[i] for i in oi],
            line=dict(width=1, color="rgba(0,0,0,0.10)")
        ),
        hovertemplate="<b>%{text}</b><extra></extra>",
        showlegend=False
    ))

if center_idx:
    ci = np.array(center_idx, dtype=int)
    fig.add_trace(go.Scatter3d(
        x=pos[ci, 0], y=pos[ci, 1], z=pos[ci, 2],
        mode="markers+text",
        text=[labels[i] for i in ci],
        textposition="top center",
        textfont=dict(size=24, color="rgba(255,80,80,1.0)"),
        marker=dict(
            size=[sizes[i] for i in ci],
            color=[colors[i] for i in ci],
            line=dict(width=2, color="rgba(255,80,80,0.8)")
        ),
        hovertemplate="<b>%{text}</b><br>ä¸­å¿ƒèª<extra></extra>",
        showlegend=False
    ))

# ä¸­å¿ƒèªã®å…‰ã®å±¤ï¼ˆå›ºå®šï¼‰
for cidx in center_indices:
    if cidx >= len(words):
        continue
    cx, cy, cz = pos[cidx]
    for layer, mult in enumerate([1.0, 1.3, 1.6], 1):
        opacity = 0.15 / layer
        fig.add_trace(go.Scatter3d(
            x=[cx], y=[cy], z=[cz],
            mode="markers",
            marker=dict(size=[35 * mult], color=f"rgba(150,200,255,{opacity})", line=dict(width=0)),
            hoverinfo="skip",
            showlegend=False
        ))

fig.update_layout(
    paper_bgcolor="rgba(6,8,18,1)",
    scene=dict(
        xaxis=dict(visible=False),
        yaxis=dict(visible=False),
        zaxis=dict(visible=False),
        bgcolor="rgba(6,8,18,1)",
        camera=dict(
            eye=dict(x=1.6, y=1.15, z=1.05),
            center=dict(x=0, y=0, z=0),
            up=dict(x=0, y=1, z=0),
        ),
        dragmode="orbit",
    ),
    margin=dict(l=0, r=0, t=0, b=0),
)

plotly_config = {
    "displayModeBar": True,
    "scrollZoom": bool(st.session_state.get("enable_zoom", True)),
    "displaylogo": False,
    "responsive": True,
    "toImageButtonOptions": {"format": "png", "filename": "quantum_oracle", "height": 800, "width": 1200, "scale": 1},
    "doubleClick": "reset",
}


# ============================================================
# 11) ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆå·¦ï¼šå®‡å®™ / å³ï¼šæ ¼è¨€+QUBOå¯è¦–åŒ–ï¼‰
# ============================================================
left, right = st.columns([2.0, 1.0], gap="large")

with left:
    st.plotly_chart(fig, use_container_width=True, config=plotly_config)
    st.caption("å˜èªï¼ˆçƒä½“ï¼‰ã¨ç¸ï¼ˆç·šï¼‰ã€‚ãƒã‚¦ã‚¹ã§å›è»¢ãƒ»ã‚ºãƒ¼ãƒ ã§ãã¾ã™ã€‚ï¼ˆç‚¹æ»…ãªã—ãƒ»é™æ­¢è¡¨ç¤ºï¼‰")

with right:
    st.markdown("<div class='card'>", unsafe_allow_html=True)

    st.markdown("### ğŸ“Š ç¾åœ¨ã®çŠ¶æ…‹")
    st.markdown(f"**æ ¸ï¼ˆæ¨å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰**ï¼š`{', '.join(keywords)}`")
    st.markdown(f"**è¨ˆç®—æ¸ˆã¿å˜èªæ•°**: {len(words)}èª")
    st.markdown(f"**æ¥ç¶šæ•°**: {len(edges)}æœ¬")
    if energies:
        mn = min(energies.values())
        mx = max(energies.values())
        st.markdown(f"**ã‚¨ãƒãƒ«ã‚®ãƒ¼ç¯„å›²**: {mn:.2f} ï½ {mx:.2f}")

    qm = st.session_state.get("quotes_meta", {})
    st.markdown(f"**æ ¼è¨€DB**: {len(FAMOUS_QUOTES)}ä»¶ï¼ˆExcelèª­è¾¼: {qm.get('loaded',0)} / {qm.get('source','')}ï¼‰")

    st.markdown("---")
    st.markdown("### ğŸ§  QUBOï¼ˆç¬¬ä¸‰è€…å‘ã‘èª¬æ˜ï¼‰")
    st.markdown(
        "- å„å˜èªã‚’ãƒãƒ¼ãƒ‰ã€å˜èªé–“ã®ç›¸äº’ä½œç”¨ã‚’ **Qè¡Œåˆ—** ã«ç½®ãã¾ã™ã€‚  \n"
        "- **ä¼¼ã¦ã„ã‚‹ã»ã©ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒä½ã„**ï¼ˆçµã³ã¤ãï¼‰ã‚ˆã†ã«è¨­è¨ˆã—ã¦ã„ã¾ã™ã€‚  \n"
        "- é…ç½®ã¯ Q ã«åŸºã¥ãç–‘ä¼¼æœ€é©åŒ–ã§ â€œç¸â€ ãŒå¼·ã„å˜èªåŒå£«ãŒè¿‘ã¥ãã‚ˆã†ã«èª¿æ•´ã—ã¾ã™ã€‚"
    )

    with st.expander("QUBOã®å½¢ï¼ˆæ¦‚å¿µï¼‰", expanded=False):
        st.latex(r"E(\mathbf{x})=\sum_i Q_{ii}x_i + \sum_{i<j} Q_{ij}x_i x_j")
        st.markdown("<div class='smallnote'>â€» æœ¬ã‚¢ãƒ—ãƒªã¯ã€ŒQç›¸äº’ä½œç”¨ã€ã‚’å¯è¦–åŒ–ãƒ»é…ç½®ã«åˆ©ç”¨ã—ã¦ã„ã¾ã™ï¼ˆç‚¹æ»…ãªã—ãƒ»é™æ­¢ï¼‰ã€‚</div>", unsafe_allow_html=True)

    # Qãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
    Q = network["Q"]
    hm = go.Figure(data=go.Heatmap(z=Q, showscale=True))
    hm.update_layout(margin=dict(l=0, r=0, t=0, b=0), height=220)
    st.plotly_chart(hm, use_container_width=True, config={"displayModeBar": False, "responsive": True})

    # å¼·ã„çµã³ã¤ã
    pairs = []
    n = Q.shape[0]
    for i in range(n):
        for j in range(i + 1, n):
            pairs.append((Q[i, j], i, j))
    pairs.sort(key=lambda x: x[0])  # ä½ã„ã»ã©å¼·ã„
    top_pairs = pairs[:8]

    with st.expander("å¼·ã„çµã³ã¤ãï¼ˆQãŒä½ã„ãƒšã‚¢ï¼‰", expanded=False):
        for val, i, j in top_pairs:
            st.write(f"- {words[i]} â†” {words[j]} : Q={val:.2f}")

    st.markdown("---")
    st.markdown("### å…ˆäººã®ã“ã¨ã°")
    q = select_relevant_quote(keywords, FAMOUS_QUOTES)
    st.markdown(f"#### ã€Œ{q['quote']}ã€")
    st.markdown(f"**å‡ºæ‰€ï¼š** {q.get('source','â€”') if q.get('source') else 'â€”'}")
    if q.get("note"):
        st.markdown(f"<div class='smallnote'>â€» {q['note']}</div>", unsafe_allow_html=True)

    st.markdown("---")
    st.markdown("### ğŸ‘‰ æ°—ã«ãªã‚‹å˜èªã‹ã‚‰æ·±æ˜ã‚Š")

    default_word = keywords[0] if keywords else (words[0] if words else "")
    default_index = 0
    if default_word in words:
        default_index = words.index(default_word)

    selected_word = st.selectbox("å˜èªã‚’é¸ã¶", options=words, index=default_index)
    cands = quote_candidates_for_word(selected_word, FAMOUS_QUOTES, max_n=6)

    st.markdown(f"**ã€Œ{selected_word}ã€ã«é–¢é€£ã™ã‚‹æ ¼è¨€å€™è£œ**")
    for qq in cands:
        qt = qq.get("quote", "")
        src = qq.get("source", "â€”")
        st.markdown(f"- **{qt}**  \n  <span class='smallnote'>å‡ºæ‰€ï¼š{src}</span>", unsafe_allow_html=True)

    st.markdown("</div>", unsafe_allow_html=True)
