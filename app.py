# -*- coding: utf-8 -*-
# ============================================================
# QUBO Ã— é‡å­ç¥è¨— UIï¼ˆStreamlit + Plotlyï¼‰
# ç›®çš„ï¼šç‚¹æ»…ï¼ˆãƒ•ãƒ©ãƒƒã‚·ãƒ¥ï¼‰ã‚’æ’é™¤ã—ã€é™çš„ã§è¦‹ã‚„ã™ã„è¡¨ç¤ºã¸
# - è‡ªå‹•æ›´æ–°ï¼ˆst_autorefreshï¼‰ã‚’å»ƒæ­¢
# - æ˜Ÿå±‘ã®ã¾ãŸãŸãã‚’å›ºå®šï¼ˆtime seedã‚’ä½¿ã‚ãªã„ï¼‰
# - ä½ç½®ã®ã‚†ã‚‰ãã¯å†è¨ˆç®—æ™‚ã®ã¿ï¼ˆé€šå¸¸ã¯é™æ­¢ï¼‰
# ============================================================

import os
import re
import time
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import plotly.graph_objects as go
import streamlit as st

# pandasï¼ˆExcelèª­ã¿è¾¼ã¿ç”¨ï¼‰
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except Exception:
    PANDAS_AVAILABLE = False
    pd = None

# ============================================================
# 0) ãƒšãƒ¼ã‚¸è¨­å®š + CSS
# ============================================================
st.set_page_config(page_title="é‡å­ç¥è¨— - ç¸ã®çƒä½“", layout="wide")

SPACE_CSS = """
<style>
.stApp{
  background:
    radial-gradient(circle at 18% 24%, rgba(110,150,255,0.12), transparent 38%),
    radial-gradient(circle at 78% 68%, rgba(255,160,220,0.08), transparent 44%),
    radial-gradient(circle at 50% 50%, rgba(255,255,255,0.03), transparent 55%),
    linear-gradient(180deg, rgba(6,8,18,1), rgba(10,12,26,1));
}
.block-container{ padding-top: 1.2rem; }
div[data-testid="stMarkdownContainer"] p,
div[data-testid="stMarkdownContainer"] li{
  font-family: "Hiragino Mincho ProN", "Yu Mincho", "Noto Serif JP", serif;
  letter-spacing: 0.02em;
  color: rgba(245,245,255,0.92);
}
h1,h2,h3{
  font-family: "Hiragino Mincho ProN", "Yu Mincho", "Noto Serif JP", serif !important;
  font-weight: 600 !important;
  color: rgba(245,245,255,0.95);
}
section[data-testid="stSidebar"]{
  background: rgba(255,255,255,0.08);
  border-right: 1px solid rgba(255,255,255,0.10);
  backdrop-filter: blur(10px);
}
div[data-testid="stPlotlyChart"] > div{
  position: relative;
  border-radius: 18px;
  overflow: hidden;
  box-shadow: 0 18px 60px rgba(0,0,0,0.30);
}
div[data-testid="stPlotlyChart"] > div::after{
  content:"";
  position:absolute;
  inset:0;
  background:
    radial-gradient(circle at 30% 25%, rgba(120,160,255,0.10), transparent 45%),
    radial-gradient(circle at 70% 65%, rgba(255,180,220,0.06), transparent 52%),
    radial-gradient(circle at 50% 50%, rgba(0,0,0,0.00), rgba(0,0,0,0.38));
  pointer-events:none;
}
</style>
"""
st.markdown(SPACE_CSS, unsafe_allow_html=True)

# ============================================================
# 0.5) ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ åˆæœŸåŒ–ï¼ˆè½ã¡ãªã„ãŸã‚ã®åŸºç›¤ï¼‰
# ============================================================
def init_session_state():
    defaults = {
        "bgm_on": True,
        "last_user_input": "",
        "last_params_hash": "",
        "network": None,
        "pos": None,
        "keywords": [],
        "center_set": set(),
    }
    for k, v in defaults.items():
        if k not in st.session_state:
            st.session_state[k] = v

init_session_state()

# ============================================================
# 0.6) BGMï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã¿ï¼‰
# ============================================================
from pathlib import Path
import streamlit as st

BGM_PATH = Path("assets/bgm.mp4")
BGM_FORMAT = "audio/mpeg"  # mp3ã¯ã“ã‚ŒãŒå®‰å®š

if "bgm_on" not in st.session_state:
    st.session_state["bgm_on"] = False  # åˆæœŸã¯OFFæ¨å¥¨ï¼ˆè‡ªå‹•å†ç”Ÿã¨èª¤è§£ã•ã‚Œã‚‹ãŸã‚ï¼‰

with st.sidebar:
    st.markdown("### ğŸµ éŸ³æ¥½")
    st.session_state["bgm_on"] = st.toggle("BGMã‚’å†ç”Ÿï¼ˆâ–¶ã‚’æŠ¼ã™ã¨é³´ã‚Šã¾ã™ï¼‰", value=st.session_state["bgm_on"])

    if st.session_state["bgm_on"]:
        if BGM_PATH.exists():
            audio_bytes = BGM_PATH.read_bytes()
            st.audio(audio_bytes, format=BGM_FORMAT)
            st.caption("â€»ãƒ–ãƒ©ã‚¦ã‚¶åˆ¶é™ã«ã‚ˆã‚Šè‡ªå‹•å†ç”Ÿã¯ã§ãã¾ã›ã‚“ã€‚â–¶ ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
        else:
            st.error(f"âš  BGMãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {BGM_PATH}ï¼ˆassets/bgm.mp3 ã‚’GitHubã«è¿½åŠ ã—ã¦ãã ã•ã„ï¼‰")

# ============================================================
# 1) ã‚°ãƒ­ãƒ¼ãƒãƒ«å˜èªDBï¼ˆä»–ã®äººã®è¨€è‘‰ï¼‰
# ============================================================
GLOBAL_WORDS_DATABASE = [
    "ä¸–ç•Œå¹³å’Œ","è²¢çŒ®","æˆé•·","å­¦ã³","æŒ‘æˆ¦","å¤¢","å¸Œæœ›","æœªæ¥",
    "æ„Ÿè¬","æ„›","å¹¸ã›","å–œã³","å®‰å¿ƒ","å……å®Ÿ","æº€è¶³","å¹³å’Œ",
    "åŠªåŠ›","ç¶™ç¶š","å¿è€","èª å®Ÿ","æ­£ç›´","å„ªã—ã•","æ€ã„ã‚„ã‚Š","å…±æ„Ÿ",
    "èª¿å’Œ","ãƒãƒ©ãƒ³ã‚¹","è‡ªç„¶","ç¾","çœŸå®Ÿ","è‡ªç”±","æ­£ç¾©","é“",
    "çµ†","ã¤ãªãŒã‚Š","å®¶æ—","å‹äºº","ä»²é–“","ä¿¡é ¼","å°Šæ•¬","å”åŠ›",
    "ä»Š","ç¬é–“","éç¨‹","å¤‰åŒ–","é€²åŒ–","ç™ºå±•","å¾ªç’°","æµã‚Œ",
    "é™ã‘ã•","é›†ä¸­","è¦šæ‚Ÿ","æ±ºæ„","å‹‡æ°—","å¼·ã•","æŸ”è»Ÿæ€§","å¯›å®¹",
]

# ============================================================
# 2) æ ¼è¨€DBï¼ˆå‡ºæ‰€ã‚‚æŒãŸã›ã‚‹ï¼‰
# ============================================================
BASE_FAMOUS_QUOTES = [
    {
        "keywords": ["å¹³å’Œ","ä¸–ç•Œ","è²¢çŒ®","å¸Œæœ›"],
        "quote": "é›ªã®ä¸‹ã§ç¨®ã¯æ˜¥ã‚’å¾…ã£ã¦ã„ã‚‹ã€‚ç„¦ã‚‹ã¹ã‹ã‚‰ãšã€æ™‚æº€ã¡ã‚‹ã‚’å¾…ã¦ã€‚",
        "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ/å¯“è©±èª¿",
        "note": "å…¬é–‹ç”¨ã«å…¸æ‹ ä»˜ãæ ¼è¨€ã¸å·®ã—æ›¿ãˆå¯"
    },
    {
        "keywords": ["æˆé•·","åŠªåŠ›","ç¶™ç¶š","æŒ‘æˆ¦"],
        "quote": "åƒé‡Œã®é“ã‚‚ä¸€æ­©ã‹ã‚‰ã€‚æ­©ã¿ã‚’æ­¢ã‚ãšã€ç¶šã‘ã‚‹ã“ã¨ã«æ„å‘³ãŒã‚ã‚‹ã€‚",
        "source": "æ•…äº‹æˆèªï¼ˆè€å­/è€å­ç­‰ã«é¡ã™ã‚‹è¡¨ç¾ã¨ã—ã¦æµé€šï¼‰â€”è¦å…¸æ‹ ç¢ºèª",
        "note": "çŸ­æ–‡åŒ–ã—ãŸå®šå‹å¥ã€‚å…¬é–‹å‰ã«å…¸æ‹ ç²¾æŸ»æ¨å¥¨"
    },
    {
        "keywords": ["æ„Ÿè¬","æ„›","çµ†","ã¤ãªãŒã‚Š"],
        "quote": "ä¸€æœŸä¸€ä¼šã€‚ä»Šã“ã®ç¬é–“ã‚’å¤§åˆ‡ã«ã€‚ã™ã¹ã¦ã¯ç¸ã§ç¹‹ãŒã£ã¦ã„ã‚‹ã€‚",
        "source": "ä¸€æœŸä¸€ä¼šï¼ˆèŒ¶é“æ€æƒ³ï¼‰ï¼‹é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”ç·¨é›†/æ„è¨³",
        "note": ""
    },
    {
        "keywords": ["è‡ªç„¶","èª¿å’Œ","ãƒãƒ©ãƒ³ã‚¹","æµã‚Œ"],
        "quote": "æ°´ã¯ã€äº‰ã‚ãªã„ã€‚å½¢ã«ã“ã ã‚ã‚‰ãšã€æµã‚Œã‚‹ãŒã¾ã¾ã«ã€‚",
        "source": "è€å­ã€é“å¾³çµŒã€ï¼ˆä¸Šå–„è‹¥æ°´ï¼‰â€”æ„è¨³/ç·¨é›†",
        "note": "å³å¯†ãªåŸæ–‡å¼•ç”¨ã§ã¯ãªãæ„è¨³"
    },
    {
        "keywords": ["é™ã‘ã•","é›†ä¸­","ä»Š","ç¬é–“"],
        "quote": "æ­¢ã¾ã‚‹ã“ã¨ã§ã€æµã‚ŒãŒè¦‹ãˆã‚‹ã€‚å‹•ã®ä¸­ã«é™ãŒã‚ã‚‹ã€‚",
        "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ",
        "note": ""
    },
    {
        "keywords": ["å‹‡æ°—","æ±ºæ„","æŒ‘æˆ¦","é“"],
        "quote": "é“ãŒåˆ†ã‹ã‚Œã¦ã„ãŸã‚‰ã€å¿µã®ãªã„æ–¹ã¸è¡Œã‘ã€‚",
        "source": "å‡ºå…¸è¦ç¢ºèªï¼ˆæµé€šå¥ï¼‰â€”æš«å®š",
        "note": "å…¬é–‹å‰ã«å…¸æ‹ ã‚’ç¢ºå®šæ¨å¥¨"
    },
    {
        "keywords": ["æ€ã„ã‚„ã‚Š","å„ªã—ã•","å…±æ„Ÿ","ä¿¡é ¼"],
        "quote": "äººã®å¿ƒã«å¯„ã‚Šæ·»ã†ã€‚ãã‚ŒãŒçœŸã®å¼·ã•ã§ã‚ã‚‹ã€‚",
        "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ",
        "note": ""
    },
    {
        "keywords": ["å¤‰åŒ–","é€²åŒ–","ç™ºå±•","æœªæ¥"],
        "quote": "ç„¡ç‚ºã«ã—ã¦ç‚ºã™ã€‚å‹•ãã“ã¨ãŒé™ã§ã‚ã‚‹ã€‚",
        "source": "æ±æ´‹æ€æƒ³ï¼ˆç„¡ç‚ºè‡ªç„¶ï¼‰â€”æ„è¨³/ç·¨é›†",
        "note": ""
    },
    {
        "keywords": ["ç¾","çœŸå®Ÿ","è‡ªç„¶","èª¿å’Œ"],
        "quote": "é–“ã“ããŒç­”ãˆã§ã‚ã‚‹ã€‚ä½™ç™½ã«ã“ãæœ¬è³ªãŒã‚ã‚‹ã€‚",
        "source": "ç¾å­¦ï¼ˆé–“/ä½™ç™½ï¼‰ï¼‹é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”ç·¨é›†",
        "note": ""
    },
    {
        "keywords": ["è‡ªç”±","æ­£ç¾©","é“","èª å®Ÿ"],
        "quote": "å·±ã«èª å®Ÿã§ã‚ã‚‹ã“ã¨ã€‚ãã‚ŒãŒè‡ªç”±ã¸ã®é“ã§ã‚ã‚‹ã€‚",
        "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ",
        "note": ""
    },
]

EXCEL_DEFAULT = "quantum_shintaku_pack_v3_with_sense_20260213_oposite_modify_with_lr022101.xlsx"

@st.cache_data(show_spinner=False)
def load_quotes_from_excel_cached(excel_path: str) -> List[Dict]:
    if not PANDAS_AVAILABLE:
        return []
    if not excel_path or not os.path.exists(excel_path):
        return []
    try:
        df = pd.read_excel(excel_path, sheet_name="QUOTES", engine="openpyxl")
    except Exception:
        return []

    quotes: List[Dict] = []

    def pick_text(row, candidates):
        for col in candidates:
            if col in df.columns:
                v = str(row.get(col, "")).strip()
                if v and v.lower() not in ("nan", "none"):
                    return v
        return ""

    for _, row in df.iterrows():
        quote_text = pick_text(row, ["æ ¼è¨€", "QUOTE", "Quote", "quote", "ãƒ†ã‚­ã‚¹ãƒˆ", "æ–‡", "è¨€è‘‰"])
        if not quote_text:
            continue

        kw_str = pick_text(row, ["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "KEYWORDS", "Keywords", "keywords", "ã‚¿ã‚°", "TAG", "Tag"])
        keywords = [k.strip() for k in kw_str.replace("ã€", ",").split(",") if k.strip()] if kw_str else []

        source = pick_text(row, ["å‡ºå…¸", "SOURCE", "Source", "source", "å‡ºæ‰€", "å…¸æ‹ ", "ä½œè€…"]) or "ä¼çµ±çš„ãªæ•™ãˆ"
        note = pick_text(row, ["å‚™è€ƒ", "NOTE", "Note", "note", "æ³¨", "ãƒ¡ãƒ¢"])

        quotes.append({"quote": quote_text, "keywords": keywords, "source": source, "note": note})

    return quotes

def build_famous_quotes() -> List[Dict]:
    fam = list(BASE_FAMOUS_QUOTES)
    excel_quotes = load_quotes_from_excel_cached(EXCEL_DEFAULT)
    if excel_quotes:
        existing = {q.get("quote", "") for q in fam}
        for q in excel_quotes:
            qt = q.get("quote", "")
            if qt and qt not in existing:
                fam.append(q)
                existing.add(qt)
    return fam

FAMOUS_QUOTES = build_famous_quotes()

# ============================================================
# 3) ãƒ†ã‚­ã‚¹ãƒˆâ†’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆç°¡æ˜“ï¼‰
# ============================================================
def extract_keywords(text: str, top_n: int = 5) -> List[str]:
    text = (text or "").strip()
    text_clean = re.sub(r"[0-9ï¼-ï¼™\W]+", " ", text)

    found = [w for w in GLOBAL_WORDS_DATABASE if w in text_clean]
    if found:
        return found[:top_n]

    tokens = [t for t in text_clean.split() if len(t) >= 2]
    if not tokens:
        return ["é™ã‘ã•", "è¿·ã„"]
    return tokens[:top_n]

# ============================================================
# 4) â€œã‚¨ãƒãƒ«ã‚®ãƒ¼â€è¨ˆç®—ï¼ˆQUBOçš„ç›¸äº’ä½œç”¨ï¼‰
# ============================================================
CATEGORIES = {
    "é¡˜ã„": ["ä¸–ç•Œå¹³å’Œ","è²¢çŒ®","æˆé•·","å¤¢","å¸Œæœ›","æœªæ¥"],
    "æ„Ÿæƒ…": ["æ„Ÿè¬","æ„›","å¹¸ã›","å–œã³","å®‰å¿ƒ","æº€è¶³","å¹³å’Œ"],
    "è¡Œå‹•": ["åŠªåŠ›","ç¶™ç¶š","å¿è€","èª å®Ÿ","æ­£ç›´"],
    "å“²å­¦": ["èª¿å’Œ","ãƒãƒ©ãƒ³ã‚¹","è‡ªç„¶","ç¾","é“","çœŸå®Ÿ","è‡ªç”±","æ­£ç¾©"],
    "é–¢ä¿‚": ["çµ†","ã¤ãªãŒã‚Š","å®¶æ—","å‹äºº","ä»²é–“","ä¿¡é ¼","å°Šæ•¬","å”åŠ›"],
    "å†…çš„": ["é™ã‘ã•","é›†ä¸­","è¦šæ‚Ÿ","æ±ºæ„","å‹‡æ°—","å¼·ã•","æŸ”è»Ÿæ€§","å¯›å®¹"],
    "æ™‚é–“": ["ä»Š","ç¬é–“","éç¨‹","å¤‰åŒ–","é€²åŒ–","ç™ºå±•","å¾ªç’°","æµã‚Œ"],
}

def calculate_semantic_similarity(word1: str, word2: str) -> float:
    if word1 == word2:
        return 1.0
    common_chars = set(word1) & set(word2)
    char_sim = len(common_chars) / max(len(set(word1)), len(set(word2)), 1)

    category_sim = 0.0
    for _, ws in CATEGORIES.items():
        w1_in = word1 in ws
        w2_in = word2 in ws
        if w1_in and w2_in:
            category_sim = 1.0
            break
        elif w1_in or w2_in:
            category_sim = 0.3

    len_sim = 1.0 - abs(len(word1) - len(word2)) / max(len(word1), len(word2), 1)
    similarity = 0.4 * char_sim + 0.4 * category_sim + 0.2 * len_sim
    return float(np.clip(similarity, 0.0, 1.0))

def calculate_energy_between_words(word1: str, word2: str, rng: np.random.Generator, jitter: float) -> float:
    similarity = calculate_semantic_similarity(word1, word2)
    energy = -2.0 * similarity + 0.5

    common = set(word1) & set(word2)
    if common:
        energy -= 0.20 * len(common) / max(len(word1), len(word2), 1)

    for _, ws in CATEGORIES.items():
        if (word1 in ws) and (word2 in ws):
            energy -= 0.60
            break

    energy += rng.normal(0, jitter)
    return float(energy)

def build_qubo_matrix_for_words(words: List[str], rng: np.random.Generator, jitter: float) -> Dict[Tuple[int, int], float]:
    n = len(words)
    Q: Dict[Tuple[int, int], float] = {}
    for i in range(n):
        Q[(i, i)] = -0.5
    for i in range(n):
        for j in range(i + 1, n):
            e = calculate_energy_between_words(words[i], words[j], rng, jitter)
            Q[(i, j)] = e
            Q[(j, i)] = e
    return Q

def solve_qubo_placement(
    Q: Dict[Tuple[int, int], float],
    n_words: int,
    center_indices: List[int],
    rng: np.random.Generator,
    n_iterations: int = 100,
    progress_callback=None,
    energies_dict: Dict[str, float] | None = None,
    words_list: List[str] | None = None,
) -> np.ndarray:
    pos = np.zeros((n_words, 3), dtype=float)
    for idx in center_indices:
        if idx < n_words:
            pos[idx] = [0.0, 0.0, 0.0]

    energies_dict = energies_dict or {}
    words_list = words_list or []

    energy_values = list(energies_dict.values()) if energies_dict else []
    if energy_values:
        min_energy = min(energy_values)
        max_energy = max(energy_values)
        energy_range = max_energy - min_energy if max_energy != min_energy else 1.0
    else:
        min_energy = -3.0
        energy_range = 3.0

    golden_angle = np.pi * (3 - np.sqrt(5))
    word_idx = 0

    for i in range(n_words):
        if i in center_indices:
            continue

        if i < len(words_list):
            w = words_list[i]
            e = energies_dict.get(w, 0.0)
        else:
            e = 0.0

        normalized = (e - min_energy) / energy_range if energy_range > 0 else 0.5
        distance = 0.3 + (1.0 - normalized) * 2.2

        theta = golden_angle * word_idx
        y = 1 - (word_idx / float(max(1, n_words - len(center_indices) - 1))) * 2
        radius_at_y = np.sqrt(max(0.0, 1 - y * y))

        x = np.cos(theta) * radius_at_y * distance
        z = np.sin(theta) * radius_at_y * distance
        pos[i] = [x, y * distance * 0.6, z]
        word_idx += 1

    for it in range(n_iterations):
        for i in range(n_words):
            if i in center_indices:
                continue

            force = np.zeros(3, dtype=float)

            for cidx in center_indices:
                vec = pos[cidx] - pos[i]
                dist = np.linalg.norm(vec)
                if dist > 0.01:
                    if i < len(words_list):
                        w = words_list[i]
                        e = energies_dict.get(w, 0.0)
                    else:
                        e = 0.0
                    target = 0.3 + (1.0 - (e - min_energy) / energy_range) * 2.2 if energy_range > 0 else 1.5

                    if dist < target * 0.9:
                        force -= vec / dist * 0.05
                    elif dist > target * 1.1:
                        force += vec / dist * 0.1

            for j in range(n_words):
                if i == j or j in center_indices:
                    continue
                eij = Q.get((i, j), 0.0)
                if eij < -0.3:
                    vec = pos[j] - pos[i]
                    dist = np.linalg.norm(vec)
                    if dist > 0.01:
                        force += vec / dist * (abs(eij) * 0.08)
                elif eij > 0.2:
                    vec = pos[i] - pos[j]
                    dist = np.linalg.norm(vec)
                    if dist > 0.01:
                        force += vec / dist * (abs(eij) * 0.03)

            pos[i] += force * 0.15

        if progress_callback:
            progress_callback(it + 1, n_iterations)

    return pos

def build_word_network(center_words: List[str], database: List[str], n_total: int,
                       rng: np.random.Generator, jitter: float) -> Dict:
    all_words = list(set(center_words + database))
    energies = {}

    for w in all_words:
        if w in center_words:
            energies[w] = -3.0
        else:
            e_list = [calculate_energy_between_words(c, w, rng, jitter) for c in center_words]
            energies[w] = float(np.mean(e_list))

    sorted_words = sorted(energies.items(), key=lambda x: x[1])

    selected = []
    for w, _ in sorted_words:
        if w in center_words:
            selected.append(w)
    for w, _ in sorted_words:
        if w not in selected:
            selected.append(w)
        if len(selected) >= n_total:
            break

    Q = build_qubo_matrix_for_words(selected, rng, jitter)

    edges = []
    center_indices = [i for i, w in enumerate(selected) if w in center_words]
    for i in range(len(selected)):
        for j in range(i + 1, len(selected)):
            e = Q.get((i, j), 0.0)
            if e < -0.25:
                edges.append((i, j, e))

    return {
        "words": selected,
        "energies": {w: energies[w] for w in selected},
        "edges": edges,
        "qubo_matrix": Q,
        "center_indices": center_indices
    }

def place_words_3d(words: List[str], center_set: set, rng: np.random.Generator,
                   noise: float, network: Dict, n_iterations: int,
                   progress_callback=None) -> np.ndarray:
    n = len(words)
    Q = network["qubo_matrix"]
    center_indices = network["center_indices"]
    energies_dict = network.get("energies", {})
    pos = solve_qubo_placement(
        Q, n, center_indices, rng,
        n_iterations=n_iterations,
        progress_callback=progress_callback,
        energies_dict=energies_dict,
        words_list=words
    )
    # ä½ç½®ã‚†ã‚‰ãã¯ã€Œå†è¨ˆç®—æ™‚ã ã‘ã€åŠ ãˆã‚‹ï¼ˆé€šå¸¸è¡¨ç¤ºã¯é™æ­¢ï¼‰
    if noise > 0:
        pos += rng.normal(0, noise, size=pos.shape)
    return pos

# ============================================================
# 6) æ ¼è¨€é¸æŠï¼ˆå‡ºæ‰€ã¤ãï¼‰
# ============================================================
def select_relevant_quote(keywords: List[str]) -> Dict[str, str]:
    if not keywords:
        keywords = ["ä»Š"]

    ks = set()
    for kw in keywords:
        k = kw.strip().lower()
        ks.add(k)
        if len(k) > 2:
            for i in range(len(k) - 1):
                ks.add(k[i:i+2])

    best = None
    best_score = -1.0

    for q in FAMOUS_QUOTES:
        qk = q.get("keywords", [])
        if not qk:
            continue

        qks = set()
        for k in qk:
            kk = k.strip().lower()
            qks.add(kk)
            if len(kk) > 2:
                for i in range(len(kk) - 1):
                    qks.add(kk[i:i+2])

        exact = len(ks & qks)

        partial = 0.0
        for a in ks:
            for b in qks:
                if a in b or b in a:
                    partial += 0.5

        text = q.get("quote", "").lower()
        text_match = 0.0
        for a in ks:
            if len(a) >= 2 and a in text:
                text_match += 0.3

        score = exact * 2.0 + partial + text_match
        if score > best_score:
            best_score = score
            best = q

    if best is None or best_score < 0.1:
        return {"quote": "ã‚ãªãŸã®è¦³æ¸¬ãŒã€ã“ã®ä¸–ç•Œç·šã‚’ç¢ºå®šã•ã›ã¾ã—ãŸã€‚", "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ", "note": ""}

    return {"quote": best.get("quote", ""), "source": best.get("source", "ä¼çµ±çš„ãªæ•™ãˆ"), "note": best.get("note", "")}

# ============================================================
# 7) UIï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼‰
# ============================================================
st.title("é‡å­ç¥è¨—ï¼ˆè©¦ä½œï¼‰â€” ç¸ã®çƒä½“ï¼ˆQUBO Ã— ã‚¢ãƒ¼ãƒˆï¼‰")

with st.sidebar:
    st.markdown("### ä»Šã®æ°—æŒã¡ï¼ˆå…¥åŠ›ï¼‰")
    user_input = st.text_area(
        "çŸ­ã„ä¸€æ–‡ã§OKï¼ˆä¾‹ï¼šäººã¨ã®ä¼šè©±ã«ç–²ã‚ŒãŸã€‚å°‘ã—è¿·ã£ã¦ã„ã‚‹ã€‚ï¼‰",
        value="äººã¨ã®ä¼šè©±ã«ç–²ã‚ŒãŸã€‚å°‘ã—è¿·ã£ã¦ã„ã‚‹ã€‚",
        height=90,
        key="user_input_text",
    )

    st.markdown("---")
    st.markdown("### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    top_n = st.slider("æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°", 2, 10, 5, 1)
    n_total = st.slider("ç©ºé–“ã«å‡ºã™å˜èªæ•°ï¼ˆä¸­å¿ƒï¼‹å‘¨è¾ºï¼‰", 15, 60, 30, 1)

    # ç‚¹æ»…ã‚’æ’é™¤ã™ã‚‹ãŸã‚ã€è‡ªå‹•æ›´æ–°ã¯å»ƒæ­¢ï¼ˆãƒˆã‚°ãƒ«ã‚‚æ’¤å»ï¼‰
    st.caption("â€»ç‚¹æ»…é˜²æ­¢ã®ãŸã‚ã€è‡ªå‹•æ›´æ–°ï¼ˆã‚†ã‚‰ãï¼‰ã¯ç„¡åŠ¹åŒ–ã—ã¦ã„ã¾ã™ã€‚")

    noise = st.slider("ä½ç½®ã®ã‚†ã‚‰ãï¼ˆå†è¨ˆç®—æ™‚ã®ã¿ï¼‰", 0.00, 0.20, 0.06, 0.01)
    jitter = st.slider("ã‚¨ãƒãƒ«ã‚®ãƒ¼æºã‚‰ã", 0.00, 0.25, 0.10, 0.01)

    qubo_iterations = st.slider(
        "QUBOæœ€é©åŒ–ã®åå¾©å›æ•°", 50, 200, 80, 10,
        help="å°‘ãªã„ã»ã©é€Ÿã„ãŒã€é…ç½®ã®ç²¾åº¦ã¯ä¸‹ãŒã‚Šã¾ã™",
    )

    st.markdown("---")
    st.markdown("### å®‡å®™ã®å¯†åº¦")
    star_count = st.slider("æ˜Ÿå±‘ã®æ•°", 200, 2200, 900, 50)
    # ç‚¹æ»…é˜²æ­¢ï¼šã¾ãŸãŸãã¯å›ºå®šï¼ˆã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã‚’æ’¤å»ï¼‰
    st.caption("â€»æ˜Ÿã®ã¾ãŸãŸãï¼ˆç‚¹æ»…ï¼‰ã¯ç„¡åŠ¹åŒ–ã—ã¦ã„ã¾ã™ã€‚")

    st.markdown("---")
    enable_zoom = st.toggle("ãƒã‚¦ã‚¹ãƒ›ã‚¤ãƒ¼ãƒ«ã§ã‚ºãƒ¼ãƒ ", value=True)

    if st.button("ğŸ”„ å†è¨ˆç®—", use_container_width=True):
        st.session_state["last_user_input"] = ""
        st.rerun()

    st.markdown("---")
    st.markdown("### ğŸµ éŸ³æ¥½")
    st.session_state["bgm_on"] = st.toggle("BGMã‚’å†ç”Ÿ", value=st.session_state["bgm_on"])
    if st.session_state["bgm_on"]:
        if BGM_PATH.exists():
            st.audio(BGM_PATH.read_bytes(), format=BGM_FORMAT)
        else:
            st.caption("âš  assets/bgm.mp3 ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆGitHubã«è¿½åŠ ã—ã¦ãã ã•ã„ï¼‰")

# ============================================================
# 7.5) å†è¨ˆç®—åˆ¤å®šï¼ˆé™æ­¢è¡¨ç¤ºï¼‰
# ============================================================
params_hash = f"{user_input}_{top_n}_{n_total}_{noise}_{jitter}_{qubo_iterations}_{star_count}"
input_changed = user_input != st.session_state["last_user_input"]
params_changed = params_hash != st.session_state["last_params_hash"]
needs_recalc = input_changed or params_changed

if needs_recalc:
    st.session_state["last_user_input"] = user_input
    st.session_state["last_params_hash"] = params_hash

# ============================================================
# 8) è¨ˆç®—ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ / é…ç½®ï¼‰
# ============================================================
def compute_all():
    progress_placeholder = st.empty()
    rng = np.random.default_rng(int(time.time() * 1000) % (2**32 - 1))

    with progress_placeholder.container():
        st.info("ğŸ”„ è¨ˆç®—ã‚’é–‹å§‹ã—ã¾ã™...")
        progress_bar = st.progress(0)
        status_text = st.empty()

    status_text.text("ğŸ“ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºä¸­...")
    progress_bar.progress(10)
    keywords = extract_keywords(user_input, top_n=top_n)
    center_set = set(keywords)

    status_text.text("ğŸ”— å˜èªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ä¸­...")
    progress_bar.progress(30)
    network = build_word_network(keywords, GLOBAL_WORDS_DATABASE, n_total=n_total, rng=rng, jitter=jitter)

    status_text.text("ğŸŒ QUBOæœ€é©åŒ–ã§3Dé…ç½®ã‚’è¨ˆç®—ä¸­...")
    progress_bar.progress(50)

    def update_progress(current, total):
        p = 50 + int((current / total) * 40)
        progress_bar.progress(p)
        status_text.text(f"ğŸŒ QUBOæœ€é©åŒ–ä¸­... ({current}/{total} åå¾©)")

    pos = place_words_3d(
        network["words"],
        center_set=center_set,
        rng=rng,
        noise=noise,
        network=network,
        n_iterations=qubo_iterations,
        progress_callback=update_progress,
    )

    progress_bar.progress(100)
    status_text.text("âœ… è¨ˆç®—å®Œäº†ï¼")
    time.sleep(0.15)
    progress_placeholder.empty()

    st.session_state["network"] = network
    st.session_state["pos"] = pos
    st.session_state["keywords"] = keywords
    st.session_state["center_set"] = center_set

# åˆå› or å¤‰æ›´æ™‚ã®ã¿è¨ˆç®—ï¼ˆé€šå¸¸ã¯é™æ­¢ï¼‰
if (st.session_state["network"] is None) or needs_recalc:
    compute_all()

network = st.session_state["network"]
pos = st.session_state["pos"]
keywords = st.session_state["keywords"]
center_set = st.session_state["center_set"]

# ============================================================
# 9) Plotlyæç”»ï¼ˆæ˜Ÿå±‘ï¼‹ç¸ï¼‹çƒä½“ï¼‹ãƒ©ãƒ™ãƒ«ï¼‰
# ============================================================
if network is None or pos is None or len(network.get("words", [])) == 0:
    st.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒä¸å®Œå…¨ã§ã™ã€‚ã€ŒğŸ”„ å†è¨ˆç®—ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

fig = go.Figure()

# --- æ˜Ÿå±‘ï¼ˆç‚¹æ»…æ’é™¤ï¼šå®Œå…¨å›ºå®šï¼‰ ---
# å›ºå®šseedã§é…ç½®ã‚‚é€æ˜åº¦ã‚‚ã‚µã‚¤ã‚ºã‚‚å›ºå®š
star_rng = np.random.default_rng(12345)
sx = star_rng.uniform(-3.2, 3.2, star_count)
sy = star_rng.uniform(-2.4, 2.4, star_count)
sz = star_rng.uniform(-2.0, 2.0, star_count)

# é€æ˜åº¦å›ºå®šï¼ˆç‚¹æ»…ã—ãªã„ï¼‰
alpha = np.full(star_count, 0.22, dtype=float)
star_size = star_rng.uniform(1.0, 2.4, star_count)
star_colors = [f"rgba(255,255,255,{a})" for a in alpha]

fig.add_trace(go.Scatter3d(
    x=sx, y=sy, z=sz,
    mode="markers",
    marker=dict(size=star_size, color=star_colors),
    hoverinfo="skip",
    showlegend=False
))

words = network["words"]
energies_dict = network.get("energies", {})
center_indices = network.get("center_indices", [])

# --- ä¸­å¿ƒèªâ†’å„å˜èªã®ç·š ---
for cidx in center_indices:
    if cidx >= len(words):
        continue
    center_word = words[cidx]
    cx, cy, cz = pos[cidx]

    for i, w in enumerate(words):
        if i == cidx or i in center_indices:
            continue
        x, y, z = pos[i]
        energy = energies_dict.get(w, 0.0)
        distance = float(np.linalg.norm(pos[i] - pos[cidx]))

        en = min(1.0, abs(energy) / 3.0)
        lw = 1.0 + 3.0 * en
        a = 0.3 + 0.5 * en

        if distance < 1.0:
            color = f"rgba(100,200,255,{a})"
        elif distance < 1.8:
            color = f"rgba(150,200,255,{a * 0.7})"
        else:
            color = f"rgba(200,220,255,{a * 0.4})"

        fig.add_trace(go.Scatter3d(
            x=[cx, x], y=[cy, y], z=[cz, z],
            mode="lines",
            line=dict(width=lw, color=color),
            hovertemplate=f"<b>{center_word}</b> â†’ <b>{w}</b><br>è·é›¢: {distance:.2f}<br>ã‚¨ãƒãƒ«ã‚®ãƒ¼: {energy:.2f}<extra></extra>",
            showlegend=False
        ))

# --- å˜èªé–“ã‚¨ãƒƒã‚¸ ---
for i, j, e in network["edges"]:
    if i in center_indices or j in center_indices:
        continue
    x0, y0, z0 = pos[i]
    x1, y1, z1 = pos[j]
    distance = float(np.linalg.norm(pos[j] - pos[i]))

    strength = min(1.0, abs(e) / 2.0)
    lw = 0.5 + 2.0 * strength
    a = min(0.70, 0.20 + 0.40 * strength)

    if e < -1.0:
        color = f"rgba(120,180,255,{a})"
    elif e < -0.5:
        color = f"rgba(160,200,255,{a})"
    else:
        color = f"rgba(200,200,255,{a})"

    fig.add_trace(go.Scatter3d(
        x=[x0, x1], y=[y0, y1], z=[z0, z1],
        mode="lines",
        line=dict(width=lw, color=color),
        hovertemplate=f"<b>{words[i]}</b> â†” <b>{words[j]}</b><br>è·é›¢: {distance:.2f}<br>ã‚¨ãƒãƒ«ã‚®ãƒ¼: {e:.2f}<extra></extra>",
        showlegend=False
    ))

# --- çƒä½“ï¼ˆè¨€è‘‰ï¼‰---
sizes, colors, labels = [], [], []
for w in words:
    energy = energies_dict.get(w, 0.0)
    if w in center_set:
        sizes.append(28)
        colors.append("rgba(255,235,100,0.98)")
        labels.append(w)
    else:
        en = min(1.0, abs(energy) / 3.0)
        sizes.append(12 + int(8 * en))
        if energy < -1.5:
            colors.append("rgba(180,220,255,0.85)")
        elif energy < -0.5:
            colors.append("rgba(220,240,255,0.75)")
        else:
            colors.append("rgba(255,255,255,0.60)")
        labels.append(w)

# --- çƒä½“ï¼ˆè¨€è‘‰ï¼‰---
sizes, colors, labels = [], [], []
for w in words:
    energy = energies_dict.get(w, 0.0)
    if w in center_set:
        sizes.append(28)
        colors.append("rgba(255,235,100,0.98)")
        labels.append(w)
    else:
        en = min(1.0, abs(energy) / 3.0)
        sizes.append(12 + int(8 * en))
        if energy < -1.5:
            colors.append("rgba(180,220,255,0.85)")
        elif energy < -0.5:
            colors.append("rgba(220,240,255,0.75)")
        else:
            colors.append("rgba(255,255,255,0.60)")
        labels.append(w)

# â˜…ä¸­å¿ƒèªã¨ãã‚Œä»¥å¤–ã§åˆ†ã‘ã‚‹ï¼ˆä¸­å¿ƒèªãƒ©ãƒ™ãƒ«ã‚’èµ¤ã«ã™ã‚‹ï¼‰
center_idx = [i for i, w in enumerate(labels) if w in center_set]
other_idx  = [i for i, w in enumerate(labels) if w not in center_set]

# â‘  ãã‚Œä»¥å¤–ï¼ˆç™½æ–‡å­—ï¼‰
if other_idx:
    oi = np.array(other_idx, dtype=int)
    fig.add_trace(go.Scatter3d(
        x=pos[oi, 0], y=pos[oi, 1], z=pos[oi, 2],
        mode="markers+text",
        text=[labels[i] for i in oi],
        textposition="top center",
        textfont=dict(size=18, color="rgba(255,255,255,1.0)"),
        marker=dict(
            size=[sizes[i] for i in oi],
            color=[colors[i] for i in oi],
            line=dict(width=1, color="rgba(0,0,0,0.10)")
        ),
        hovertemplate="<b>%{text}</b><extra></extra>",
        showlegend=False
    ))

# â‘¡ ä¸­å¿ƒèªï¼ˆèµ¤æ–‡å­—ï¼‰
if center_idx:
    ci = np.array(center_idx, dtype=int)
    fig.add_trace(go.Scatter3d(
        x=pos[ci, 0], y=pos[ci, 1], z=pos[ci, 2],
        mode="markers+text",
        text=[labels[i] for i in ci],
        textposition="top center",
        textfont=dict(size=24, color="rgba(255,80,80,1.0)"),  # â˜…èµ¤
        marker=dict(
            size=[sizes[i] for i in ci],
            color=[colors[i] for i in ci],
            line=dict(width=2, color="rgba(255,80,80,0.8)")
        ),
        hovertemplate="<b>%{text}</b><br>ä¸­å¿ƒèª<extra></extra>",
        showlegend=False
    ))


# ä¸­å¿ƒèªã®â€œå…‰ã®å±¤â€ï¼ˆå›ºå®šï¼šç‚¹æ»…ã—ãªã„ï¼‰
for cidx in center_indices:
    if cidx >= len(words):
        continue
    cx, cy, cz = pos[cidx]
    for layer, mult in enumerate([1.0, 1.3, 1.6], 1):
        opacity = 0.15 / layer
        fig.add_trace(go.Scatter3d(
            x=[cx], y=[cy], z=[cz],
            mode="markers",
            marker=dict(size=[35 * mult], color=f"rgba(150,200,255,{opacity})", line=dict(width=0)),
            hoverinfo="skip",
            showlegend=False
        ))

fig.update_layout(
    paper_bgcolor="rgba(6,8,18,1)",
    scene=dict(
        xaxis=dict(visible=False),
        yaxis=dict(visible=False),
        zaxis=dict(visible=False),
        bgcolor="rgba(6,8,18,1)",
        camera=dict(
            eye=dict(x=1.6, y=1.15, z=1.05),
            center=dict(x=0, y=0, z=0),
            up=dict(x=0, y=1, z=0),
        ),
        dragmode="orbit",
    ),
    margin=dict(l=0, r=0, t=0, b=0),
)

plotly_config = {
    "displayModeBar": True,
    "scrollZoom": bool(enable_zoom),
    "displaylogo": False,
    "responsive": True,
    "toImageButtonOptions": {"format": "png", "filename": "quantum_oracle", "height": 800, "width": 1200, "scale": 1},
    "doubleClick": "reset",
}

# ============================================================
# 10) ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆå·¦ï¼šå®‡å®™ / å³ï¼šæ ¼è¨€+å‡ºæ‰€ï¼‰
# ============================================================
left, right = st.columns([2.0, 1.0], gap="large")

with left:
    st.plotly_chart(fig, use_container_width=True, config=plotly_config)
    st.caption("å˜èªï¼ˆçƒä½“ï¼‰ã¨ç¸ï¼ˆç·šï¼‰ã€‚ãƒã‚¦ã‚¹ã§å›è»¢ãƒ»ã‚ºãƒ¼ãƒ ã§ãã¾ã™ã€‚ï¼ˆç‚¹æ»…ãªã—ï¼‰")

with right:
    st.markdown("### ğŸ“Š ç¾åœ¨ã®çŠ¶æ…‹")
    st.markdown(f"**è¨ˆç®—æ¸ˆã¿å˜èªæ•°**: {len(network.get('words', []))}èª")
    st.markdown(f"**æ¥ç¶šæ•°**: {len(network.get('edges', []))}æœ¬")
    if network.get("energies"):
        mn = min(network["energies"].values())
        mx = max(network["energies"].values())
        st.markdown(f"**ã‚¨ãƒãƒ«ã‚®ãƒ¼ç¯„å›²**: {mn:.2f} ï½ {mx:.2f}")
    st.markdown("---")

    st.markdown(
        """
        <div style="
          background: rgba(255,255,255,0.06);
          border: 1px solid rgba(255,255,255,0.10);
          border-radius: 18px;
          padding: 16px 16px 10px 16px;
          box-shadow: 0 18px 60px rgba(0,0,0,0.18);
        ">
        """,
        unsafe_allow_html=True
    )

    st.markdown("### å…ˆäººã®ã“ã¨ã°")
    st.markdown(f"**ã„ã¾ã®æ ¸ï¼ˆæ¨å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰**ï¼š`{', '.join(keywords)}`")
    st.markdown("---")

    q = select_relevant_quote(keywords)

    with st.expander("ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆæ ¼è¨€é¸æŠï¼‰", expanded=False):
        st.write(f"**æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: {keywords}")
        st.write(f"**åˆ©ç”¨å¯èƒ½ãªæ ¼è¨€æ•°**: {len(FAMOUS_QUOTES)}ä»¶")
        st.write(f"**Excelæ ¼è¨€ï¼ˆèª­ã¿è¾¼ã¿ï¼‰**: {len(load_quotes_from_excel_cached(EXCEL_DEFAULT))}ä»¶")
        st.write(f"**å‡ºæ‰€**: {q.get('source', 'â€”')}")

    st.markdown(f"#### ã€Œ{q['quote']}ã€")
    st.markdown("---")
    st.markdown(f"**å‡ºæ‰€ï¼š** {q.get('source','â€”') if q.get('source') else 'â€”'}")
    if q.get("note"):
        st.markdown(f"<div style='opacity:0.80; font-size:0.92rem;'>â€» {q['note']}</div>", unsafe_allow_html=True)

    st.markdown("")
    st.markdown("### æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆç¢ºèªï¼‰")
    for k in keywords:
        st.markdown(f"- {k}")

    st.markdown("</div>", unsafe_allow_html=True)
