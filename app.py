# -*- coding: utf-8 -*-
# ============================================================
# QUBO Ã— é‡å­ç¥è¨— UIï¼ˆStreamlit + Plotlyï¼‰
# - å…¥åŠ›æ–‡ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
# - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸­å¿ƒã«ã€Œã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒè¿‘ã„å˜èªã€ãŒé›†ã¾ã‚‹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰
# - 3Dã§â€œçƒä½“ï¼ˆè¨€è‘‰ï¼‰ï¼‹ç¸ï¼ˆç·šï¼‰ï¼‹æ˜Ÿå±‘ï¼ˆå®‡å®™ï¼‰â€ã‚’æç”»
# - æ ¼è¨€ã¯ã€Œå‡ºæ‰€ï¼ˆå…¸æ‹ /ä½œè€…/æ„è¨³/å‰µä½œï¼‰ã€ã‚‚è¡¨ç¤º
# - ãƒã‚¦ã‚¹ã§å›è»¢/ã‚ºãƒ¼ãƒ /ãƒªã‚»ãƒƒãƒˆå¯èƒ½
# ============================================================

import re
import time
import random
import os
from typing import Dict, List, Tuple

import numpy as np
import plotly.graph_objects as go
import streamlit as st
from streamlit_autorefresh import st_autorefresh

# pandasã¨openpyxlã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆExcelèª­ã¿è¾¼ã¿ç”¨ï¼‰
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    pd = None

# =========================
# 0) ãƒšãƒ¼ã‚¸è¨­å®š + CSSï¼ˆå®‡å®™ï¼‰
# =========================
st.set_page_config(page_title="é‡å­ç¥è¨— - ç¸ã®çƒä½“", layout="wide")
from pathlib import Path

# ======================
# BGMè¨­å®š
# ======================
BGM_PATH = Path("assets/bgm.mp3")

if "bgm_on" not in st.session_state:
    st.session_state.bgm_on = True

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–ï¼ˆæœ€åˆã«å®Ÿè¡Œï¼‰
if "excel_quotes_loaded" not in st.session_state:
    st.session_state.excel_quotes_loaded = False
    st.session_state.excel_quotes = []

SPACE_CSS = """
<style>
.stApp{
  background:
    radial-gradient(circle at 18% 24%, rgba(110,150,255,0.12), transparent 38%),
    radial-gradient(circle at 78% 68%, rgba(255,160,220,0.08), transparent 44%),
    radial-gradient(circle at 50% 50%, rgba(255,255,255,0.03), transparent 55%),
    linear-gradient(180deg, rgba(6,8,18,1), rgba(10,12,26,1));
}
.block-container{ padding-top: 1.2rem; }
div[data-testid="stMarkdownContainer"] p,
div[data-testid="stMarkdownContainer"] li{
  font-family: "Hiragino Mincho ProN", "Yu Mincho", "Noto Serif JP", serif;
  letter-spacing: 0.02em;
  color: rgba(245,245,255,0.92);
}
h1,h2,h3{
  font-family: "Hiragino Mincho ProN", "Yu Mincho", "Noto Serif JP", serif !important;
  font-weight: 600 !important;
  color: rgba(245,245,255,0.95);
}
section[data-testid="stSidebar"]{
  background: rgba(255,255,255,0.08);
  border-right: 1px solid rgba(255,255,255,0.10);
  backdrop-filter: blur(10px);
}
div[data-testid="stPlotlyChart"] > div{
  position: relative;
  border-radius: 18px;
  overflow: hidden;
  box-shadow: 0 18px 60px rgba(0,0,0,0.30);
}
div[data-testid="stPlotlyChart"] > div::after{
  content:"";
  position:absolute;
  inset:0;
  background:
    radial-gradient(circle at 30% 25%, rgba(120,160,255,0.10), transparent 45%),
    radial-gradient(circle at 70% 65%, rgba(255,180,220,0.06), transparent 52%),
    radial-gradient(circle at 50% 50%, rgba(0,0,0,0.00), rgba(0,0,0,0.38));
  pointer-events:none;
}
</style>
"""
st.markdown(SPACE_CSS, unsafe_allow_html=True)

# =========================
# 1) ã‚°ãƒ­ãƒ¼ãƒãƒ«å˜èªDBï¼ˆä»–ã®äººã®è¨€è‘‰ï¼‰
# =========================
GLOBAL_WORDS_DATABASE = [
    "ä¸–ç•Œå¹³å’Œ","è²¢çŒ®","æˆé•·","å­¦ã³","æŒ‘æˆ¦","å¤¢","å¸Œæœ›","æœªæ¥",
    "æ„Ÿè¬","æ„›","å¹¸ã›","å–œã³","å®‰å¿ƒ","å……å®Ÿ","æº€è¶³","å¹³å’Œ",
    "åŠªåŠ›","ç¶™ç¶š","å¿è€","èª å®Ÿ","æ­£ç›´","å„ªã—ã•","æ€ã„ã‚„ã‚Š","å…±æ„Ÿ",
    "èª¿å’Œ","ãƒãƒ©ãƒ³ã‚¹","è‡ªç„¶","ç¾","çœŸå®Ÿ","è‡ªç”±","æ­£ç¾©","é“",
    "çµ†","ã¤ãªãŒã‚Š","å®¶æ—","å‹äºº","ä»²é–“","ä¿¡é ¼","å°Šæ•¬","å”åŠ›",
    "ä»Š","ç¬é–“","éç¨‹","å¤‰åŒ–","é€²åŒ–","ç™ºå±•","å¾ªç’°","æµã‚Œ",
    "é™ã‘ã•","é›†ä¸­","è¦šæ‚Ÿ","æ±ºæ„","å‹‡æ°—","å¼·ã•","æŸ”è»Ÿæ€§","å¯›å®¹",
]

# =========================
# 2) æ ¼è¨€DBï¼ˆå‡ºæ‰€ã‚‚æŒãŸã›ã‚‹ï¼‰
# =========================

def load_quotes_from_excel(excel_path: str = None) -> List[Dict]:
    """
    Excelãƒ•ã‚¡ã‚¤ãƒ«ã®QUOTESãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã‹ã‚‰æ ¼è¨€ã‚’èª­ã¿è¾¼ã‚€
    """
    quotes = []
    
    if not PANDAS_AVAILABLE:
        return quotes
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®Excelãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    if excel_path is None:
        excel_path = "quantum_shintaku_pack_v3_with_sense_20260213_oposite_modify_with_lr022101.xlsx"
    
    if not os.path.exists(excel_path):
        return quotes
    
    try:
        # QUOTESãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã‚€
        df = pd.read_excel(excel_path, sheet_name='QUOTES', engine='openpyxl')
        
        # ãƒ‡ãƒãƒƒã‚°: åˆ—åã‚’è¡¨ç¤º
        print(f"Excelãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ—å: {df.columns.tolist()}")
        print(f"Excelãƒ•ã‚¡ã‚¤ãƒ«ã®è¡Œæ•°: {len(df)}")
        
        # åˆ—åã‚’ç¢ºèªã—ã¦é©åˆ‡ã«ãƒãƒƒãƒ”ãƒ³ã‚°
        for idx, row in df.iterrows():
            quote_dict = {}
            
            # æ ¼è¨€ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ§˜ã€…ãªåˆ—åã«å¯¾å¿œï¼‰
            quote_text = None
            for col in ['æ ¼è¨€', 'QUOTE', 'Quote', 'quote', 'ãƒ†ã‚­ã‚¹ãƒˆ', 'æ–‡', 'è¨€è‘‰']:
                if col in df.columns:
                    quote_text = str(row.get(col, "")).strip()
                    if quote_text and quote_text.lower() not in ("nan", "none", ""):
                        break
            
            if not quote_text:
                continue
            
            quote_dict["quote"] = quote_text
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæ§˜ã€…ãªåˆ—åã«å¯¾å¿œï¼‰
            keywords = []
            for col in ['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'KEYWORDS', 'Keywords', 'keywords', 'ã‚¿ã‚°', 'TAG', 'Tag']:
                if col in df.columns:
                    kw_str = str(row.get(col, "")).strip()
                    if kw_str and kw_str.lower() not in ("nan", "none", ""):
                        keywords = [k.strip() for k in kw_str.replace("ã€", ",").split(",") if k.strip()]
                        break
            
            quote_dict["keywords"] = keywords if keywords else []
            
            # å‡ºå…¸ï¼ˆæ§˜ã€…ãªåˆ—åã«å¯¾å¿œï¼‰
            source = None
            for col in ['å‡ºå…¸', 'SOURCE', 'Source', 'source', 'å‡ºæ‰€', 'å…¸æ‹ ', 'ä½œè€…']:
                if col in df.columns:
                    source = str(row.get(col, "")).strip()
                    if source and source.lower() not in ("nan", "none", ""):
                        break
            
            quote_dict["source"] = source or "ä¼çµ±çš„ãªæ•™ãˆ"
            
            # å‚™è€ƒï¼ˆæ§˜ã€…ãªåˆ—åã«å¯¾å¿œï¼‰
            note = None
            for col in ['å‚™è€ƒ', 'NOTE', 'Note', 'note', 'æ³¨', 'ãƒ¡ãƒ¢']:
                if col in df.columns:
                    note = str(row.get(col, "")).strip()
                    if note and note.lower() not in ("nan", "none", ""):
                        break
            
            quote_dict["note"] = note or ""
            
            quotes.append(quote_dict)
        
        return quotes
    
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚æ—¢å­˜ã®FAMOUS_QUOTESã‚’ä½¿ç”¨
        # st.warningã¯Streamlitã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¤–ã§ã¯ä½¿ãˆãªã„ã®ã§ã€printã§ä»£ç”¨
        print(f"Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®æ ¼è¨€èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return []

# Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ ¼è¨€ã‚’èª­ã¿è¾¼ã‚€ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
# æ³¨æ„: Streamlitã®å®Ÿè¡Œæ™‚ã«ã¯æ¯å›èª­ã¿è¾¼ã¾ã‚Œã‚‹ãŸã‚ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜ã™ã‚‹
try:
    if not st.session_state.excel_quotes_loaded:
        st.session_state.excel_quotes = load_quotes_from_excel()
        st.session_state.excel_quotes_loaded = True
        if st.session_state.excel_quotes:
            print(f"Excelã‹ã‚‰{len(st.session_state.excel_quotes)}ä»¶ã®æ ¼è¨€ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
except Exception as e:
    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚å‡¦ç†ã‚’ç¶šè¡Œ
    print(f"Excelæ ¼è¨€èª­ã¿è¾¼ã¿ã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    import traceback
    traceback.print_exc()
    st.session_state.excel_quotes_loaded = True
    st.session_state.excel_quotes = []

# æ—¢å­˜ã®FAMOUS_QUOTESï¼ˆåŸºæœ¬ã®æ ¼è¨€ï¼‰
BASE_FAMOUS_QUOTES = [
    {
        "keywords": ["å¹³å’Œ","ä¸–ç•Œ","è²¢çŒ®","å¸Œæœ›"],
        "quote": "é›ªã®ä¸‹ã§ç¨®ã¯æ˜¥ã‚’å¾…ã£ã¦ã„ã‚‹ã€‚ç„¦ã‚‹ã¹ã‹ã‚‰ãšã€æ™‚æº€ã¡ã‚‹ã‚’å¾…ã¦ã€‚",
        "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ/å¯“è©±èª¿",
        "note": "å…¬é–‹ç”¨ã«å…¸æ‹ ä»˜ãæ ¼è¨€ã¸å·®ã—æ›¿ãˆå¯"
    },
    {
        "keywords": ["æˆé•·","åŠªåŠ›","ç¶™ç¶š","æŒ‘æˆ¦"],
        "quote": "åƒé‡Œã®é“ã‚‚ä¸€æ­©ã‹ã‚‰ã€‚æ­©ã¿ã‚’æ­¢ã‚ãšã€ç¶šã‘ã‚‹ã“ã¨ã«æ„å‘³ãŒã‚ã‚‹ã€‚",
        "source": "æ•…äº‹æˆèªï¼ˆè€å­/è€å­ç­‰ã«é¡ã™ã‚‹è¡¨ç¾ã¨ã—ã¦æµé€šï¼‰â€”è¦å…¸æ‹ ç¢ºèª",
        "note": "çŸ­æ–‡åŒ–ã—ãŸå®šå‹å¥ã€‚å…¬é–‹å‰ã«å…¸æ‹ ç²¾æŸ»æ¨å¥¨"
    },
    {
        "keywords": ["æ„Ÿè¬","æ„›","çµ†","ã¤ãªãŒã‚Š"],
        "quote": "ä¸€æœŸä¸€ä¼šã€‚ä»Šã“ã®ç¬é–“ã‚’å¤§åˆ‡ã«ã€‚ã™ã¹ã¦ã¯ç¸ã§ç¹‹ãŒã£ã¦ã„ã‚‹ã€‚",
        "source": "ä¸€æœŸä¸€ä¼šï¼ˆèŒ¶é“æ€æƒ³ï¼‰ï¼‹é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”ç·¨é›†/æ„è¨³",
        "note": ""
    },
    {
        "keywords": ["è‡ªç„¶","èª¿å’Œ","ãƒãƒ©ãƒ³ã‚¹","æµã‚Œ"],
        "quote": "æ°´ã¯ã€äº‰ã‚ãªã„ã€‚å½¢ã«ã“ã ã‚ã‚‰ãšã€æµã‚Œã‚‹ãŒã¾ã¾ã«ã€‚",
        "source": "è€å­ã€é“å¾³çµŒã€ï¼ˆä¸Šå–„è‹¥æ°´ï¼‰â€”æ„è¨³/ç·¨é›†",
        "note": "å³å¯†ãªåŸæ–‡å¼•ç”¨ã§ã¯ãªãæ„è¨³"
    },
    {
        "keywords": ["é™ã‘ã•","é›†ä¸­","ä»Š","ç¬é–“"],
        "quote": "æ­¢ã¾ã‚‹ã“ã¨ã§ã€æµã‚ŒãŒè¦‹ãˆã‚‹ã€‚å‹•ã®ä¸­ã«é™ãŒã‚ã‚‹ã€‚",
        "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ",
        "note": ""
    },
    {
        "keywords": ["å‹‡æ°—","æ±ºæ„","æŒ‘æˆ¦","é“"],
        "quote": "é“ãŒåˆ†ã‹ã‚Œã¦ã„ãŸã‚‰ã€å¿µã®ãªã„æ–¹ã¸è¡Œã‘ã€‚",
        "source": "å‡ºå…¸è¦ç¢ºèªï¼ˆæµé€šå¥ï¼‰â€”æš«å®š",
        "note": "å…¬é–‹å‰ã«å…¸æ‹ ã‚’ç¢ºå®šæ¨å¥¨"
    },
    {
        "keywords": ["æ€ã„ã‚„ã‚Š","å„ªã—ã•","å…±æ„Ÿ","ä¿¡é ¼"],
        "quote": "äººã®å¿ƒã«å¯„ã‚Šæ·»ã†ã€‚ãã‚ŒãŒçœŸã®å¼·ã•ã§ã‚ã‚‹ã€‚",
        "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ",
        "note": ""
    },
    {
        "keywords": ["å¤‰åŒ–","é€²åŒ–","ç™ºå±•","æœªæ¥"],
        "quote": "ç„¡ç‚ºã«ã—ã¦ç‚ºã™ã€‚å‹•ãã“ã¨ãŒé™ã§ã‚ã‚‹ã€‚",
        "source": "æ±æ´‹æ€æƒ³ï¼ˆç„¡ç‚ºè‡ªç„¶ï¼‰â€”æ„è¨³/ç·¨é›†",
        "note": ""
    },
    {
        "keywords": ["ç¾","çœŸå®Ÿ","è‡ªç„¶","èª¿å’Œ"],
        "quote": "é–“ã“ããŒç­”ãˆã§ã‚ã‚‹ã€‚ä½™ç™½ã«ã“ãæœ¬è³ªãŒã‚ã‚‹ã€‚",
        "source": "ç¾å­¦ï¼ˆé–“/ä½™ç™½ï¼‰ï¼‹é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”ç·¨é›†",
        "note": ""
    },
    {
        "keywords": ["è‡ªç”±","æ­£ç¾©","é“","èª å®Ÿ"],
        "quote": "å·±ã«èª å®Ÿã§ã‚ã‚‹ã“ã¨ã€‚ãã‚ŒãŒè‡ªç”±ã¸ã®é“ã§ã‚ã‚‹ã€‚",
        "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ",
        "note": ""
    },
]

# Excelã‹ã‚‰èª­ã¿è¾¼ã‚“ã æ ¼è¨€ã‚’è¿½åŠ ï¼ˆæ—¢å­˜ã®ã‚‚ã®ã¨é‡è¤‡ã—ãªã„ã‚ˆã†ã«ï¼‰
# FAMOUS_QUOTESã‚’åˆæœŸåŒ–ï¼ˆBASE_FAMOUS_QUOTESã‹ã‚‰é–‹å§‹ï¼‰
FAMOUS_QUOTES = BASE_FAMOUS_QUOTES.copy()

try:
    excel_quotes = st.session_state.get("excel_quotes", [])
    if excel_quotes:
        # æ—¢å­˜ã®æ ¼è¨€ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        existing_quotes = {q.get("quote", "") for q in FAMOUS_QUOTES}
        
        # æ–°ã—ã„æ ¼è¨€ã‚’è¿½åŠ 
        added_count = 0
        for excel_quote in excel_quotes:
            excel_quote_text = excel_quote.get("quote", "")
            if excel_quote_text and excel_quote_text not in existing_quotes:
                FAMOUS_QUOTES.append(excel_quote)
                existing_quotes.add(excel_quote_text)
                added_count += 1
        
        if added_count > 0:
            print(f"Excelã‹ã‚‰{added_count}ä»¶ã®æ–°ã—ã„æ ¼è¨€ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼ˆåˆè¨ˆ: {len(FAMOUS_QUOTES)}ä»¶ï¼‰")
except Exception as e:
    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚å‡¦ç†ã‚’ç¶šè¡Œï¼ˆBASE_FAMOUS_QUOTESã®ã¿ä½¿ç”¨ï¼‰
    print(f"Excelæ ¼è¨€ã®çµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
    import traceback
    traceback.print_exc()

# =========================
# 3) ãƒ†ã‚­ã‚¹ãƒˆâ†’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆç°¡æ˜“ï¼‰
# =========================
def extract_keywords(text: str, top_n: int = 5) -> List[str]:
    text = (text or "").strip()
    # è¨˜å·/æ•°å­—ã‚’ã–ã£ãã‚Šé™¤å»
    text_clean = re.sub(r"[0-9ï¼-ï¼™\W]+", " ", text)

    found = [w for w in GLOBAL_WORDS_DATABASE if w in text_clean]
    if found:
        # å…¥åŠ›ã«å«ã¾ã‚ŒãŸDBèªã‚’å„ªå…ˆ
        return found[:top_n]

    # fallbackï¼šã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã‚„çŸ­æ–‡ã‹ã‚‰æ‹¾ã†
    tokens = [t for t in text_clean.split() if len(t) >= 2]
    if not tokens:
        return ["é™ã‘ã•", "è¿·ã„"]  # ä½•ã‚‚ãªã‘ã‚Œã°ä¿é™º
    return tokens[:top_n]

# =========================
# 4) â€œã‚¨ãƒãƒ«ã‚®ãƒ¼â€è¨ˆç®—ï¼ˆQUBOçš„ç›¸äº’ä½œç”¨ã®ç²¾å¯†ãƒ¢ãƒ‡ãƒ«ï¼‰
# =========================
CATEGORIES = {
    "é¡˜ã„": ["ä¸–ç•Œå¹³å’Œ","è²¢çŒ®","æˆé•·","å¤¢","å¸Œæœ›","æœªæ¥"],
    "æ„Ÿæƒ…": ["æ„Ÿè¬","æ„›","å¹¸ã›","å–œã³","å®‰å¿ƒ","æº€è¶³","å¹³å’Œ"],
    "è¡Œå‹•": ["åŠªåŠ›","ç¶™ç¶š","å¿è€","èª å®Ÿ","æ­£ç›´"],
    "å“²å­¦": ["èª¿å’Œ","ãƒãƒ©ãƒ³ã‚¹","è‡ªç„¶","ç¾","é“","çœŸå®Ÿ","è‡ªç”±","æ­£ç¾©"],
    "é–¢ä¿‚": ["çµ†","ã¤ãªãŒã‚Š","å®¶æ—","å‹äºº","ä»²é–“","ä¿¡é ¼","å°Šæ•¬","å”åŠ›"],
    "å†…çš„": ["é™ã‘ã•","é›†ä¸­","è¦šæ‚Ÿ","æ±ºæ„","å‹‡æ°—","å¼·ã•","æŸ”è»Ÿæ€§","å¯›å®¹"],
    "æ™‚é–“": ["ä»Š","ç¬é–“","éç¨‹","å¤‰åŒ–","é€²åŒ–","ç™ºå±•","å¾ªç’°","æµã‚Œ"],
}

# å˜èªé–“ã®æ„å‘³çš„é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼ˆäº‹å‰è¨ˆç®—ç”¨ï¼‰
WORD_SEMANTIC_WEIGHTS = {}

def calculate_semantic_similarity(word1: str, word2: str) -> float:
    """æ„å‘³çš„é¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆ0-1ã®ç¯„å›²ã€1ã«è¿‘ã„ã»ã©é¡ä¼¼ï¼‰"""
    if word1 == word2:
        return 1.0
    
    # æ–‡å­—ãƒ¬ãƒ™ãƒ«ã®é¡ä¼¼åº¦
    common_chars = set(word1) & set(word2)
    char_sim = len(common_chars) / max(len(set(word1)), len(set(word2)), 1)
    
    # ã‚«ãƒ†ã‚´ãƒªã®ä¸€è‡´åº¦
    category_sim = 0.0
    for _, ws in CATEGORIES.items():
        w1_in = word1 in ws
        w2_in = word2 in ws
        if w1_in and w2_in:
            category_sim = 1.0
            break
        elif w1_in or w2_in:
            category_sim = 0.3
    
    # é•·ã•ã®é¡ä¼¼åº¦
    len_sim = 1.0 - abs(len(word1) - len(word2)) / max(len(word1), len(word2), 1)
    
    # é‡ã¿ä»˜ãå¹³å‡
    similarity = 0.4 * char_sim + 0.4 * category_sim + 0.2 * len_sim
    return float(np.clip(similarity, 0.0, 1.0))

def calculate_energy_between_words(word1: str, word2: str, rng: np.random.Generator, jitter: float) -> float:
    """
    ã‚ˆã‚Šç²¾å¯†ãªã‚¨ãƒãƒ«ã‚®ãƒ¼è¨ˆç®—ï¼ˆQUBOç›¸äº’ä½œç”¨ï¼‰
    å°ã•ã„ï¼ˆã‚ˆã‚Šè² ï¼‰ã»ã©â€œè¿‘ã„â€æ‰±ã„ã€‚
    """
    # æ„å‘³çš„é¡ä¼¼åº¦ã‹ã‚‰ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’è¨ˆç®—
    similarity = calculate_semantic_similarity(word1, word2)
    
    # é¡ä¼¼åº¦ãŒé«˜ã„ã»ã©è² ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆçµåˆãŒå¼·ã„ï¼‰
    # ã‚¨ãƒãƒ«ã‚®ãƒ¼ = -2.0 * similarity + 0.5ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
    energy = -2.0 * similarity + 0.5
    
    # æ–‡å­—ã®å…±é€šæˆåˆ†ã«ã‚ˆã‚‹è£œæ­£
    common = set(word1) & set(word2)
    if common:
        energy -= 0.20 * len(common) / max(len(word1), len(word2), 1)
    
    # åŒã‚«ãƒ†ã‚´ãƒªãªã‚‰ã•ã‚‰ã«è¿‘ã¥ã
    for _, ws in CATEGORIES.items():
        if (word1 in ws) and (word2 in ws):
            energy -= 0.60
            break
    
    # é‡å­çš„æºã‚‰ãï¼ˆQUBOã®æœ¬è³ªï¼‰
    energy += rng.normal(0, jitter)
    return float(energy)

def build_qubo_matrix_for_words(words: List[str], rng: np.random.Generator, jitter: float) -> Dict[Tuple[int, int], float]:
    """
    å˜èªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”¨ã®QUBOãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰
    Q[i,j] = å˜èªiã¨å˜èªjã®ç›¸äº’ä½œç”¨ã‚¨ãƒãƒ«ã‚®ãƒ¼
    """
    n = len(words)
    Q: Dict[Tuple[int, int], float] = {}
    
    # å¯¾è§’é …ï¼ˆå„å˜èªã®ãƒã‚¤ã‚¢ã‚¹ï¼‰
    for i in range(n):
        # ä¸­å¿ƒèªã¯ã‚ˆã‚Šä½ã„ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆé¸æŠã•ã‚Œã‚„ã™ã„ï¼‰
        Q[(i, i)] = -0.5 if i < len(words) else 0.0
    
    # éå¯¾è§’é …ï¼ˆå˜èªé–“ã®ç›¸äº’ä½œç”¨ï¼‰
    for i in range(n):
        for j in range(i + 1, n):
            energy = calculate_energy_between_words(words[i], words[j], rng, jitter)
            # QUBOå½¢å¼ï¼šx_i * x_j ã®ä¿‚æ•°
            Q[(i, j)] = energy
            Q[(j, i)] = energy  # å¯¾ç§°æ€§
    
    return Q

def solve_qubo_placement(Q: Dict[Tuple[int, int], float], n_words: int, 
                         center_indices: List[int], rng: np.random.Generator,
                         n_iterations: int = 100, progress_callback=None,
                         energies_dict: Dict[str, float] = None,
                         words_list: List[str] = None) -> np.ndarray:
    """
    QUBOæœ€é©åŒ–ã‚’ä½¿ã£ã¦å˜èªã®3Dé…ç½®ã‚’æ±ºå®š
    ä¸­å¿ƒèªã‚’åŸç‚¹ã«é…ç½®ã—ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼ã«åŸºã¥ã„ã¦è·é›¢ã‚’æ±ºå®š
    """
    pos = np.zeros((n_words, 3), dtype=float)
    
    # ä¸­å¿ƒèªã‚’åŸç‚¹ï¼ˆ0,0,0ï¼‰ã«é…ç½®
    for idx in center_indices:
        if idx < n_words:
            pos[idx] = [0.0, 0.0, 0.0]
    
    # ã‚¨ãƒãƒ«ã‚®ãƒ¼ã«åŸºã¥ã„ã¦å„å˜èªã®ä¸­å¿ƒèªã‹ã‚‰ã®è·é›¢ã‚’æ±ºå®š
    if energies_dict is None:
        energies_dict = {}
    if words_list is None:
        words_list = []
    
    # ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®ç¯„å›²ã‚’å–å¾—ï¼ˆè·é›¢è¨ˆç®—ç”¨ï¼‰
    energy_values = list(energies_dict.values()) if energies_dict else []
    if energy_values:
        min_energy = min(energy_values)
        max_energy = max(energy_values)
        energy_range = max_energy - min_energy if max_energy != min_energy else 1.0
    else:
        min_energy = -3.0
        energy_range = 3.0
    
    # å„å˜èªã‚’ã‚¨ãƒãƒ«ã‚®ãƒ¼ã«åŸºã¥ã„ã¦é…ç½®
    golden_angle = np.pi * (3 - np.sqrt(5))
    word_idx = 0
    
    for i in range(n_words):
        if i in center_indices:
            continue  # ä¸­å¿ƒèªã¯æ—¢ã«é…ç½®æ¸ˆã¿
        
        # å˜èªã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’å–å¾—
        if i < len(words_list):
            word = words_list[i]
            energy = energies_dict.get(word, 0.0)
        else:
            energy = 0.0
        
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’è·é›¢ã«å¤‰æ›ï¼ˆè² ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒå¤§ãã„ã»ã©è¿‘ãï¼‰
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼ç¯„å›²: -3.0 ï½ 0.0 â†’ è·é›¢ç¯„å›²: 0.3 ï½ 2.5
        normalized_energy = (energy - min_energy) / energy_range if energy_range > 0 else 0.5
        distance = 0.3 + (1.0 - normalized_energy) * 2.2  # 0.3ã‹ã‚‰2.5ã®ç¯„å›²
        
        # çƒé¢ä¸Šã«å‡ç­‰ã«é…ç½®ï¼ˆä¸­å¿ƒèªã‹ã‚‰ã®è·é›¢ã‚’ç¶­æŒï¼‰
        theta = golden_angle * word_idx
        y = 1 - (word_idx / float(max(1, n_words - len(center_indices) - 1))) * 2
        radius_at_y = np.sqrt(max(0.0, 1 - y * y))
        
        # è·é›¢ã‚’é©ç”¨
        x = np.cos(theta) * radius_at_y * distance
        z = np.sin(theta) * radius_at_y * distance
        
        pos[i] = [x, y * distance * 0.6, z]  # yæ–¹å‘ã‚‚è·é›¢ã«æ¯”ä¾‹
        word_idx += 1
    
    # QUBOã‚¨ãƒãƒ«ã‚®ãƒ¼ã«åŸºã¥ã„ã¦å¾®èª¿æ•´ï¼ˆå˜èªé–“ã®ç›¸äº’ä½œç”¨ï¼‰
    for iteration in range(n_iterations):
        for i in range(n_words):
            if i in center_indices:
                continue  # ä¸­å¿ƒèªã¯å‹•ã‹ã•ãªã„
            
            force = np.zeros(3, dtype=float)
            
            # ä¸­å¿ƒèªã‹ã‚‰ã®å¼•åŠ›ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ã«åŸºã¥ãï¼‰
            for center_idx in center_indices:
                vec_to_center = pos[center_idx] - pos[i]
                dist_to_center = np.linalg.norm(vec_to_center)
                if dist_to_center > 0.01:
                    # ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒä½ã„ã»ã©å¼·ãå¼•ãåˆã†
                    if i < len(words_list):
                        word = words_list[i]
                        energy = energies_dict.get(word, 0.0)
                    else:
                        energy = 0.0
                    target_distance = 0.3 + (1.0 - (energy - min_energy) / energy_range) * 2.2 if energy_range > 0 else 1.5
                    
                    # ç›®æ¨™è·é›¢ã«å‘ã‹ã†åŠ›
                    if dist_to_center < target_distance * 0.9:
                        # è¿‘ã™ãã‚‹å ´åˆã¯å°‘ã—é›¢ã™
                        force -= vec_to_center / dist_to_center * 0.05
                    elif dist_to_center > target_distance * 1.1:
                        # é ã™ãã‚‹å ´åˆã¯è¿‘ã¥ã‘ã‚‹
                        force += vec_to_center / dist_to_center * 0.1
            
            # ä»–ã®å˜èªã¨ã®ç›¸äº’ä½œç”¨
            for j in range(n_words):
                if i == j or j in center_indices:
                    continue
                
                energy = Q.get((i, j), 0.0)
                if energy < -0.3:  # å¼·ã„è² ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ = å¼•ãåˆã†
                    vec = pos[j] - pos[i]
                    dist = np.linalg.norm(vec)
                    if dist > 0.01:
                        strength = abs(energy) * 0.08
                        force += vec / dist * strength
                elif energy > 0.2:  # æ­£ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ = åç™º
                    vec = pos[i] - pos[j]
                    dist = np.linalg.norm(vec)
                    if dist > 0.01:
                        strength = abs(energy) * 0.03
                        force += vec / dist * strength
            
            # ä½ç½®ã‚’æ›´æ–°
            pos[i] += force * 0.15
        
        # é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if progress_callback:
            progress_callback(iteration + 1, n_iterations)
    
    return pos

def build_word_network(center_words: List[str], database: List[str], n_total: int,
                       rng: np.random.Generator, jitter: float) -> Dict:
    """
    ã‚ˆã‚Šç²¾å¯†ãªQUBOãƒ™ãƒ¼ã‚¹ã®å˜èªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰
    """
    all_words = list(set(center_words + database))
    energies = {}

    # ä¸­å¿ƒèªã¨ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’è¨ˆç®—
    for w in all_words:
        if w in center_words:
            energies[w] = -3.0  # ä¸­å¿ƒèªã¯éå¸¸ã«ä½ã„ã‚¨ãƒãƒ«ã‚®ãƒ¼
        else:
            e_list = [calculate_energy_between_words(c, w, rng, jitter) for c in center_words]
            energies[w] = float(np.mean(e_list))

    # ã‚¨ãƒãƒ«ã‚®ãƒ¼é †ã«ã‚½ãƒ¼ãƒˆ
    sorted_words = sorted(energies.items(), key=lambda x: x[1])

    # ä¸­å¿ƒèªã‚’å„ªå…ˆçš„ã«é¸æŠ
    selected = []
    for w, _ in sorted_words:
        if w in center_words:
            selected.append(w)
    
    # ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒä½ã„é †ã«è¿½åŠ 
    for w, _ in sorted_words:
        if w not in selected:
            selected.append(w)
        if len(selected) >= n_total:
            break

    # QUBOãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰
    Q = build_qubo_matrix_for_words(selected, rng, jitter)
    
    # ã‚¨ãƒƒã‚¸ã‚’è¨ˆç®—ï¼ˆQUBOã‚¨ãƒãƒ«ã‚®ãƒ¼ã«åŸºã¥ãï¼‰
    edges = []
    center_indices = [i for i, w in enumerate(selected) if w in center_words]
    
    for i in range(len(selected)):
        for j in range(i + 1, len(selected)):
            energy = Q.get((i, j), 0.0)
            # è² ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼ˆçµåˆãŒå¼·ã„ï¼‰ã®ã¿ã‚¨ãƒƒã‚¸ã¨ã—ã¦è¿½åŠ 
            if energy < -0.25:
                edges.append((i, j, energy))

    return {
        "words": selected, 
        "energies": {w: energies[w] for w in selected}, 
        "edges": edges,
        "qubo_matrix": Q,
        "center_indices": center_indices
    }

# =========================
# 5) 3Dé…ç½®ï¼ˆQUBOæœ€é©åŒ–ãƒ™ãƒ¼ã‚¹ï¼‰
# =========================
def place_words_3d(words: List[str], center_set: set, rng: np.random.Generator, 
                   noise: float, network: Dict = None, n_iterations: int = 80,
                   progress_callback=None) -> np.ndarray:
    """
    QUBOæœ€é©åŒ–ã‚’ä½¿ã£ã¦ã‚¨ãƒãƒ«ã‚®ãƒ¼ã«åŸºã¥ã„ãŸ3Dé…ç½®ã‚’ç”Ÿæˆ
    """
    n = len(words)
    
    # QUBOãƒãƒˆãƒªãƒƒã‚¯ã‚¹ãŒæä¾›ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
    if network and "qubo_matrix" in network and "center_indices" in network:
        Q = network["qubo_matrix"]
        center_indices = network["center_indices"]
        energies_dict = network.get("energies", {})
        pos = solve_qubo_placement(Q, n, center_indices, rng, n_iterations=n_iterations,
                                   progress_callback=progress_callback,
                                   energies_dict=energies_dict,
                                   words_list=words)
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¾“æ¥ã®æ–¹æ³•
        pos = np.zeros((n, 3), dtype=float)
        golden_angle = np.pi * (3 - np.sqrt(5))
        for i in range(n):
            w = words[i]
            theta = golden_angle * i
            y = 1 - (i / float(max(1, n - 1))) * 2
            radius_at_y = np.sqrt(max(0.0, 1 - y * y))
            r = 1.0 + rng.uniform(-0.15, 0.20)
            x = np.cos(theta) * radius_at_y * r
            z = np.sin(theta) * radius_at_y * r
            if w in center_set:
                x *= 0.35
                y *= 0.35
                z += 1.10
            pos[i] = [x, y, z]
    
    # æœ€çµ‚çš„ãªæºã‚‰ã
    pos += rng.normal(0, noise, size=pos.shape)
    return pos

# =========================
# 6) æ ¼è¨€é¸æŠï¼ˆå‡ºæ‰€ã¤ãï¼‰
# =========================
def select_relevant_quote(keywords: List[str]) -> Dict[str, str]:
    """
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åŸºã¥ã„ã¦æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„æ ¼è¨€ã‚’é¸æŠ
    """
    if not keywords:
        keywords = ["ä»Š"]
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ­£è¦åŒ–ï¼ˆå°æ–‡å­—åŒ–ã€éƒ¨åˆ†æ–‡å­—åˆ—ã‚‚è€ƒæ…®ï¼‰
    ks_normalized = set()
    for kw in keywords:
        kw_clean = kw.strip().lower()
        ks_normalized.add(kw_clean)
        # éƒ¨åˆ†æ–‡å­—åˆ—ã‚‚è¿½åŠ ï¼ˆä¾‹ï¼šã€Œäººã¨ã®ä¼šè©±ã«ç–²ã‚ŒãŸã€â†’ã€Œç–²ã‚ŒãŸã€ã€Œä¼šè©±ã€ãªã©ï¼‰
        if len(kw_clean) > 2:
            for i in range(len(kw_clean) - 1):
                if len(kw_clean[i:i+2]) >= 2:
                    ks_normalized.add(kw_clean[i:i+2])
    
    best = None
    best_score = -1.0

    for q in FAMOUS_QUOTES:
        quote_keywords = q.get("keywords", [])
        if not quote_keywords:
            continue
        
        # æ ¼è¨€ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚æ­£è¦åŒ–
        quote_kw_normalized = set()
        for qkw in quote_keywords:
            qkw_clean = qkw.strip().lower()
            quote_kw_normalized.add(qkw_clean)
            # éƒ¨åˆ†æ–‡å­—åˆ—ã‚‚è¿½åŠ 
            if len(qkw_clean) > 2:
                for i in range(len(qkw_clean) - 1):
                    if len(qkw_clean[i:i+2]) >= 2:
                        quote_kw_normalized.add(qkw_clean[i:i+2])
        
        # å®Œå…¨ä¸€è‡´ã®ã‚¹ã‚³ã‚¢
        exact_match = len(ks_normalized & quote_kw_normalized)
        
        # éƒ¨åˆ†ä¸€è‡´ã®ã‚¹ã‚³ã‚¢ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæ ¼è¨€ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å«ã¾ã‚Œã‚‹ã€ã¾ãŸã¯ãã®é€†ï¼‰
        partial_match = 0.0
        for kw in ks_normalized:
            for qkw in quote_kw_normalized:
                if kw in qkw or qkw in kw:
                    partial_match += 0.5
        
        # æ ¼è¨€ã®ãƒ†ã‚­ã‚¹ãƒˆå†…ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹å ´åˆã‚‚åŠ ç‚¹
        quote_text = q.get("quote", "").lower()
        text_match = 0.0
        for kw in ks_normalized:
            if len(kw) >= 2 and kw in quote_text:
                text_match += 0.3
        
        # ç·åˆã‚¹ã‚³ã‚¢
        score = exact_match * 2.0 + partial_match + text_match
        
        if score > best_score:
            best_score = score
            best = q

    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆå¿…è¦ã«å¿œã˜ã¦ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰
    # if best:
    #     print(f"é¸æŠã•ã‚ŒãŸæ ¼è¨€: {best.get('quote', '')[:50]}... (ã‚¹ã‚³ã‚¢: {best_score:.2f})")
    
    if best is None or best_score < 0.1:
        return {
            "quote": "ã‚ãªãŸã®è¦³æ¸¬ãŒã€ã“ã®ä¸–ç•Œç·šã‚’ç¢ºå®šã•ã›ã¾ã—ãŸã€‚",
            "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ",
            "note": ""
        }
    
    return {
        "quote": best.get("quote", ""),
        "source": best.get("source", "ä¼çµ±çš„ãªæ•™ãˆ"),
        "note": best.get("note", "")
    }

# =========================
# 7) UI
# =========================
st.title("é‡å­ç¥è¨—ï¼ˆè©¦ä½œï¼‰â€” ç¸ã®çƒä½“ï¼ˆQUBO Ã— ã‚¢ãƒ¼ãƒˆï¼‰")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if "last_user_input" not in st.session_state:
    st.session_state.last_user_input = ""
if "last_params_hash" not in st.session_state:
    st.session_state.last_params_hash = ""

with st.sidebar:
    st.markdown("### ä»Šã®æ°—æŒã¡ï¼ˆå…¥åŠ›ï¼‰")
    user_input = st.text_area(
        "çŸ­ã„ä¸€æ–‡ã§OKï¼ˆä¾‹ï¼šäººã¨ã®ä¼šè©±ã«ç–²ã‚ŒãŸã€‚å°‘ã—è¿·ã£ã¦ã„ã‚‹ã€‚ï¼‰",
        value="äººã¨ã®ä¼šè©±ã«ç–²ã‚ŒãŸã€‚å°‘ã—è¿·ã£ã¦ã„ã‚‹ã€‚",
        height=90,
        key="user_input_text"
    )

    st.markdown("---")
    st.markdown("### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    top_n = st.slider("æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°", 2, 10, 5, 1)
    n_total = st.slider("ç©ºé–“ã«å‡ºã™å˜èªæ•°ï¼ˆä¸­å¿ƒï¼‹å‘¨è¾ºï¼‰", 15, 60, 30, 1)

    auto = st.toggle("ã‚†ã‚‰ãï¼ˆè‡ªå‹•æ›´æ–°ï¼‰", value=True)
    refresh_ms = st.slider("æ›´æ–°é–“éš”(ms)", 200, 1500, 650, 50)

    noise = st.slider("ä½ç½®ã®ã‚†ã‚‰ã", 0.00, 0.20, 0.06, 0.01)
    jitter = st.slider("ã‚¨ãƒãƒ«ã‚®ãƒ¼æºã‚‰ã", 0.00, 0.25, 0.10, 0.01)
    
    # QUBOæœ€é©åŒ–ã®åå¾©å›æ•°ï¼ˆè¨ˆç®—æ™‚é–“ã‚’èª¿æ•´å¯èƒ½ã«ï¼‰
    qubo_iterations = st.slider("QUBOæœ€é©åŒ–ã®åå¾©å›æ•°", 50, 200, 80, 10, 
                                help="å°‘ãªã„ã»ã©é€Ÿã„ãŒã€é…ç½®ã®ç²¾åº¦ã¯ä¸‹ãŒã‚Šã¾ã™")

    st.markdown("---")
    st.markdown("### å®‡å®™ã®å¯†åº¦")
    star_count = st.slider("æ˜Ÿå±‘ã®æ•°", 200, 2200, 900, 50)
    star_twinkle = st.slider("æ˜Ÿã®ã¾ãŸãŸã", 0.00, 0.15, 0.04, 0.01)

    st.markdown("---")
    enable_zoom = st.toggle("ãƒã‚¦ã‚¹ãƒ›ã‚¤ãƒ¼ãƒ«ã§ã‚ºãƒ¼ãƒ ", value=True)
    
    # æ‰‹å‹•æ›´æ–°ãƒœã‚¿ãƒ³
    if st.button("ğŸ”„ å†è¨ˆç®—", use_container_width=True):
        st.session_state.last_user_input = ""  # å¼·åˆ¶çš„ã«å†è¨ˆç®—
        st.rerun()

# å…¥åŠ›ã¾ãŸã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¤‰æ›´ã•ã‚ŒãŸã‹ãƒã‚§ãƒƒã‚¯
params_hash = f"{user_input}_{top_n}_{n_total}_{noise}_{jitter}_{qubo_iterations}"
input_changed = user_input != st.session_state.last_user_input
params_changed = params_hash != st.session_state.last_params_hash
needs_recalc = input_changed or params_changed

# è‡ªå‹•æ›´æ–°ï¼ˆå…¥åŠ›å¤‰æ›´æ™‚ã¯ä¸€æ™‚åœæ­¢ï¼‰
if auto and not needs_recalc:
    # è‡ªå‹•æ›´æ–°ä¸­ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ï¼ˆãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ã«è¡¨ç¤ºï¼‰
    if "network" in st.session_state:
        st.caption(f"ğŸ”„ è‡ªå‹•æ›´æ–°ä¸­ï¼ˆ{refresh_ms}msé–“éš”ï¼‰ - çƒä½“ãŒã‚†ã‚‰ãã¾ã™")
    st_autorefresh(interval=refresh_ms, key="refresh")

# å…¥åŠ›å¤‰æ›´æ™‚ã¯å†è¨ˆç®—ã‚’æ˜ç¤ºçš„ã«å®Ÿè¡Œ
if needs_recalc:
    st.session_state.last_user_input = user_input
    st.session_state.last_params_hash = params_hash

# è¨ˆç®—ä¸­è¡¨ç¤º
if needs_recalc:
    # é€²æ—ãƒãƒ¼ç”¨ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
    progress_placeholder = st.empty()
    
    # RNGï¼ˆæºã‚‰ãã‚’æ¯å›å¤‰ãˆã‚‹ï¼‰
    rng = np.random.default_rng(int(time.time() * 1000) % (2**32 - 1))

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
    with progress_placeholder.container():
        st.info("ğŸ”„ è¨ˆç®—ã‚’é–‹å§‹ã—ã¾ã™...")
        progress_bar = st.progress(0)
        status_text = st.empty()
        status_text.text("ğŸ“ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºä¸­...")
        progress_bar.progress(10)
    
    keywords = extract_keywords(user_input, top_n=top_n)
    center_set = set(keywords)

    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰ï¼ˆQUBOãƒ™ãƒ¼ã‚¹ï¼‰
    with progress_placeholder.container():
        status_text.text("ğŸ”— å˜èªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ä¸­...")
        progress_bar.progress(30)
    
    network = build_word_network(keywords, GLOBAL_WORDS_DATABASE, n_total=n_total, rng=rng, jitter=jitter)

    # 3Dä½ç½®ï¼ˆQUBOæœ€é©åŒ–ãƒ™ãƒ¼ã‚¹ï¼‰- åå¾©å›æ•°ã‚’èª¿æ•´å¯èƒ½ã«
    with progress_placeholder.container():
        status_text.text("ğŸŒ QUBOæœ€é©åŒ–ã§3Dé…ç½®ã‚’è¨ˆç®—ä¸­...")
        progress_bar.progress(50)
    
    # é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
    def update_progress(current, total):
        progress = 50 + int((current / total) * 40)  # 50%ã‹ã‚‰90%ã¾ã§
        with progress_placeholder.container():
            progress_bar.progress(progress)
            status_text.text(f"ğŸŒ QUBOæœ€é©åŒ–ä¸­... ({current}/{total} åå¾©)")
    
    pos = place_words_3d(network["words"], center_set=center_set, rng=rng, noise=noise, 
                        network=network, n_iterations=qubo_iterations,
                        progress_callback=update_progress)
    
    # å®Œäº†
    with progress_placeholder.container():
        progress_bar.progress(100)
        status_text.text("âœ… è¨ˆç®—å®Œäº†ï¼")
        time.sleep(0.2)
    
    # é€²æ—ãƒãƒ¼ã‚’å‰Šé™¤ï¼ˆæ¬¡ã®ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã§è‡ªå‹•çš„ã«ä¸Šæ›¸ãã•ã‚Œã‚‹ï¼‰
    progress_placeholder.empty()
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
    st.session_state.network = network
    st.session_state.pos = pos
    st.session_state.keywords = keywords
    st.session_state.center_set = center_set
else:
    # å‰å›ã®è¨ˆç®—çµæœã‚’ä½¿ç”¨ï¼ˆè‡ªå‹•æ›´æ–°æ™‚ã®æºã‚‰ãã®ã¿ï¼‰
    if "network" in st.session_state and "pos" in st.session_state:
        network = st.session_state.network
        pos = st.session_state.pos.copy()  # ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ
        keywords = st.session_state.keywords
        center_set = st.session_state.center_set
        
        # è‡ªå‹•æ›´æ–°æ™‚ã¯ä½ç½®ã«å°ã•ãªæºã‚‰ãã‚’è¿½åŠ ï¼ˆè¦–è¦šçš„ãªå‹•ãã‚’è¿½åŠ ï¼‰
        if auto:
            rng = np.random.default_rng(int(time.time() * 1000) % (2**32 - 1))
            # ã‚ˆã‚Šè¦–è¦šçš„ã«åˆ†ã‹ã‚‹æºã‚‰ãã‚’è¿½åŠ ï¼ˆè‡ªå‹•æ›´æ–°æ™‚ã®å‹•ãã‚’å¼·èª¿ï¼‰
            pos = pos + rng.normal(0, noise * 0.6, size=pos.shape)
    else:
        # åˆå›å®Ÿè¡Œæ™‚
        progress_container = st.container()
        with progress_container:
            st.info("ğŸ”„ è¨ˆç®—ã‚’é–‹å§‹ã—ã¾ã™...")
            progress_bar = st.progress(0)
            status_text = st.empty()
        
        rng = np.random.default_rng(int(time.time() * 1000) % (2**32 - 1))
        
        with progress_container:
            status_text.text("ğŸ“ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºä¸­...")
            progress_bar.progress(10)
        
        keywords = extract_keywords(user_input, top_n=top_n)
        center_set = set(keywords)
        
        with progress_container:
            status_text.text("ğŸ”— å˜èªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ä¸­...")
            progress_bar.progress(30)
        
        network = build_word_network(keywords, GLOBAL_WORDS_DATABASE, n_total=n_total, rng=rng, jitter=jitter)
        
        with progress_container:
            status_text.text("ğŸŒ QUBOæœ€é©åŒ–ã§3Dé…ç½®ã‚’è¨ˆç®—ä¸­...")
            progress_bar.progress(50)
        
        def update_progress(current, total):
            progress = 50 + int((current / total) * 40)
            progress_bar.progress(progress)
            status_text.text(f"ğŸŒ QUBOæœ€é©åŒ–ä¸­... ({current}/{total} åå¾©)")
        
        pos = place_words_3d(network["words"], center_set=center_set, rng=rng, noise=noise, 
                            network=network, n_iterations=qubo_iterations,
                            progress_callback=update_progress)
        
        with progress_container:
            progress_bar.progress(100)
            status_text.text("âœ… è¨ˆç®—å®Œäº†ï¼")
            time.sleep(0.3)
        
        # é€²æ—ãƒãƒ¼ã¯æ¬¡ã®ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã§è‡ªå‹•çš„ã«ä¸Šæ›¸ãã•ã‚Œã‚‹
        
        st.session_state.network = network
        st.session_state.pos = pos
        st.session_state.keywords = keywords
        st.session_state.center_set = center_set

# =========================
# 8) Plotlyæç”»ï¼ˆæ˜Ÿå±‘ï¼‹ç¸ï¼‹çƒä½“ï¼‹ãƒ©ãƒ™ãƒ«ï¼‰
# =========================
# networkã¨posãŒç¢ºå®Ÿã«å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
try:
    # å¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
    _ = network
    _ = pos
    _ = keywords
    _ = center_set
except NameError:
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰å–å¾—ã‚’è©¦ã¿ã‚‹
    if "network" in st.session_state and "pos" in st.session_state:
        network = st.session_state.network
        pos = st.session_state.pos
        keywords = st.session_state.keywords
        center_set = st.session_state.center_set
    else:
        # åˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        st.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è¨ˆç®—ä¸­ã§ã™...")
        st.info("å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å¤‰æ›´ã™ã‚‹ã‹ã€ã€ŒğŸ”„ å†è¨ˆç®—ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

# ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
if network is None or pos is None or len(network.get("words", [])) == 0:
    st.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒä¸å®Œå…¨ã§ã™ã€‚å†è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

fig = go.Figure()

# æ˜Ÿå±‘ï¼ˆèƒŒæ™¯ï¼‰
star_rng = np.random.default_rng(12345)  # å›ºå®šseedã§ãƒãƒ©ã¤ãæŠ‘åˆ¶
sx = star_rng.uniform(-3.2, 3.2, star_count)
sy = star_rng.uniform(-2.4, 2.4, star_count)
sz = star_rng.uniform(-2.0, 2.0, star_count)

# RNGï¼ˆæ˜Ÿå±‘ç”¨ï¼‰
star_rng_for_twinkle = np.random.default_rng(int(time.time() * 1000) % (2**32 - 1))
tw = np.clip(star_rng_for_twinkle.normal(0, star_twinkle, size=star_count), -0.15, 0.15)
alpha = np.clip(0.22 + tw, 0.10, 0.42)
star_size = star_rng.uniform(1.0, 2.4, star_count)
star_colors = [f"rgba(255,255,255,{a})" for a in alpha]

fig.add_trace(go.Scatter3d(
    x=sx, y=sy, z=sz,
    mode="markers",
    marker=dict(size=star_size, color=star_colors),
    hoverinfo="skip",
    showlegend=False
))

# ä¸­å¿ƒèªã‹ã‚‰å„å˜èªã¸ã®ç·šï¼ˆè·é›¢ã‚’è¦–è¦šåŒ–ï¼‰
center_indices = network.get("center_indices", [])
words = network["words"]
energies_dict = network.get("energies", {})

for center_idx in center_indices:
    if center_idx >= len(words):
        continue
    
    center_word = words[center_idx]
    cx, cy, cz = pos[center_idx]
    
    # ä¸­å¿ƒèªã‹ã‚‰å„å˜èªã¸ã®ç·šã‚’æç”»
    for i, word in enumerate(words):
        if i == center_idx or i in center_indices:
            continue
        
        x, y, z = pos[i]
        energy = energies_dict.get(word, 0.0)
        
        # è·é›¢ã‚’è¨ˆç®—
        distance = np.linalg.norm(pos[i] - pos[center_idx])
        
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼ã«åŸºã¥ã„ã¦ç·šã®å¤ªã•ã¨è‰²ã‚’æ±ºå®š
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒä½ã„ï¼ˆè¿‘ã„ï¼‰ã»ã©å¤ªãæ˜ã‚‹ã
        energy_normalized = min(1.0, abs(energy) / 3.0)
        lw = 1.0 + 3.0 * energy_normalized
        a = 0.3 + 0.5 * energy_normalized
        
        # è·é›¢ã«å¿œã˜ã¦è‰²ã‚’å¤‰ãˆã‚‹ï¼ˆè¿‘ã„=æ˜ã‚‹ã„é’ã€é ã„=è–„ã„é’ï¼‰
        if distance < 1.0:
            color = f"rgba(100,200,255,{a})"  # è¿‘ã„ = æ˜ã‚‹ã„é’
        elif distance < 1.8:
            color = f"rgba(150,200,255,{a * 0.7})"  # ä¸­è·é›¢ = é’
        else:
            color = f"rgba(200,220,255,{a * 0.4})"  # é ã„ = è–„ã„é’
        
        # ä¸­å¿ƒèªã‹ã‚‰å„å˜èªã¸ã®ç·š
        fig.add_trace(go.Scatter3d(
            x=[cx, x], y=[cy, y], z=[cz, z],
            mode="lines",
            line=dict(width=lw, color=color),
            hovertemplate=f"<b>{center_word}</b> â†’ <b>{word}</b><br>" +
                         f"è·é›¢: {distance:.2f}<br>" +
                         f"ã‚¨ãƒãƒ«ã‚®ãƒ¼: {energy:.2f}<extra></extra>",
            showlegend=False
        ))

# å˜èªé–“ã®ã‚¨ãƒƒã‚¸ï¼ˆé–¢é€£ãƒ¯ãƒ¼ãƒ‰åŒå£«ã‚’ç¹‹ãï¼‰
for i, j, e in network["edges"]:
    # ä¸­å¿ƒèªã¯æ—¢ã«ä¸Šã§æç”»æ¸ˆã¿ãªã®ã§ã‚¹ã‚­ãƒƒãƒ—
    if i in center_indices or j in center_indices:
        continue
    
    x0, y0, z0 = pos[i]
    x1, y1, z1 = pos[j]
    
    # è·é›¢ã‚’è¨ˆç®—
    distance = np.linalg.norm(pos[j] - pos[i])
    
    # ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒä½ã„ï¼ˆè² ã®å€¤ãŒå¤§ãã„ï¼‰ã»ã©å¼·ã„çµåˆ
    energy_strength = abs(e)
    normalized_strength = min(1.0, energy_strength / 2.0)
    
    # ç·šã®å¤ªã•ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒä½ã„ã»ã©å¤ªãï¼‰
    lw = 0.5 + 2.0 * normalized_strength
    
    # é€æ˜åº¦ï¼ˆå¼·ã„çµåˆã»ã©æ˜ã‚‹ãï¼‰
    a = min(0.70, 0.20 + 0.40 * normalized_strength)
    
    # è‰²ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ã«å¿œã˜ã¦å¤‰åŒ–ï¼‰
    if e < -1.0:
        color = f"rgba(120,180,255,{a})"  # å¼·ã„çµåˆ = æ˜ã‚‹ã„é’
    elif e < -0.5:
        color = f"rgba(160,200,255,{a})"  # ä¸­ç¨‹åº¦ = é’
    else:
        color = f"rgba(200,200,255,{a})"  # å¼±ã„çµåˆ = è–„ã„é’

    fig.add_trace(go.Scatter3d(
        x=[x0, x1], y=[y0, y1], z=[z0, z1],
        mode="lines",
        line=dict(width=lw, color=color),
        hovertemplate=f"<b>{words[i]}</b> â†” <b>{words[j]}</b><br>" +
                     f"è·é›¢: {distance:.2f}<br>" +
                     f"ã‚¨ãƒãƒ«ã‚®ãƒ¼: {e:.2f}<extra></extra>",
        showlegend=False
    ))

# çƒä½“ï¼ˆè¨€è‘‰ï¼‰- ã‚¨ãƒãƒ«ã‚®ãƒ¼ã«åŸºã¥ã„ã¦ã‚µã‚¤ã‚ºã¨è‰²ã‚’å¤‰ãˆã‚‹
words = network["words"]
energies_dict = network.get("energies", {})

# ã‚¨ãƒãƒ«ã‚®ãƒ¼ã«åŸºã¥ã„ã¦ã‚µã‚¤ã‚ºã¨è‰²ã‚’æ±ºå®š
sizes = []
colors = []
label = []
for w in words:
    energy = energies_dict.get(w, 0.0)
    
    if w in center_set:
        # ä¸­å¿ƒèªã¯å¤§ããæ˜ã‚‹ã
        sizes.append(28)
        colors.append("rgba(255,235,100,0.98)")  # é‡‘è‰²
        label.append(w)
    else:
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒä½ã„ã»ã©å¤§ããæ˜ã‚‹ã
        energy_normalized = min(1.0, abs(energy) / 3.0)
        size = 12 + int(8 * energy_normalized)
        sizes.append(size)
        
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼ã«å¿œã˜ã¦è‰²ã‚’å¤‰ãˆã‚‹
        if energy < -1.5:
            colors.append("rgba(180,220,255,0.85)")  # ä½ã‚¨ãƒãƒ«ã‚®ãƒ¼ = æ˜ã‚‹ã„é’
        elif energy < -0.5:
            colors.append("rgba(220,240,255,0.75)")  # ä¸­ç¨‹åº¦ = è–„ã„é’
        else:
            colors.append("rgba(255,255,255,0.60)")  # é«˜ã‚¨ãƒãƒ«ã‚®ãƒ¼ = ç™½
        
        label.append(w)

# ä¸­å¿ƒèªã¨ãã®ä»–ã®å˜èªã‚’åˆ†ã‘ã¦æç”»ï¼ˆæ–‡å­—ã‚µã‚¤ã‚ºã‚’å€‹åˆ¥ã«åˆ¶å¾¡ï¼‰
center_texts = []
center_positions = []
other_texts = []
other_positions = []
other_sizes = []
other_colors = []

for i, (w, size, color) in enumerate(zip(label, sizes, colors)):
    if w in center_set:
        center_texts.append(w)
        center_positions.append(pos[i])
    else:
        other_texts.append(w)
        other_positions.append(pos[i])
        other_sizes.append(size)
        other_colors.append(color)

# ãã®ä»–ã®å˜èªã‚’æç”»ï¼ˆæ‹¡å¤§ç¸®å°ã«å¿œã˜ã¦ã‚µã‚¤ã‚ºãŒå¤‰ã‚ã‚‹ï¼‰
if other_texts:
    other_positions = np.array(other_positions)
    fig.add_trace(go.Scatter3d(
        x=other_positions[:, 0], y=other_positions[:, 1], z=other_positions[:, 2],
        mode="markers+text",
        text=other_texts,
        textposition="top center",
        textfont=dict(size=18, color="rgba(255,255,255,1.0)"),
        marker=dict(size=other_sizes, color=other_colors, line=dict(width=1, color="rgba(0,0,0,0.10)")),
        hovertemplate="<b>%{text}</b><extra></extra>",
        showlegend=False
    ))

# ä¸­å¿ƒèªã‚’å€‹åˆ¥ã«æç”»ï¼ˆæ‹¡å¤§ç¸®å°ã«å¿œã˜ã¦ã‚µã‚¤ã‚ºãŒå¤‰ã‚ã‚‹ï¼‰
if center_texts:
    center_positions = np.array(center_positions)
    center_indices_in_label = [i for i, w in enumerate(label) if w in center_set]
    center_sizes = [sizes[i] for i in center_indices_in_label]
    center_colors_list = [colors[i] for i in center_indices_in_label]
    
    fig.add_trace(go.Scatter3d(
        x=center_positions[:, 0], y=center_positions[:, 1], z=center_positions[:, 2],
        mode="markers+text",
        text=center_texts,
        textposition="top center",
        textfont=dict(size=24, color="rgba(255,80,80,1.0)"),  # ä¸­å¿ƒèªã¯èµ¤è‰²ã€ã‚µã‚¤ã‚ºã‚’å¤§ãã
        marker=dict(size=center_sizes, color=center_colors_list, line=dict(width=2, color="rgba(255,80,80,0.8)")),
        hovertemplate="<b>%{text}</b><br>ä¸­å¿ƒèª<extra></extra>",
        showlegend=False
    ))

# ä¸­å¿ƒèªã‚’çƒä½“ã¨ã—ã¦è¡¨ç¤ºï¼ˆè–„ã„é’è‰²ã®çƒä½“ï¼‰
for center_idx in center_indices:
    if center_idx >= len(words):
        continue
    
    center_word = words[center_idx]
    cx, cy, cz = pos[center_idx]
    
    # ä¸­å¿ƒèªã®çƒä½“ï¼ˆè–„ã„é’è‰²ã€è¤‡æ•°ã®å±¤ã§ç«‹ä½“æ„Ÿã‚’å‡ºã™ï¼‰
    for layer, size_mult in enumerate([1.0, 1.3, 1.6], 1):
        opacity = 0.15 / layer  # å¤–å´ã»ã©è–„ã
        fig.add_trace(go.Scatter3d(
            x=[cx], y=[cy], z=[cz],
            mode="markers",
            marker=dict(
                size=[35 * size_mult],
                color=f"rgba(150,200,255,{opacity})",
                line=dict(width=0)
            ),
            hoverinfo="skip",
            showlegend=False
        ))
    
    # ã€ã€‘ä»˜ãã®ãƒ†ã‚­ã‚¹ãƒˆã¯å‰Šé™¤ï¼ˆ892-908è¡Œç›®ã§æ—¢ã«æç”»æ¸ˆã¿ï¼‰

fig.update_layout(
    paper_bgcolor="rgba(6,8,18,1)",
    scene=dict(
        xaxis=dict(visible=False),
        yaxis=dict(visible=False),
        zaxis=dict(visible=False),
        bgcolor="rgba(6,8,18,1)",
        camera=dict(
            eye=dict(x=1.6, y=1.15, z=1.05),
            center=dict(x=0, y=0, z=0),  # ä¸­å¿ƒã‚’åŸç‚¹ã«è¨­å®š
            up=dict(x=0, y=1, z=0)
        ),
        # ãƒ‰ãƒ©ãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’è¨­å®šï¼ˆå·¦ã‚¯ãƒªãƒƒã‚¯ã§å›è»¢è»¸ã‚’å¤‰æ›´å¯èƒ½ã«ï¼‰
        dragmode="orbit"  # orbitãƒ¢ãƒ¼ãƒ‰ã§å·¦ã‚¯ãƒªãƒƒã‚¯ä½ç½®ã‚’ä¸­å¿ƒã«å›è»¢
    ),
    margin=dict(l=0, r=0, t=0, b=0),
)

plotly_config = {
    "displayModeBar": True,
    "scrollZoom": bool(enable_zoom),
    "displaylogo": False,
    "responsive": True,
    "toImageButtonOptions": {
        "format": "png",
        "filename": "quantum_oracle",
        "height": 800,
        "width": 1200,
        "scale": 1
    },
    # æ‹¡å¤§ç¸®å°ã«å¿œã˜ã¦æ–‡å­—ã‚µã‚¤ã‚ºã‚‚å¤‰ã‚ã‚‹ã‚ˆã†ã«
    "doubleClick": "reset",
    "modeBarButtonsToAdd": ["select2d", "lasso2d"],
}

# =========================
# 9) ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆå·¦ï¼šå®‡å®™ / å³ï¼šæ ¼è¨€+å‡ºæ‰€ï¼‰
# =========================
left, right = st.columns([2.0, 1.0], gap="large")

with left:
    # è‡ªå‹•æ›´æ–°æ™‚ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
    if auto and not needs_recalc and "network" in st.session_state:
        st.caption(f"ğŸ”„ è‡ªå‹•æ›´æ–°ä¸­ï¼ˆ{refresh_ms}msé–“éš”ï¼‰ - çƒä½“ãŒã‚†ã‚‰ãã¾ã™")
    
    st.plotly_chart(fig, use_container_width=True, config=plotly_config)
    st.caption("å˜èªï¼ˆçƒä½“ï¼‰ã¨ç¸ï¼ˆç·šï¼‰ã€‚ãƒã‚¦ã‚¹ã§å›è»¢ãƒ»ã‚ºãƒ¼ãƒ ã§ãã¾ã™ã€‚")

with right:
    # å³ä¸Šã®ç©ºæ¬„ã‚’æ´»ç”¨ï¼šç¾åœ¨ã®çŠ¶æ…‹ã‚„çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    st.markdown("### ğŸ“Š ç¾åœ¨ã®çŠ¶æ…‹")
    st.markdown(f"**è¨ˆç®—æ¸ˆã¿å˜èªæ•°**: {len(network.get('words', []))}èª")
    st.markdown(f"**æ¥ç¶šæ•°**: {len(network.get('edges', []))}æœ¬")
    if "energies" in network:
        min_energy = min(network["energies"].values()) if network["energies"] else 0.0
        max_energy = max(network["energies"].values()) if network["energies"] else 0.0
        st.markdown(f"**ã‚¨ãƒãƒ«ã‚®ãƒ¼ç¯„å›²**: {min_energy:.2f} ï½ {max_energy:.2f}")
    st.markdown("---")
    
    # â€œã‚«ãƒ¼ãƒ‰â€
    st.markdown(
        """
        <div style="
          background: rgba(255,255,255,0.06);
          border: 1px solid rgba(255,255,255,0.10);
          border-radius: 18px;
          padding: 16px 16px 10px 16px;
          box-shadow: 0 18px 60px rgba(0,0,0,0.18);
        ">
        """,
        unsafe_allow_html=True
    )

    st.markdown("### å…ˆäººã®ã“ã¨ã°")
    st.markdown(f"**ã„ã¾ã®æ ¸ï¼ˆæ¨å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰**ï¼š`{', '.join(keywords)}`")
    st.markdown("---")

    q = select_relevant_quote(keywords)
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆå±•é–‹å¯èƒ½ï¼‰
    with st.expander("ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆæ ¼è¨€é¸æŠï¼‰", expanded=False):
        st.write(f"**æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: {keywords}")
        st.write(f"**åˆ©ç”¨å¯èƒ½ãªæ ¼è¨€æ•°**: {len(FAMOUS_QUOTES)}ä»¶")
        excel_quotes_count = len(st.session_state.get("excel_quotes", []))
        st.write(f"**Excelã‹ã‚‰èª­ã¿è¾¼ã‚“ã æ ¼è¨€æ•°**: {excel_quotes_count}ä»¶")
        if keywords:
            st.write(f"**é¸æŠã•ã‚ŒãŸæ ¼è¨€**: {q.get('quote', '')[:100]}...")
            st.write(f"**å‡ºæ‰€**: {q.get('source', 'â€”')}")

    st.markdown("### ğŸµ éŸ³æ¥½")
    st.session_state.bgm_on = st.toggle("BGMã‚’å†ç”Ÿ", value=st.session_state.bgm_on)

    if st.session_state.bgm_on and BGM_PATH.exists():
        st.audio(str(BGM_PATH), format="audio/mp3")
    
    st.markdown(f"#### ã€Œ{q['quote']}ã€")
    st.markdown("---")
    st.markdown(f"**å‡ºæ‰€ï¼š** {q.get('source','â€”') if q.get('source') else 'â€”'}")
    if q.get("note"):
        st.markdown(f"<div style='opacity:0.80; font-size:0.92rem;'>â€» {q['note']}</div>", unsafe_allow_html=True)

    st.markdown("")
    st.markdown("### æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆç¢ºèªï¼‰")
    for k in keywords:
        st.markdown(f"- {k}")

    st.markdown("</div>", unsafe_allow_html=True)
