# -*- coding: utf-8 -*-
# ============================================================
# QUBO Ã— é‡å­ç¥è¨— UIï¼ˆStreamlit + Plotlyï¼‰
# æ”¹å–„ç‰ˆï¼š
# - ç·šãƒˆãƒ¬ãƒ¼ã‚¹ã‚’é›†ç´„ï¼ˆé«˜é€ŸåŒ–ãƒ»ãƒãƒ©ã¤ãæ¸›ï¼‰
# - seedã‚’ã€Œå…¥åŠ›+ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ç”±æ¥ã«ã—ã¦é™çš„é…ç½®
# - BGM UI ã‚’1ç®‡æ‰€ã«çµ±åˆ
# - é‡è¤‡ã‚³ãƒ¼ãƒ‰é™¤å»ï¼ˆsizes/colors/labelsï¼‰
# - ã€ŒQUBOã®èª¬æ˜ã€ã‚’å¯è¦–åŒ–ï¼ˆQè¡Œåˆ—ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—/ä¸Šä½ç›¸äº’ä½œç”¨ï¼‰
# - ã€Œæ°—ã«ãªã‚‹å˜èªâ†’æ ¼è¨€å€™è£œã€å°ç·šã‚’è¿½åŠ 
# ============================================================

import os
import re
import zlib
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import numpy as np
import plotly.graph_objects as go
import streamlit as st

# pandasï¼ˆExcelèª­ã¿è¾¼ã¿ç”¨ï¼‰
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except Exception:
    PANDAS_AVAILABLE = False
    pd = None


# ============================================================
# 0) ãƒšãƒ¼ã‚¸è¨­å®š + CSS
# ============================================================
st.set_page_config(page_title="é‡å­ç¥è¨— - ç¸ã®çƒä½“", layout="wide")

SPACE_CSS = """
<style>
.stApp{
  background:
    radial-gradient(circle at 18% 24%, rgba(110,150,255,0.12), transparent 38%),
    radial-gradient(circle at 78% 68%, rgba(255,160,220,0.08), transparent 44%),
    radial-gradient(circle at 50% 50%, rgba(255,255,255,0.03), transparent 55%),
    linear-gradient(180deg, rgba(6,8,18,1), rgba(10,12,26,1));
}
.block-container{ padding-top: 1.2rem; }
div[data-testid="stMarkdownContainer"] p,
div[data-testid="stMarkdownContainer"] li{
  font-family: "Hiragino Mincho ProN", "Yu Mincho", "Noto Serif JP", serif;
  letter-spacing: 0.02em;
  color: rgba(245,245,255,0.92);
}
h1,h2,h3{
  font-family: "Hiragino Mincho ProN", "Yu Mincho", "Noto Serif JP", serif !important;
  font-weight: 600 !important;
  color: rgba(245,245,255,0.95);
}
section[data-testid="stSidebar"]{
  background: rgba(255,255,255,0.08);
  border-right: 1px solid rgba(255,255,255,0.10);
  backdrop-filter: blur(10px);
}
div[data-testid="stPlotlyChart"] > div{
  position: relative;
  border-radius: 18px;
  overflow: hidden;
  box-shadow: 0 18px 60px rgba(0,0,0,0.30);
}
div[data-testid="stPlotlyChart"] > div::after{
  content:"";
  position:absolute;
  inset:0;
  background:
    radial-gradient(circle at 30% 25%, rgba(120,160,255,0.10), transparent 45%),
    radial-gradient(circle at 70% 65%, rgba(255,180,220,0.06), transparent 52%),
    radial-gradient(circle at 50% 50%, rgba(0,0,0,0.00), rgba(0,0,0,0.38));
  pointer-events:none;
}
.smallnote{opacity:0.78; font-size:0.92rem;}
</style>
"""
st.markdown(SPACE_CSS, unsafe_allow_html=True)


# ============================================================
# 0.5) ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ åˆæœŸåŒ–
# ============================================================
def init_session_state():
    defaults = {
        "bgm_on": False,
        "last_params_hash": "",
        "network": None,
        "pos": None,
        "keywords": [],
        "center_set": set(),
        "selected_word": "",
    }
    for k, v in defaults.items():
        if k not in st.session_state:
            st.session_state[k] = v

init_session_state()


# ============================================================
# 1) ã‚°ãƒ­ãƒ¼ãƒãƒ«å˜èªDBï¼ˆä»–ã®äººã®è¨€è‘‰ï¼‰
# ============================================================
GLOBAL_WORDS_DATABASE = [
    "ä¸–ç•Œå¹³å’Œ","è²¢çŒ®","æˆé•·","å­¦ã³","æŒ‘æˆ¦","å¤¢","å¸Œæœ›","æœªæ¥",
    "æ„Ÿè¬","æ„›","å¹¸ã›","å–œã³","å®‰å¿ƒ","å……å®Ÿ","æº€è¶³","å¹³å’Œ",
    "åŠªåŠ›","ç¶™ç¶š","å¿è€","èª å®Ÿ","æ­£ç›´","å„ªã—ã•","æ€ã„ã‚„ã‚Š","å…±æ„Ÿ",
    "èª¿å’Œ","ãƒãƒ©ãƒ³ã‚¹","è‡ªç„¶","ç¾","çœŸå®Ÿ","è‡ªç”±","æ­£ç¾©","é“",
    "çµ†","ã¤ãªãŒã‚Š","å®¶æ—","å‹äºº","ä»²é–“","ä¿¡é ¼","å°Šæ•¬","å”åŠ›",
    "ä»Š","ç¬é–“","éç¨‹","å¤‰åŒ–","é€²åŒ–","ç™ºå±•","å¾ªç’°","æµã‚Œ",
    "é™ã‘ã•","é›†ä¸­","è¦šæ‚Ÿ","æ±ºæ„","å‹‡æ°—","å¼·ã•","æŸ”è»Ÿæ€§","å¯›å®¹",
]

CATEGORIES = {
    "é¡˜ã„": ["ä¸–ç•Œå¹³å’Œ","è²¢çŒ®","æˆé•·","å¤¢","å¸Œæœ›","æœªæ¥"],
    "æ„Ÿæƒ…": ["æ„Ÿè¬","æ„›","å¹¸ã›","å–œã³","å®‰å¿ƒ","æº€è¶³","å¹³å’Œ"],
    "è¡Œå‹•": ["åŠªåŠ›","ç¶™ç¶š","å¿è€","èª å®Ÿ","æ­£ç›´"],
    "å“²å­¦": ["èª¿å’Œ","ãƒãƒ©ãƒ³ã‚¹","è‡ªç„¶","ç¾","é“","çœŸå®Ÿ","è‡ªç”±","æ­£ç¾©"],
    "é–¢ä¿‚": ["çµ†","ã¤ãªãŒã‚Š","å®¶æ—","å‹äºº","ä»²é–“","ä¿¡é ¼","å°Šæ•¬","å”åŠ›"],
    "å†…çš„": ["é™ã‘ã•","é›†ä¸­","è¦šæ‚Ÿ","æ±ºæ„","å‹‡æ°—","å¼·ã•","æŸ”è»Ÿæ€§","å¯›å®¹"],
    "æ™‚é–“": ["ä»Š","ç¬é–“","éç¨‹","å¤‰åŒ–","é€²åŒ–","ç™ºå±•","å¾ªç’°","æµã‚Œ"],
}

# ============================================================
# 2) æ ¼è¨€DBï¼ˆå‡ºæ‰€ã‚‚æŒãŸã›ã‚‹ï¼‰
# ============================================================
BASE_FAMOUS_QUOTES = [
    {
        "keywords": ["å¹³å’Œ","ä¸–ç•Œ","è²¢çŒ®","å¸Œæœ›"],
        "quote": "é›ªã®ä¸‹ã§ç¨®ã¯æ˜¥ã‚’å¾…ã£ã¦ã„ã‚‹ã€‚ç„¦ã‚‹ã¹ã‹ã‚‰ãšã€æ™‚æº€ã¡ã‚‹ã‚’å¾…ã¦ã€‚",
        "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ/å¯“è©±èª¿",
        "note": "å…¬é–‹ç”¨ã«å…¸æ‹ ä»˜ãæ ¼è¨€ã¸å·®ã—æ›¿ãˆå¯"
    },
    {
        "keywords": ["æˆé•·","åŠªåŠ›","ç¶™ç¶š","æŒ‘æˆ¦"],
        "quote": "åƒé‡Œã®é“ã‚‚ä¸€æ­©ã‹ã‚‰ã€‚æ­©ã¿ã‚’æ­¢ã‚ãšã€ç¶šã‘ã‚‹ã“ã¨ã«æ„å‘³ãŒã‚ã‚‹ã€‚",
        "source": "æ•…äº‹æˆèªï¼ˆè¦å…¸æ‹ ç¢ºèªï¼‰â€”æš«å®š",
        "note": "å…¬é–‹å‰ã«å…¸æ‹ ç²¾æŸ»æ¨å¥¨"
    },
    {
        "keywords": ["æ„Ÿè¬","æ„›","çµ†","ã¤ãªãŒã‚Š"],
        "quote": "ä¸€æœŸä¸€ä¼šã€‚ä»Šã“ã®ç¬é–“ã‚’å¤§åˆ‡ã«ã€‚ã™ã¹ã¦ã¯ç¸ã§ç¹‹ãŒã£ã¦ã„ã‚‹ã€‚",
        "source": "ä¸€æœŸä¸€ä¼šï¼ˆèŒ¶é“æ€æƒ³ï¼‰ï¼‹é‡å­ç¥è¨— è©¦ä½œï¼ˆç·¨é›†/æ„è¨³ï¼‰",
        "note": ""
    },
    {
        "keywords": ["è‡ªç„¶","èª¿å’Œ","ãƒãƒ©ãƒ³ã‚¹","æµã‚Œ"],
        "quote": "æ°´ã¯ã€äº‰ã‚ãªã„ã€‚å½¢ã«ã“ã ã‚ã‚‰ãšã€æµã‚Œã‚‹ãŒã¾ã¾ã«ã€‚",
        "source": "è€å­ã€é“å¾³çµŒã€ï¼ˆä¸Šå–„è‹¥æ°´ï¼‰â€”æ„è¨³/ç·¨é›†",
        "note": "å³å¯†ãªåŸæ–‡å¼•ç”¨ã§ã¯ãªãæ„è¨³"
    },
    {
        "keywords": ["é™ã‘ã•","é›†ä¸­","ä»Š","ç¬é–“"],
        "quote": "æ­¢ã¾ã‚‹ã“ã¨ã§ã€æµã‚ŒãŒè¦‹ãˆã‚‹ã€‚å‹•ã®ä¸­ã«é™ãŒã‚ã‚‹ã€‚",
        "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ",
        "note": ""
    },
    {
        "keywords": ["å‹‡æ°—","æ±ºæ„","æŒ‘æˆ¦","é“"],
        "quote": "é“ãŒåˆ†ã‹ã‚Œã¦ã„ãŸã‚‰ã€å¿µã®ãªã„æ–¹ã¸è¡Œã‘ã€‚",
        "source": "å‡ºå…¸è¦ç¢ºèªï¼ˆæµé€šå¥ï¼‰â€”æš«å®š",
        "note": "å…¬é–‹å‰ã«å…¸æ‹ ã‚’ç¢ºå®šæ¨å¥¨"
    },
]

EXCEL_DEFAULT = "quantum_shintaku_pack_v3_with_sense_20260213_oposite_modify_with_lr022101.xlsx"

@st.cache_data(show_spinner=False)
def load_quotes_from_excel_cached(excel_path: str) -> List[Dict]:
    if (not PANDAS_AVAILABLE) or (not excel_path) or (not os.path.exists(excel_path)):
        return []
    try:
        df = pd.read_excel(excel_path, sheet_name="QUOTES", engine="openpyxl")
    except Exception:
        return []

    def pick_text(row, candidates):
        for col in candidates:
            if col in df.columns:
                v = str(row.get(col, "")).strip()
                if v and v.lower() not in ("nan", "none"):
                    return v
        return ""

    quotes: List[Dict] = []
    for _, row in df.iterrows():
        quote_text = pick_text(row, ["æ ¼è¨€", "QUOTE", "Quote", "quote", "ãƒ†ã‚­ã‚¹ãƒˆ", "æ–‡", "è¨€è‘‰"])
        if not quote_text:
            continue
        kw_str = pick_text(row, ["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "KEYWORDS", "Keywords", "keywords", "ã‚¿ã‚°", "TAG", "Tag"])
        keywords = [k.strip() for k in kw_str.replace("ã€", ",").split(",") if k.strip()] if kw_str else []
        source = pick_text(row, ["å‡ºå…¸", "SOURCE", "Source", "source", "å‡ºæ‰€", "å…¸æ‹ ", "ä½œè€…"]) or "ä¼çµ±çš„ãªæ•™ãˆ"
        note = pick_text(row, ["å‚™è€ƒ", "NOTE", "Note", "note", "æ³¨", "ãƒ¡ãƒ¢"])
        quotes.append({"quote": quote_text, "keywords": keywords, "source": source, "note": note})
    return quotes

def build_famous_quotes() -> List[Dict]:
    fam = list(BASE_FAMOUS_QUOTES)
    excel_quotes = load_quotes_from_excel_cached(EXCEL_DEFAULT)
    if excel_quotes:
        existing = {q.get("quote", "") for q in fam}
        for q in excel_quotes:
            qt = q.get("quote", "")
            if qt and qt not in existing:
                fam.append(q)
                existing.add(qt)
    return fam

FAMOUS_QUOTES = build_famous_quotes()


# ============================================================
# 3) ãƒ†ã‚­ã‚¹ãƒˆâ†’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆç°¡æ˜“ãƒ»æ”¹å–„ï¼‰
#    - ã€Œã—ãŸ/ãŸã„/ã„ã€ãªã©ã‚’è½ã¨ã™æœ€ä½é™ã®æ—¥æœ¬èªãƒ•ã‚£ãƒ«ã‚¿
# ============================================================
STOP_TOKENS = set([
    "ã—ãŸ","ãŸã„","ã„ã‚‹","ã„","ã“ã¨","ãã‚Œ","ã“ã‚Œ","ãŸã‚","ã‚ˆã†","ã®ã§","ã‹ã‚‰",
    "ã§ã™","ã¾ã™","ã§ã™ã€‚","ã¾ã™ã€‚","ã‚ã‚‹","ãªã„","ãã—ã¦","ã§ã‚‚","ã—ã‹ã—","ã¾ãŸ",
    "è‡ªåˆ†","ç§","ã‚ãªãŸ","ã‚‚ã®","æ„Ÿã˜","æ°—æŒã¡"
])

def extract_keywords(text: str, top_n: int = 5) -> List[str]:
    text = (text or "").strip()
    if not text:
        return ["é™ã‘ã•", "è¿·ã„"]

    # ã¾ãšã¯DBèªã®ç›´æ¥ãƒ’ãƒƒãƒˆï¼ˆå„ªå…ˆï¼‰
    found = [w for w in GLOBAL_WORDS_DATABASE if w in text]
    if found:
        return found[:top_n]

    # é›‘ã«åˆ†å‰²ï¼ˆæ—¥æœ¬èªã¯å½¢æ…‹ç´ ãŒç†æƒ³ã ãŒã€ä¾å­˜å¢—ã‚„ã•ãªã„æ–¹é‡ã§æœ€ä½é™ï¼‰
    text_clean = re.sub(r"[0-9ï¼-ï¼™ã€ã€‚ï¼,.!ï¼?ï¼Ÿ\(\)\[\]{}ã€Œã€ã€ã€\"'ï¼š:;ï¼/\\\n\r\t]+", " ", text)
    tokens = [t.strip() for t in re.split(r"\s+", text_clean) if t.strip()]

    # 2æ–‡å­—ä»¥ä¸Š + ã‚¹ãƒˆãƒƒãƒ—é™¤å¤–
    tokens = [t for t in tokens if (len(t) >= 2 and t not in STOP_TOKENS)]
    if not tokens:
        return ["é™ã‘ã•", "è¿·ã„"]

    # ä¸Šä½Nï¼ˆé•·ã„èªã‚’å°‘ã—å„ªå…ˆï¼‰
    tokens = sorted(tokens, key=lambda s: (-len(s), s))
    return tokens[:top_n]


# ============================================================
# 4) â€œã‚¨ãƒãƒ«ã‚®ãƒ¼â€è¨ˆç®—ï¼ˆQUBOçš„ç›¸äº’ä½œç”¨ï¼‰
# ============================================================
def calculate_semantic_similarity(word1: str, word2: str) -> float:
    if word1 == word2:
        return 1.0

    common_chars = set(word1) & set(word2)
    char_sim = len(common_chars) / max(len(set(word1)), len(set(word2)), 1)

    category_sim = 0.0
    for _, ws in CATEGORIES.items():
        w1_in = word1 in ws
        w2_in = word2 in ws
        if w1_in and w2_in:
            category_sim = 1.0
            break
        elif w1_in or w2_in:
            category_sim = max(category_sim, 0.3)

    len_sim = 1.0 - abs(len(word1) - len(word2)) / max(len(word1), len(word2), 1)
    similarity = 0.4 * char_sim + 0.4 * category_sim + 0.2 * len_sim
    return float(np.clip(similarity, 0.0, 1.0))

def calculate_energy_between_words(word1: str, word2: str, rng: np.random.Generator, jitter: float) -> float:
    similarity = calculate_semantic_similarity(word1, word2)
    # ã€Œä¼¼ã¦ã‚‹ã»ã©ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒä½ã„ï¼ˆ=çµã³ã¤ãï¼‰ã€è¨­è¨ˆ
    energy = -2.0 * similarity + 0.5

    common = set(word1) & set(word2)
    if common:
        energy -= 0.20 * len(common) / max(len(word1), len(word2), 1)

    for _, ws in CATEGORIES.items():
        if (word1 in ws) and (word2 in ws):
            energy -= 0.60
            break

    if jitter > 0:
        energy += rng.normal(0, jitter)
    return float(energy)

def build_qubo_matrix_for_words(words: List[str], rng: np.random.Generator, jitter: float) -> np.ndarray:
    n = len(words)
    Q = np.zeros((n, n), dtype=float)
    np.fill_diagonal(Q, -0.5)
    for i in range(n):
        for j in range(i + 1, n):
            e = calculate_energy_between_words(words[i], words[j], rng, jitter)
            Q[i, j] = e
            Q[j, i] = e
    return Q

def solve_qubo_placement(
    Q: np.ndarray,
    words: List[str],
    center_indices: List[int],
    energies: Dict[str, float],
    rng: np.random.Generator,
    n_iterations: int = 100,
    progress_callback=None,
) -> np.ndarray:
    n = len(words)
    pos = np.zeros((n, 3), dtype=float)
    for idx in center_indices:
        if idx < n:
            pos[idx] = [0.0, 0.0, 0.0]

    # ã‚¨ãƒãƒ«ã‚®ãƒ¼â†’è·é›¢ï¼ˆä½ã„ã»ã©è¿‘ã„ï¼‰
    ev = list(energies.values()) if energies else []
    if ev:
        mn, mx = min(ev), max(ev)
        er = (mx - mn) if mx != mn else 1.0
    else:
        mn, er = -3.0, 3.0

    golden_angle = np.pi * (3 - np.sqrt(5))
    k = 0
    for i in range(n):
        if i in center_indices:
            continue
        w = words[i]
        e = energies.get(w, 0.0)
        norm = (e - mn) / er
        dist = 0.3 + (1.0 - norm) * 2.2

        theta = golden_angle * k
        y = 1 - (k / float(max(1, n - len(center_indices) - 1))) * 2
        r = np.sqrt(max(0.0, 1 - y * y))
        x = np.cos(theta) * r * dist
        z = np.sin(theta) * r * dist
        pos[i] = [x, y * dist * 0.6, z]
        k += 1

    # ç–‘ä¼¼åŠ›å­¦ã§æ•´ãˆã‚‹
    for it in range(n_iterations):
        for i in range(n):
            if i in center_indices:
                continue
            force = np.zeros(3, dtype=float)

            # ä¸­å¿ƒã¨ã®è·é›¢ã‚’ä¿ã¤
            for cidx in center_indices:
                vec = pos[cidx] - pos[i]
                d = np.linalg.norm(vec)
                if d > 0.01:
                    w = words[i]
                    e = energies.get(w, 0.0)
                    norm = (e - mn) / er if er > 0 else 0.5
                    target = 0.3 + (1.0 - norm) * 2.2
                    if d < target * 0.9:
                        force -= vec / d * 0.05
                    elif d > target * 1.1:
                        force += vec / d * 0.10

            # Qã®ç›¸äº’ä½œç”¨ï¼ˆå¼•åŠ›/æ–¥åŠ›ï¼‰
            for j in range(n):
                if i == j or j in center_indices:
                    continue
                eij = Q[i, j]
                if eij < -0.3:
                    vec = pos[j] - pos[i]
                    d = np.linalg.norm(vec)
                    if d > 0.01:
                        force += vec / d * (abs(eij) * 0.08)
                elif eij > 0.2:
                    vec = pos[i] - pos[j]
                    d = np.linalg.norm(vec)
                    if d > 0.01:
                        force += vec / d * (abs(eij) * 0.03)

            pos[i] += force * 0.15

        if progress_callback:
            progress_callback(it + 1, n_iterations)

    return pos


def build_word_network(center_words: List[str], database: List[str], n_total: int,
                       rng: np.random.Generator, jitter: float) -> Dict:
    all_words = list(dict.fromkeys(center_words + database))  # é †åºç¶­æŒ
    energies: Dict[str, float] = {}

    for w in all_words:
        if w in center_words:
            energies[w] = -3.0
        else:
            e_list = [calculate_energy_between_words(c, w, rng, jitter) for c in center_words]
            energies[w] = float(np.mean(e_list))

    sorted_words = sorted(energies.items(), key=lambda x: x[1])  # ä½ã„ã»ã©ä¸­å¿ƒã«è¿‘ã„
    selected: List[str] = []

    for w, _ in sorted_words:
        if w in center_words and w not in selected:
            selected.append(w)
    for w, _ in sorted_words:
        if w not in selected:
            selected.append(w)
        if len(selected) >= n_total:
            break

    Q = build_qubo_matrix_for_words(selected, rng, jitter)

    center_indices = [i for i, w in enumerate(selected) if w in center_words]

    # ã‚¨ãƒƒã‚¸æŠ½å‡º
    edges: List[Tuple[int, int, float]] = []
    n = len(selected)
    for i in range(n):
        for j in range(i + 1, n):
            e = Q[i, j]
            if e < -0.25:
                edges.append((i, j, float(e)))

    return {
        "words": selected,
        "energies": {w: energies[w] for w in selected},
        "edges": edges,
        "Q": Q,
        "center_indices": center_indices,
    }


# ============================================================
# 5) æ ¼è¨€é¸æŠï¼ˆå‡ºæ‰€ã¤ãï¼‰
# ============================================================
def select_relevant_quote(keywords: List[str]) -> Dict[str, str]:
    if not keywords:
        keywords = ["ä»Š"]

    ks = set()
    for kw in keywords:
        k = kw.strip().lower()
        ks.add(k)
        if len(k) > 2:
            for i in range(len(k) - 1):
                ks.add(k[i:i+2])

    best = None
    best_score = -1.0

    for q in FAMOUS_QUOTES:
        qk = q.get("keywords", [])
        if not qk:
            continue

        qks = set()
        for k in qk:
            kk = k.strip().lower()
            qks.add(kk)
            if len(kk) > 2:
                for i in range(len(kk) - 1):
                    qks.add(kk[i:i+2])

        exact = len(ks & qks)

        partial = 0.0
        for a in ks:
            for b in qks:
                if a in b or b in a:
                    partial += 0.5

        text = q.get("quote", "").lower()
        text_match = 0.0
        for a in ks:
            if len(a) >= 2 and a in text:
                text_match += 0.3

        score = exact * 2.0 + partial + text_match
        if score > best_score:
            best_score = score
            best = q

    if best is None or best_score < 0.1:
        return {"quote": "ã‚ãªãŸã®è¦³æ¸¬ãŒã€ã“ã®ä¸–ç•Œç·šã‚’ç¢ºå®šã•ã›ã¾ã—ãŸã€‚", "source": "é‡å­ç¥è¨— è©¦ä½œï¼ˆç¦ç”°é›…å½¦ï¼‰â€”å‰µä½œ", "note": ""}

    return {"quote": best.get("quote", ""), "source": best.get("source", "ä¼çµ±çš„ãªæ•™ãˆ"), "note": best.get("note", "")}

def quote_candidates_for_word(word: str, max_n: int = 6) -> List[Dict]:
    if not word:
        return []
    w = word.strip().lower()
    scored = []
    for q in FAMOUS_QUOTES:
        ks = [k.strip().lower() for k in q.get("keywords", [])]
        score = 0.0
        if w in ks:
            score += 3.0
        else:
            # éƒ¨åˆ†ä¸€è‡´
            for k in ks:
                if w in k or k in w:
                    score += 1.0
        # æœ¬æ–‡ã«å«ã¾ã‚Œã‚‹ã‹
        if w in (q.get("quote","").lower()):
            score += 0.5
        if score > 0:
            scored.append((score, q))
    scored.sort(key=lambda x: (-x[0], x[1].get("quote","")))
    return [q for _, q in scored[:max_n]]


# ============================================================
# 6) UIï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼‰
# ============================================================
st.title("é‡å­ç¥è¨—ï¼ˆè©¦ä½œï¼‰â€” ç¸ã®çƒä½“ï¼ˆQUBO Ã— ã‚¢ãƒ¼ãƒˆï¼‰")

BGM_PATH = Path("assets/bgm.mp3")  # mp3æ¨å¥¨
BGM_FORMAT = "audio/mpeg"

with st.sidebar:
    st.markdown("### ä»Šã®æ°—æŒã¡ï¼ˆå…¥åŠ›ï¼‰")
    user_input = st.text_area(
        "çŸ­ã„ä¸€æ–‡ã§OKï¼ˆä¾‹ï¼šäººã¨ã®ä¼šè©±ã«ç–²ã‚ŒãŸã€‚å°‘ã—è¿·ã£ã¦ã„ã‚‹ã€‚ï¼‰",
        value="äººã¨ã®ä¼šè©±ã«ç–²ã‚ŒãŸã€‚å°‘ã—è¿·ã£ã¦ã„ã‚‹ã€‚",
        height=90,
        key="user_input_text",
    )

    st.markdown("---")
    st.markdown("### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    top_n = st.slider("æŠ½å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°", 2, 10, 5, 1)
    n_total = st.slider("ç©ºé–“ã«å‡ºã™å˜èªæ•°ï¼ˆä¸­å¿ƒï¼‹å‘¨è¾ºï¼‰", 15, 60, 30, 1)
    noise = st.slider("ä½ç½®ã®ã‚†ã‚‰ãï¼ˆå†è¨ˆç®—æ™‚ã®ã¿ï¼‰", 0.00, 0.20, 0.06, 0.01)
    jitter = st.slider("ã‚¨ãƒãƒ«ã‚®ãƒ¼æºã‚‰ã", 0.00, 0.25, 0.10, 0.01)
    qubo_iterations = st.slider("QUBOæœ€é©åŒ–ã®åå¾©å›æ•°", 50, 200, 80, 10)

    st.markdown("---")
    st.markdown("### å®‡å®™ã®å¯†åº¦")
    star_count = st.slider("æ˜Ÿå±‘ã®æ•°", 200, 2200, 900, 50)

    st.markdown("---")
    enable_zoom = st.toggle("ãƒã‚¦ã‚¹ãƒ›ã‚¤ãƒ¼ãƒ«ã§ã‚ºãƒ¼ãƒ ", value=True)

    st.markdown("---")
    st.markdown("### ğŸµ éŸ³æ¥½")
    st.session_state["bgm_on"] = st.toggle("BGMã‚’å†ç”Ÿï¼ˆâ–¶ã‚’æŠ¼ã™ã¨é³´ã‚Šã¾ã™ï¼‰", value=st.session_state["bgm_on"])
    if st.session_state["bgm_on"]:
        if BGM_PATH.exists():
            st.audio(BGM_PATH.read_bytes(), format=BGM_FORMAT)
            st.caption("â€»ãƒ–ãƒ©ã‚¦ã‚¶åˆ¶é™ã«ã‚ˆã‚Šè‡ªå‹•å†ç”Ÿã¯ã§ãã¾ã›ã‚“ã€‚â–¶ ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
        else:
            st.error(f"âš  BGMãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {BGM_PATH}ï¼ˆassets/bgm.mp3 ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ï¼‰")

    if st.button("ğŸ”„ å†è¨ˆç®—", use_container_width=True):
        st.session_state["last_params_hash"] = ""  # å¼·åˆ¶å†è¨ˆç®—
        st.rerun()


# ============================================================
# 7) å†è¨ˆç®—åˆ¤å®šï¼ˆé™æ­¢è¡¨ç¤ºï¼‰+ seedå›ºå®š
# ============================================================
params_hash = f"{user_input}|{top_n}|{n_total}|{noise}|{jitter}|{qubo_iterations}|{star_count}"
needs_recalc = params_hash != st.session_state["last_params_hash"]

def make_seed(s: str) -> int:
    # å…¥åŠ›+ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰æ±ºã¾ã‚‹seedï¼ˆåŒæ¡ä»¶ãªã‚‰åŒé…ç½®ï¼‰
    return int(zlib.adler32(s.encode("utf-8")) & 0xFFFFFFFF)

# ============================================================
# 8) è¨ˆç®—ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ / é…ç½®ï¼‰
# ============================================================
def compute_all():
    progress_placeholder = st.empty()
    seed = make_seed(params_hash)
    rng = np.random.default_rng(seed)

    with progress_placeholder.container():
        st.info("ğŸ”„ è¨ˆç®—ã‚’é–‹å§‹ã—ã¾ã™...")
        progress_bar = st.progress(0)
        status_text = st.empty()

    status_text.text("ğŸ“ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºä¸­...")
    progress_bar.progress(10)
    keywords = extract_keywords(user_input, top_n=top_n)
    center_set = set(keywords)

    status_text.text("ğŸ”— å˜èªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ä¸­...")
    progress_bar.progress(30)
    network = build_word_network(keywords, GLOBAL_WORDS_DATABASE, n_total=n_total, rng=rng, jitter=jitter)

    status_text.text("ğŸŒ QUBOæœ€é©åŒ–ã§3Dé…ç½®ã‚’è¨ˆç®—ä¸­...")
    progress_bar.progress(50)

    def update_progress(current, total):
        p = 50 + int((current / total) * 40)
        progress_bar.progress(p)
        status_text.text(f"ğŸŒ QUBOæœ€é©åŒ–ä¸­... ({current}/{total} åå¾©)")

    pos = solve_qubo_placement(
        network["Q"],
        network["words"],
        network["center_indices"],
        network["energies"],
        rng=rng,
        n_iterations=qubo_iterations,
        progress_callback=update_progress,
    )

    # ä½ç½®ã‚†ã‚‰ãã¯ã€Œå†è¨ˆç®—æ™‚ã ã‘ã€
    if noise > 0:
        pos = pos + rng.normal(0, noise, size=pos.shape)

    progress_bar.progress(100)
    status_text.text("âœ… è¨ˆç®—å®Œäº†ï¼")
    progress_placeholder.empty()

    st.session_state["network"] = network
    st.session_state["pos"] = pos
    st.session_state["keywords"] = keywords
    st.session_state["center_set"] = center_set
    st.session_state["last_params_hash"] = params_hash

# åˆå› or å¤‰æ›´æ™‚ã®ã¿è¨ˆç®—ï¼ˆé€šå¸¸ã¯é™æ­¢ï¼‰
if (st.session_state["network"] is None) or needs_recalc:
    compute_all()

network = st.session_state["network"]
pos = st.session_state["pos"]
keywords = st.session_state["keywords"]
center_set = st.session_state["center_set"]

if network is None or pos is None or len(network.get("words", [])) == 0:
    st.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒä¸å®Œå…¨ã§ã™ã€‚ã€ŒğŸ”„ å†è¨ˆç®—ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
    st.stop()


# ============================================================
# 9) Plotlyæç”»ï¼ˆæ˜Ÿå±‘ï¼‹ç¸ï¼‹çƒä½“ï¼‹ãƒ©ãƒ™ãƒ«ï¼‰
#    - ç·šã‚’é›†ç´„ã—ã¦é«˜é€ŸåŒ–
# ============================================================
fig = go.Figure()

# --- æ˜Ÿå±‘ï¼ˆå®Œå…¨å›ºå®šï¼‰ ---
star_rng = np.random.default_rng(12345)
sx = star_rng.uniform(-3.2, 3.2, star_count)
sy = star_rng.uniform(-2.4, 2.4, star_count)
sz = star_rng.uniform(-2.0, 2.0, star_count)
alpha = np.full(star_count, 0.22, dtype=float)
star_size = star_rng.uniform(1.0, 2.4, star_count)
star_colors = [f"rgba(255,255,255,{a})" for a in alpha]

fig.add_trace(go.Scatter3d(
    x=sx, y=sy, z=sz,
    mode="markers",
    marker=dict(size=star_size, color=star_colors),
    hoverinfo="skip",
    showlegend=False
))

words = network["words"]
energies = network.get("energies", {})
center_indices = network.get("center_indices", [])
edges = network.get("edges", [])

# --- ç·šï¼šä¸­å¿ƒâ†’å‘¨è¾ºï¼ˆé›†ç´„ï¼‰ ---
xL, yL, zL, hoverL = [], [], [], []
for cidx in center_indices:
    if cidx >= len(words):
        continue
    cx, cy, cz = pos[cidx]
    cword = words[cidx]

    for i, w in enumerate(words):
        if i == cidx or i in center_indices:
            continue
        x, y, z = pos[i]
        e = energies.get(w, 0.0)
        d = float(np.linalg.norm(pos[i] - pos[cidx]))
        # NoneåŒºåˆ‡ã‚Š
        xL += [cx, x, None]
        yL += [cy, y, None]
        zL += [cz, z, None]
        hoverL += [f"{cword} â†’ {w}<br>è·é›¢:{d:.2f}<br>ã‚¨ãƒãƒ«ã‚®ãƒ¼:{e:.2f}", "", ""]

fig.add_trace(go.Scatter3d(
    x=xL, y=yL, z=zL,
    mode="lines",
    line=dict(width=2, color="rgba(150,200,255,0.35)"),
    hoverinfo="text",
    text=hoverL,
    showlegend=False
))

# --- ç·šï¼šå˜èªé–“ã‚¨ãƒƒã‚¸ï¼ˆé›†ç´„ï¼‰ ---
xE, yE, zE, hoverE = [], [], [], []
for i, j, e in edges:
    if i in center_indices or j in center_indices:
        continue
    x0, y0, z0 = pos[i]
    x1, y1, z1 = pos[j]
    d = float(np.linalg.norm(pos[j] - pos[i]))
    xE += [x0, x1, None]
    yE += [y0, y1, None]
    zE += [z0, z1, None]
    hoverE += [f"{words[i]} â†” {words[j]}<br>è·é›¢:{d:.2f}<br>ã‚¨ãƒãƒ«ã‚®ãƒ¼:{e:.2f}", "", ""]

fig.add_trace(go.Scatter3d(
    x=xE, y=yE, z=zE,
    mode="lines",
    line=dict(width=1, color="rgba(200,220,255,0.22)"),
    hoverinfo="text",
    text=hoverE,
    showlegend=False
))

# --- çƒä½“ï¼ˆè¨€è‘‰ï¼‰ + ãƒ©ãƒ™ãƒ«è‰²åˆ†ã‘ ---
sizes, colors, labels = [], [], []
for w in words:
    e = energies.get(w, 0.0)
    if w in center_set:
        sizes.append(28)
        colors.append("rgba(255,235,100,0.98)")
        labels.append(w)
    else:
        en = min(1.0, abs(e) / 3.0)
        sizes.append(12 + int(8 * en))
        if e < -1.5:
            colors.append("rgba(180,220,255,0.85)")
        elif e < -0.5:
            colors.append("rgba(220,240,255,0.75)")
        else:
            colors.append("rgba(255,255,255,0.60)")
        labels.append(w)

center_idx = [i for i, w in enumerate(labels) if w in center_set]
other_idx  = [i for i, w in enumerate(labels) if w not in center_set]

if other_idx:
    oi = np.array(other_idx, dtype=int)
    fig.add_trace(go.Scatter3d(
        x=pos[oi, 0], y=pos[oi, 1], z=pos[oi, 2],
        mode="markers+text",
        text=[labels[i] for i in oi],
        textposition="top center",
        textfont=dict(size=18, color="rgba(255,255,255,1.0)"),
        marker=dict(
            size=[sizes[i] for i in oi],
            color=[colors[i] for i in oi],
            line=dict(width=1, color="rgba(0,0,0,0.10)")
        ),
        hovertemplate="<b>%{text}</b><extra></extra>",
        showlegend=False
    ))

if center_idx:
    ci = np.array(center_idx, dtype=int)
    fig.add_trace(go.Scatter3d(
        x=pos[ci, 0], y=pos[ci, 1], z=pos[ci, 2],
        mode="markers+text",
        text=[labels[i] for i in ci],
        textposition="top center",
        textfont=dict(size=24, color="rgba(255,80,80,1.0)"),
        marker=dict(
            size=[sizes[i] for i in ci],
            color=[colors[i] for i in ci],
            line=dict(width=2, color="rgba(255,80,80,0.8)")
        ),
        hovertemplate="<b>%{text}</b><br>ä¸­å¿ƒèª<extra></extra>",
        showlegend=False
    ))

# ä¸­å¿ƒèªã®â€œå…‰ã®å±¤â€ï¼ˆå›ºå®šï¼‰
for cidx in center_indices:
    if cidx >= len(words):
        continue
    cx, cy, cz = pos[cidx]
    for layer, mult in enumerate([1.0, 1.3, 1.6], 1):
        opacity = 0.15 / layer
        fig.add_trace(go.Scatter3d(
            x=[cx], y=[cy], z=[cz],
            mode="markers",
            marker=dict(size=[35 * mult], color=f"rgba(150,200,255,{opacity})", line=dict(width=0)),
            hoverinfo="skip",
            showlegend=False
        ))

fig.update_layout(
    paper_bgcolor="rgba(6,8,18,1)",
    scene=dict(
        xaxis=dict(visible=False),
        yaxis=dict(visible=False),
        zaxis=dict(visible=False),
        bgcolor="rgba(6,8,18,1)",
        camera=dict(
            eye=dict(x=1.6, y=1.15, z=1.05),
            center=dict(x=0, y=0, z=0),
            up=dict(x=0, y=1, z=0),
        ),
        dragmode="orbit",
    ),
    margin=dict(l=0, r=0, t=0, b=0),
)

plotly_config = {
    "displayModeBar": True,
    "scrollZoom": bool(enable_zoom),
    "displaylogo": False,
    "responsive": True,
    "toImageButtonOptions": {"format": "png", "filename": "quantum_oracle", "height": 800, "width": 1200, "scale": 1},
    "doubleClick": "reset",
}


# ============================================================
# 10) ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆå·¦ï¼šå®‡å®™ / å³ï¼šæ ¼è¨€+QUBOå¯è¦–åŒ–ï¼‰
# ============================================================
left, right = st.columns([2.0, 1.0], gap="large")

with left:
    st.plotly_chart(fig, use_container_width=True, config=plotly_config)
    st.caption("å˜èªï¼ˆçƒä½“ï¼‰ã¨ç¸ï¼ˆç·šï¼‰ã€‚ãƒã‚¦ã‚¹ã§å›è»¢ãƒ»ã‚ºãƒ¼ãƒ ã§ãã¾ã™ã€‚ï¼ˆé™æ­¢è¡¨ç¤ºï¼‰")

with right:
    st.markdown("### ğŸ“Œ ç¾åœ¨ã®çŠ¶æ…‹")
    st.markdown(f"**æ ¸ï¼ˆæ¨å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰**ï¼š`{', '.join(keywords)}`")
    st.markdown(f"**å˜èªæ•°**: {len(words)} / **ã‚¨ãƒƒã‚¸æ•°**: {len(edges)}")
    if energies:
        mn = min(energies.values()); mx = max(energies.values())
        st.markdown(f"**ã‚¨ãƒãƒ«ã‚®ãƒ¼ç¯„å›²**: {mn:.2f} ï½ {mx:.2f}")
    st.markdown("---")

    st.markdown("### ğŸ§  QUBOï¼ˆç¬¬ä¸‰è€…å‘ã‘èª¬æ˜ï¼‰")
    st.markdown(
        "- å„å˜èªã‚’ãƒãƒ¼ãƒ‰ã€å˜èªé–“ã®ç›¸äº’ä½œç”¨ã‚’ **Qè¡Œåˆ—** ã«ç½®ãã¾ã™ã€‚  \n"
        "- **ä¼¼ã¦ã„ã‚‹ã»ã©ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒä½ã„**ï¼ˆçµã³ã¤ãï¼‰ã‚ˆã†ã«è¨­è¨ˆã—ã¦ã„ã¾ã™ã€‚  \n"
        "- å³ã®å›³ã¯ Q è¡Œåˆ—ã®å¼·ã•ï¼ˆå€¤ï¼‰ã‚’å¯è¦–åŒ–ã—ãŸã‚‚ã®ã§ã™ã€‚"
    )

    with st.expander("QUBOã®å½¢ï¼ˆæ¦‚å¿µï¼‰", expanded=False):
        st.latex(r"E(\mathbf{x})=\sum_i Q_{ii}x_i + \sum_{i<j} Q_{ij}x_i x_j")
        st.markdown("<div class='smallnote'>â€» ã“ã“ã§ã¯é…ç½®ã®ãŸã‚ã®ã€Œç›¸äº’ä½œç”¨ï¼ˆQï¼‰ã€ã‚’ä½¿ã„ã€ç–‘ä¼¼æœ€é©åŒ–ã§â€œç¸â€ã‚’æ•´ãˆã¦ã„ã¾ã™ã€‚</div>", unsafe_allow_html=True)

    Q = network["Q"]
    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆå°ã•ã‚ï¼‰
    hm = go.Figure(data=go.Heatmap(z=Q, showscale=True))
    hm.update_layout(margin=dict(l=0,r=0,t=0,b=0), height=220)
    st.plotly_chart(hm, use_container_width=True, config={"displayModeBar": False, "responsive": True})

    # ä¸Šä½ç›¸äº’ä½œç”¨ï¼ˆå¼·ã„çµã³ã¤ãï¼‰
    pairs = []
    n = Q.shape[0]
    for i in range(n):
        for j in range(i+1, n):
            pairs.append((Q[i, j], i, j))
    pairs.sort(key=lambda x: x[0])  # ä½ã„ã»ã©å¼·ã„çµã³ã¤ã
    top_pairs = pairs[:8]

    with st.expander("å¼·ã„çµã³ã¤ãï¼ˆQãŒä½ã„ãƒšã‚¢ï¼‰", expanded=False):
        for val, i, j in top_pairs:
            st.write(f"- {words[i]} â†” {words[j]} : Q={val:.2f}")

    st.markdown("---")
    st.markdown("### ğŸ—ï¸ å…ˆäººã®ã“ã¨ã°ï¼ˆæ ¼è¨€ï¼‰")

    q = select_relevant_quote(keywords)
    st.markdown(f"#### ã€Œ{q['quote']}ã€")
    st.markdown(f"**å‡ºæ‰€ï¼š** {q.get('source','â€”') if q.get('source') else 'â€”'}")
    if q.get("note"):
        st.markdown(f"<div class='smallnote'>â€» {q['note']}</div>", unsafe_allow_html=True)

    st.markdown("---")
    st.markdown("### ğŸ‘‰ æ°—ã«ãªã‚‹å˜èªã‹ã‚‰æ·±æ˜ã‚Š")
    default_word = keywords[0] if keywords else (words[0] if words else "")
    selected_word = st.selectbox("å˜èªã‚’é¸ã¶", options=words, index=words.index(default_word) if default_word in words else 0)
    cands = quote_candidates_for_word(selected_word)

    if cands:
        st.markdown(f"**ã€Œ{selected_word}ã€ã«é–¢é€£ã™ã‚‹æ ¼è¨€å€™è£œ**")
        for qq in cands:
            st.markdown(f"- **{qq.get('quote','')}**  \n  <span class='smallnote'>å‡ºæ‰€ï¼š{qq.get('source','â€”')}</span>", unsafe_allow_html=True)
    else:
        st.markdown("<div class='smallnote'>ã“ã®å˜èªã«ç›´æ¥ãƒ’ãƒƒãƒˆã™ã‚‹æ ¼è¨€ã¯æœªç™»éŒ²ã§ã™ï¼ˆExcelå´ã‚’å¢—ã‚„ã™ã¨å¼·åŒ–ã•ã‚Œã¾ã™ï¼‰ã€‚</div>", unsafe_allow_html=True)
